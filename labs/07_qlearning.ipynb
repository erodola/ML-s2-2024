{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/erodola/DLAI-s2-2023/blob/main/labs/04/4_Logistic_Regression_and_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"4C5Ct9yoZKYa"},"source":["# Machine Learning\n","\n","# Tutorial 6: Q-Learning\n","\n","In this tutorial, we will cover:\n","\n","- Q-Learning\n","- Deep Q-Learning\n","\n","Authors:\n","\n","- Antonio Ricciardi, PhD student\n","\n","Course:\n","\n","- Lectures and notebooks at https://github.com/erodola/ML-s2-2024/"]},{"cell_type":"markdown","metadata":{"id":"iXd3HJRDfLEO"},"source":["# Imports and utilities"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!sudo apt-get update\n","!sudo apt-get install -y python3-opengl\n","!apt install ffmpeg xvfb\n","!pip3 install pyvirtualdisplay"]},{"cell_type":"markdown","metadata":{},"source":["Trick to run virtual screen"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","os.kill(os.getpid(), 9)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"]},{"cell_type":"markdown","metadata":{},"source":["## Import the packages\n","\n","- `random` is to generate random numbers (that will be useful for epsilon-greedy policy).\n","- `imageio` is to generate a replay video."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import gymnasium as gym\n","import random\n","import imageio\n","import os\n","import tqdm\n","import pickle5 as pickle\n","from tqdm.notebook import tqdm\n","import base64\n","from IPython.display import HTML\n","# import gym\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from collections import deque\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def record_video(env, Qtable, out_directory, max_steps, fps=1):\n","  \"\"\"\n","  Generate a replay video of the agent\n","  :param env\n","  :param Qtable: Qtable of our agent\n","  :param out_directory\n","  :param max_steps: maximum number of steps\n","  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n","  \"\"\"\n","  images = []\n","  terminated = False\n","  truncated = False\n","  state, info = env.reset(seed=random.randint(0, 500))\n","  img = env.render()\n","  images.append(img)\n","  score = 0\n","  step = 0\n","  while not terminated and not truncated and step < max_steps:\n","    # Take the action (index) that have the maximum expected future reward given that state\n","    action = np.argmax(Qtable[state][:])\n","    state, reward, terminated, truncated, info = env.step(action)  # We directly put next_state = state for recording logic\n","    img = env.render()\n","    images.append(img)\n","    score += reward\n","    step += 1\n","  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n","  print(\"Final Score:\", score)\n"]},{"cell_type":"markdown","metadata":{},"source":["We're now ready to code our Q-Learning algorithm üî•"]},{"cell_type":"markdown","metadata":{},"source":["# Part 1: Frozen Lake ‚õÑ (non slippery version)"]},{"cell_type":"markdown","metadata":{},"source":["## Create and understand [FrozenLake environment ‚õÑ]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/))\n","---\n","\n","A good habit when you start to use an environment is to check its documentation\n","\n","https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n","\n","---\n","\n","We're going to train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.\n","\n","We can have two sizes of environment:\n","\n","- `map_name=\"4x4\"`: a 4x4 grid version\n","- `map_name=\"8x8\"`: a 8x8 grid version\n","\n","\n","The environment has two modes:\n","\n","- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).\n","- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic)."]},{"cell_type":"markdown","metadata":{},"source":["For now let's keep it simple with the 4x4 map and non-slippery.\n","We add a parameter called `render_mode` that specifies how the environment should be visualised. In our case because we **want to record a video of the environment at the end, we need to set render_mode to rgb_array**.\n","\n","As [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) ‚Äúrgb_array‚Äù: Return a single frame representing the current state of the environment. A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n","print(\"_____OBSERVATION SPACE_____ \\n\")\n","print(\"Observation Space\", env.observation_space)\n","print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"]},{"cell_type":"markdown","metadata":{},"source":["We see with `Observation Space Shape Discrete(16)` that the observation is an integer representing the **agent‚Äôs current position as current_row * ncols + current_col (where both the row and col start at 0)**.\n","\n","For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. **For example, the 4x4 map has 16 possible observations.**\n","\n","\n","For instance, this is what state = 0 looks like:\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"\\n _____ACTION SPACE_____ \\n\")\n","print(\"Action Space Shape\", env.action_space.n)\n","print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"]},{"cell_type":"markdown","metadata":{},"source":["The action space (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ:\n","- 0: GO LEFT\n","- 1: GO DOWN\n","- 2: GO RIGHT\n","- 3: GO UP\n","\n","Reward function üí∞:\n","- Reach goal: +1\n","- Reach hole: 0\n","- Reach frozen: 0"]},{"cell_type":"markdown","metadata":{},"source":["# Let's start coding üöÄ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["state_space =\n","print(\"There are \", state_space, \" possible states\")\n","\n","action_space =\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\n","def initialize_q_table(state_space, action_space):\n","  Qtable =\n","  return Qtable"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Qtable_frozenlake = initialize_q_table(state_space, action_space)"]},{"cell_type":"markdown","metadata":{},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["state_space = env.observation_space.n\n","print(\"There are \", state_space, \" possible states\")\n","\n","action_space = env.action_space.n\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n","def initialize_q_table(state_space, action_space):\n","  Qtable = np.zeros((state_space, action_space))\n","  return Qtable"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Qtable_frozenlake = initialize_q_table(state_space, action_space)"]},{"cell_type":"markdown","metadata":{},"source":["## Define the Greedy Policy\n","\n","Remember we have two policies since Q-Learning is an **off-policy** algorithm. This means we're using a **different policy for acting and updating the value function**.\n","\n","- Epsilon-greedy policy (acting policy)\n","- Greedy-policy (updating policy)\n","\n","The greedy policy will also be the final policy we'll have when the Q-learning agent completes training. The greedy policy is used to select an action using the Q-table."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#¬†define the greedy policy\n","def greedy_policy(Qtable, state):\n","  # Exploitation: take the action with the highest state, action value\n","  action =\n","\n","  return action"]},{"cell_type":"markdown","metadata":{},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def greedy_policy(Qtable, state):\n","  # Exploitation: take the action with the highest state, action value\n","  action = np.argmax(Qtable[state][:])\n","\n","  return action"]},{"cell_type":"markdown","metadata":{},"source":["## Define the epsilon-greedy policy ü§ñ\n","\n","Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.\n","\n","The idea with epsilon-greedy:\n","\n","- With *probability 1‚Ää-‚Ää…õ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n","\n","- With *probability …õ*: we do **exploration** (trying a random action).\n","\n","As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def epsilon_greedy_policy(Qtable, state, epsilon):\n","  # Randomly generate a number between 0 and 1\n","  random_num =\n","  # if random_num > greater than epsilon --> exploitation\n","  if random_num > epsilon:\n","    # Take the action with the highest value given a state\n","    # np.argmax can be useful here\n","    action =\n","  # else --> exploration\n","  else:\n","    action = # Take a random action\n","\n","  return action"]},{"cell_type":"markdown","metadata":{},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def epsilon_greedy_policy(Qtable, state, epsilon):\n","  # Randomly generate a number between 0 and 1\n","  random_num = random.uniform(0,1)\n","  # if random_num > greater than epsilon --> exploitation\n","  if random_num > epsilon:\n","    # Take the action with the highest value given a state\n","    # np.argmax can be useful here\n","    action = greedy_policy(Qtable, state)\n","  # else --> exploration\n","  else:\n","    action = env.action_space.sample()\n","\n","  return action"]},{"cell_type":"markdown","metadata":{},"source":["## Define the hyperparameters ‚öôÔ∏è\n","\n","The exploration related hyperparamters are some of the most important ones.\n","\n","- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n","- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space and hence can't solve the problem."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training parameters\n","n_training_episodes = 10000  # Total training episodes\n","learning_rate = 0.7          # Learning rate\n","\n","# Evaluation parameters\n","n_eval_episodes = 100        # Total number of test episodes\n","\n","# Environment parameters\n","env_id = \"FrozenLake-v1\"     # Name of the environment\n","max_steps = 99               # Max steps per episode\n","gamma = 0.95                 # Discounting rate\n","eval_seed = []               # The evaluation seed of the environment\n","\n","# Exploration parameters\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.05            # Minimum exploration probability\n","decay_rate = 0.0005            # Exponential decay rate for exploration prob"]},{"cell_type":"markdown","metadata":{},"source":["### Test and show frozenlake episode with untrained agent"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the output directory and filename for the video\n","output_directory = \"episode_video.mp4\"\n","\n","# Record the video for an episode\n","record_video(env, Qtable_frozenlake, output_directory, max_steps=99, fps=1)\n","\n","# Display the video in Jupyter Notebook\n","video = open(output_directory, \"rb\").read()\n","video_tag = f'<video controls alt=\"Episode Video\" src=\"data:video/mp4;base64,{base64.b64encode(video).decode()}\" type=\"video/mp4\">'\n","display(HTML(video_tag))"]},{"cell_type":"markdown","metadata":{},"source":["# Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n","  for episode in tqdm(range(n_training_episodes)):\n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    # Reset the environment\n","    state, info = env.reset()\n","    step = 0\n","    terminated = False\n","    truncated = False\n","\n","    # repeat\n","    for step in range(max_steps):\n","      # Choose the action At using epsilon greedy policy\n","      action =\n","\n","      # Take action At and observe Rt+1 and St+1\n","      # Take the action (a) and observe the outcome state(s') and reward (r)\n","      new_state, reward, terminated, truncated, info =\n","\n","      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","      Qtable[state][action] =\n","\n","      # If terminated or truncated finish the episode\n","      if terminated or truncated:\n","        break\n","\n","      # Our next state is the new state\n","      state = new_state\n","  return Qtable"]},{"cell_type":"markdown","metadata":{},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n","  for episode in tqdm(range(n_training_episodes)):\n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    # Reset the environment\n","    state, info = env.reset()\n","    step = 0\n","    terminated = False\n","    truncated = False\n","\n","    # repeat\n","    for step in range(max_steps):\n","      # Choose the action At using epsilon greedy policy\n","      action = epsilon_greedy_policy(Qtable, state, epsilon)\n","\n","      # Take action At and observe Rt+1 and St+1\n","      # Take the action (a) and observe the outcome state(s') and reward (r)\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","\n","      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n","\n","      # If terminated or truncated finish the episode\n","      if terminated or truncated:\n","        break\n","\n","      # Our next state is the new state\n","      state = new_state\n","  return Qtable"]},{"cell_type":"markdown","metadata":{},"source":["## Train the Q-Learning agent ü§ñ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"]},{"cell_type":"markdown","metadata":{},"source":["## Let's check our Q-table!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Qtable_frozenlake"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation üìù\n","\n","- We defined the evaluation method that we're going to use to test our Q-Learning agent."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n","  \"\"\"\n","  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n","  :param env: The evaluation environment\n","  :param max_steps: Maximum number of steps per episode\n","  :param n_eval_episodes: Number of episode to evaluate the agent\n","  :param Q: The Q-table\n","  :param seed: The evaluation seed array (for taxi-v3)\n","  \"\"\"\n","  episode_rewards = []\n","  for episode in tqdm(range(n_eval_episodes)):\n","    if seed:\n","      state, info = env.reset(seed=seed[episode])\n","    else:\n","      state, info = env.reset()\n","    step = 0\n","    truncated = False\n","    terminated = False\n","    total_rewards_ep = 0\n","\n","    for step in range(max_steps):\n","      # Take the action (index) that have the maximum expected future reward given that state\n","      action = greedy_policy(Q, state)\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","      total_rewards_ep += reward\n","\n","      if terminated or truncated:\n","        break\n","      state = new_state\n","    episode_rewards.append(total_rewards_ep)\n","  mean_reward = np.mean(episode_rewards)\n","  std_reward = np.std(episode_rewards)\n","\n","  return mean_reward, std_reward"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate our Q-Learning agent üìà\n","\n","- Usually, you should have a mean reward of 1.0\n","- The **environment is relatively easy** since the state space is really small (16). What you can try to do is [to replace it with the slippery version](https://gymnasium.farama.org/environments/toy_text/frozen_lake/), which introduces stochasticity, making the environment more complex."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Evaluate our Agent\n","mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n","print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Show trained agent"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the output directory and filename for the video\n","output_directory = \"episode_video.mp4\"\n","\n","# Record the video for an episode\n","record_video(env, Qtable_frozenlake, output_directory, max_steps=99, fps=1)\n","\n","# Display the video in Jupyter Notebook\n","video = open(output_directory, \"rb\").read()\n","video_tag = f'<video controls alt=\"Episode Video\" src=\"data:video/mp4;base64,{base64.b64encode(video).decode()}\" type=\"video/mp4\">'\n","display(HTML(video_tag))"]},{"cell_type":"markdown","metadata":{},"source":["# Part 2: Taxi-v3 üöñ\n","\n","## Create and understand [Taxi-v3 üöï](https://gymnasium.farama.org/environments/toy_text/taxi/)\n","---\n","\n","üí° A good habit when you start to use an environment is to check its documentation\n","\n","üëâ https://gymnasium.farama.org/environments/toy_text/taxi/\n","\n","---\n","\n","In `Taxi-v3` üöï, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue).\n","\n","When the episode starts, **the taxi starts off at a random square** and the passenger is at a random location. The taxi drives to the passenger‚Äôs location, **picks up the passenger**, drives to the passenger‚Äôs destination (another one of the four specified locations), and then **drops off the passenger**. Once the passenger is dropped off, the episode ends.\n","\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{},"source":["There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["state_space = env.observation_space.n\n","print(\"There are \", state_space, \" possible states\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["action_space = env.action_space.n\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"markdown","metadata":{},"source":["The action space (the set of possible actions the agent can take) is discrete with **6 actions available üéÆ**:\n","\n","- 0: move south\n","- 1: move north\n","- 2: move east\n","- 3: move west\n","- 4: pickup passenger\n","- 5: drop off passenger\n","\n","Reward function üí∞:\n","\n","- -1 per step unless other reward is triggered.\n","- +20 delivering passenger.\n","- -10 executing ‚Äúpickup‚Äù and ‚Äúdrop-off‚Äù actions illegally."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create our Q table with state_size rows and action_size columns (500x6)\n","Qtable_taxi = initialize_q_table(state_space, action_space)\n","print(Qtable_taxi)\n","print(\"Q-table shape: \", Qtable_taxi .shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameters ‚öôÔ∏è\n","\n","‚ö† DO NOT MODIFY EVAL_SEED: the eval_seed array **allows us to evaluate your agent with the same taxi starting positions for every classmate**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training parameters\n","n_training_episodes = 25000   # Total training episodes\n","learning_rate = 0.7           # Learning rate\n","\n","# Evaluation parameters\n","n_eval_episodes = 100        # Total number of test episodes\n","\n","# DO NOT MODIFY EVAL_SEED\n","eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n"," 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n"," 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n","                                                          # Each seed has a specific starting state\n","\n","# Environment parameters\n","env_id = \"Taxi-v3\"           # Name of the environment\n","max_steps = 99               # Max steps per episode\n","gamma = 0.95                 # Discounting rate\n","\n","# Exploration parameters\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.05           # Minimum exploration probability\n","decay_rate = 0.005            # Exponential decay rate for exploration prob\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train our Q-Learning agent ü§ñ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n","Qtable_taxi"]},{"cell_type":"markdown","metadata":{},"source":["###¬†Evaluate Q-Learning agent on the Taxi-v3 environment üöï"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the output directory and filename for the video\n","output_directory = \"episode_video.mp4\"\n","\n","# Record the video for an episode\n","record_video(env, Qtable_taxi, output_directory, max_steps=200, fps=1)\n","\n","# Display the video in Jupyter Notebook\n","video = open(output_directory, \"rb\").read()\n","video_tag = f'<video controls alt=\"Episode Video\" src=\"data:video/mp4;base64,{base64.b64encode(video).decode()}\" type=\"video/mp4\">'\n","display(HTML(video_tag))\n"]},{"cell_type":"markdown","metadata":{},"source":["# Deep Q-Learning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# @title Play Video function\n","from IPython.display import HTML\n","from base64 import b64encode\n","from pyvirtualdisplay import Display\n","\n","# create the directory to store the video(s)\n","os.makedirs(\"./video\", exist_ok=True)\n","\n","display = Display(visible=False, size=(1400, 900))\n","_ = display.start()\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment\n","and displaying it.\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","def render_mp4(videopath: str) -> str:\n","    \"\"\"\n","    Gets a string containing a b4-encoded version of the MP4 video\n","    at the specified path.\n","    \"\"\"\n","    mp4 = open(videopath, 'rb').read()\n","    base64_encoded_mp4 = b64encode(mp4).decode()\n","    return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n","        f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n","\n","# import gymnasium as gym\n","# from gym import spaces\n","#from gym.envs.box2d.lunar_lander import *\n","from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n","\n","def record_video_neuralnet(env, model, out_directory, max_steps=1000, fps=30):\n","    \"\"\"\n","    Generate a replay video of the agent\n","    :param env\n","    :param model: Neural network model\n","    :param out_directory\n","    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n","    \"\"\"\n","    vid = VideoRecorder(env, path=f\"video/vid.mp4\")\n","    state = env.reset()[0]\n","    step = 0\n","    total_reward = 0\n","    terminated = False\n","    truncated = False\n","    while not terminated and not truncated and step < max_steps:\n","        frame = env.render()\n","        vid.capture_frame()\n","        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","        action_probs = model(state_tensor)\n","        action = torch.argmax(action_probs).item()\n","        state, reward, terminated, truncated, info = env.step(action)\n","        total_reward += reward\n","        step += 1\n","\n","    vid.close()\n","    env.close()\n","    print(f\"\\nTotal reward: {total_reward}\")\n","\n","    # show video\n","    html = render_mp4(f\"video/vid.mp4\")\n","    HTML(html)"]},{"cell_type":"markdown","metadata":{},"source":["# Part 3: CartPole \n","\n","## Create and understand [CartPole-v1 üõí](https://gymnasium.farama.org/environments/classic_control/cartpole/)\n","---\n","\n","üí° A good habit when you start to use an environment is to check its documentation\n","\n","üëâ [CartPole-v1 Documentation](https://gymnasium.farama.org/environments/classic_control/cartpole/)\n","\n","---\n","\n","CartPole-v1 is a classic control environment in which the goal is to balance a pole on a cart. The environment is considered solved when the agent can balance the pole for 200 time steps.\n","\n","The state space consists of four continuous variables:\n","- Cart position\n","- Cart velocity\n","- Pole angle\n","- Pole angular velocity\n","\n","The action space consists of two discrete actions:\n","- Push the cart to the left\n","- Push the cart to the right\n","\n","The reward function is as follows:\n","- +1 for each time step the pole remains upright\n","\n","To solve this environment, we will use reinforcement learning algorithms Deep Q-learning.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create the CartPole environment\n","env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{},"source":["## Neural Network Architecture\n","\n","We will use a simple neural network with the following architecture:\n","- Input layer: corresponding to the state space\n","- Hidden layer: 128 units with ReLU activation\n","- Output layer: corresponding to the action space"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class NeuralNetwork(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(NeuralNetwork, self).__init__()\n","        self.fc1 = nn.Linear(input_size, 128)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(128, output_size)\n","        \n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create an instance of the neural network\n","model = NeuralNetwork(input_size=env.observation_space.shape[0], output_size=env.action_space.n)\n","\n","# Print the model architecture\n","print(model)"]},{"cell_type":"markdown","metadata":{},"source":["### Test with random actions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Reset the environment\n","state, _ = env.reset()\n","\n","# Initialize the score\n","score = 0\n","\n","# Perform actions in the environment until the episode is done\n","done = False\n","while not done:\n","    # Choose a random action\n","    action = env.action_space.sample()\n","    \n","    # Take the action in the environment\n","    next_state, reward, terminated, truncated, info = env.step(action)\n","    \n","    # Update the score\n","    score += reward\n","    \n","    # Update the current state\n","    state = next_state\n","\n","    done = np.logical_or(terminated, truncated)\n","\n","# Print the final score\n","print(\"Final Score:\", score)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Render Video"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the output directory and filename for the video\n","output_directory = \"episode_video.mp4\"\n","\n","# Record the video for an episode\n","record_video_neuralnet(env, model, output_directory, max_steps=200, fps=30)\n","# show video\n","html = render_mp4(f\"video/vid.mp4\")\n","HTML(html)"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop"]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameters\n","\n","- `num_episodes`: The total number of episodes to train the agent.\n","- `max_steps`: The maximum number of steps per episode.\n","- `batch_size`: The batch size for training the neural network.\n","- `gamma`: The discount factor for future rewards.\n","- `epsilon_start`: The initial exploration rate for epsilon-greedy policy.\n","- `epsilon_end`: The final exploration rate for epsilon-greedy policy.\n","- `epsilon_decay`: The decay rate for the exploration rate.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set hyperparameters\n","num_episodes = 350 # 1000\n","max_steps = 500\n","batch_size = 32\n","gamma = 0.99\n","epsilon_start = 1.0\n","epsilon_end = 0.01\n","epsilon_decay = 0.995"]},{"cell_type":"markdown","metadata":{},"source":["## Traininig the Deep Q-Learning agent ü§ñ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize the target and policy networks\n","target_net = NeuralNetwork(input_size=env.observation_space.shape[0], output_size=env.action_space.n)\n","policy_net = NeuralNetwork(input_size=env.observation_space.shape[0], output_size=env.action_space.n)"]},{"cell_type":"markdown","metadata":{},"source":["## Let's define the training loop üîÑ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_deep_q():\n","    episode_rewards = []\n","    mean_rewards = []\n","    std_rewards = []\n","    epsilons = []\n","\n","    # Create a replay buffer\n","    replay_buffer = deque(maxlen=10000)\n","\n","    # Define the optimizer\n","    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n","\n","    # Define the loss function\n","    loss_fn = F.mse_loss\n","\n","    # Initialize the epsilon value\n","    epsilon = epsilon_start\n","    \n","    # Training loop\n","    for episode in range(num_episodes):\n","        # Reset the environment and initialize the state\n","        state, _ = env.reset()\n","        total_reward = 0\n","        \n","        for step in range(max_steps):\n","            # Select an action using an epsilon-greedy policy\n","            if random.random() < epsilon:\n","                action = env.action_space.sample()\n","            else:\n","                with torch.no_grad():\n","                    q_values = policy_net(torch.tensor(state, dtype=torch.float32))\n","                    action = torch.argmax(q_values).item()\n","            \n","            # Take the selected action and observe the next state, reward, and done flag\n","            next_state, reward, terminated, truncated, info = env.step(action)\n","            total_reward += reward\n","\n","            done = np.logical_or(terminated, truncated)\n","            \n","            # Store the experience in the replay buffer\n","            replay_buffer.append((state, action, reward, next_state, done))\n","            \n","            # Update the current state\n","            state = next_state\n","            \n","            # Sample a batch of experiences from the replay buffer\n","            if len(replay_buffer) >= batch_size:\n","                batch = random.sample(replay_buffer, batch_size)\n","                states, actions, rewards, next_states, dones = zip(*batch)\n","                \n","                # Convert the batch to tensors\n","                states = torch.tensor(states, dtype=torch.float32)\n","                actions = torch.tensor(actions, dtype=torch.long)\n","                rewards = torch.tensor(rewards, dtype=torch.float32)\n","                next_states = torch.tensor(next_states, dtype=torch.float32)\n","                dones = torch.tensor(dones, dtype=torch.float32)\n","                \n","                \"\"\" ---------- FILL HERE ---------- \"\"\"\n","                # Calculate the target Q-values using the target network\n","                with torch.no_grad():\n","                    target_q_values = \n","                \n","                # Calculate the predicted Q-values using the policy network\n","                q_values = \n","                predicted_q_values = \n","                \n","                # Calculate the loss between the target and predicted Q-values\n","                loss = \n","                \"\"\" ---------- UNTIL HERE ---------- \"\"\"\n","                \n","                # Update the weights of the policy network using gradient descent\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","                \n","                # Soft update the target network weights with the policy network weights\n","                for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n","                    target_param.data.copy_(0.01 * policy_param.data + 0.99 * target_param.data)\n","            \n","            if done:\n","                episode_rewards.append(total_reward)\n","                mean_reward = np.mean(episode_rewards[-10:])\n","                mean_rewards.append(mean_reward)\n","\n","                std_reward = np.std(episode_rewards[-10:])\n","                std_rewards.append(std_reward)\n","\n","                break\n","        \n","        # Decay the exploration rate\n","        epsilon = max(epsilon_end, epsilon_decay * epsilon)\n","        \n","        # Print the training statistics\n","        print(f\"Episode: {episode+1}/{num_episodes}, Total Reward: {total_reward}\")\n","        \n","    return episode_rewards, mean_rewards, std_rewards\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_deep_q():\n","    episode_rewards = []\n","    mean_rewards = []\n","    std_rewards = []\n","\n","    # Create a replay buffer\n","    replay_buffer = deque(maxlen=10000)\n","\n","    # Define the optimizer\n","    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n","\n","    # Define the loss function\n","    loss_fn = F.mse_loss\n","\n","    # Initialize the epsilon value\n","    epsilon = epsilon_start\n","    \n","    # Training loop\n","    for episode in range(num_episodes):\n","        # Reset the environment and initialize the state\n","        state, _ = env.reset()\n","        total_reward = 0\n","        \n","        for step in range(max_steps):\n","            # Select an action using an epsilon-greedy policy\n","            if random.random() < epsilon:\n","                action = env.action_space.sample()\n","            else:\n","                with torch.no_grad():\n","                    q_values = policy_net(torch.tensor(state, dtype=torch.float32))\n","                    action = torch.argmax(q_values).item()\n","            \n","            # Take the selected action and observe the next state, reward, and done flag\n","            next_state, reward, terminated, truncated, info = env.step(action)\n","            total_reward += reward\n","\n","            done = np.logical_or(terminated, truncated)\n","            \n","            # Store the experience in the replay buffer\n","            replay_buffer.append((state, action, reward, next_state, done))\n","            \n","            # Update the current state\n","            state = next_state\n","            \n","            # Sample a batch of experiences from the replay buffer\n","            if len(replay_buffer) >= batch_size:\n","                batch = random.sample(replay_buffer, batch_size)\n","                states, actions, rewards, next_states, dones = zip(*batch)\n","                \n","                # Convert the batch to tensors\n","                states = torch.tensor(states, dtype=torch.float32)\n","                actions = torch.tensor(actions, dtype=torch.long)\n","                rewards = torch.tensor(rewards, dtype=torch.float32)\n","                next_states = torch.tensor(next_states, dtype=torch.float32)\n","                dones = torch.tensor(dones, dtype=torch.float32)\n","                \n","                # Calculate the target Q-values using the target network\n","                with torch.no_grad():\n","                    target_q_values = rewards + gamma * (1 - dones) * torch.max(target_net(next_states), dim=1).values\n","                \n","                # Calculate the predicted Q-values using the policy network\n","                q_values = policy_net(states)\n","                predicted_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n","                \n","                # Calculate the loss between the target and predicted Q-values\n","                loss = loss_fn(predicted_q_values, target_q_values)\n","                \n","                # Update the weights of the policy network using gradient descent\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","                \n","                # Soft update the target network weights with the policy network weights\n","                for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n","                    target_param.data.copy_(0.01 * policy_param.data + 0.99 * target_param.data)\n","            \n","            if done:\n","                episode_rewards.append(total_reward)\n","                mean_reward = np.mean(episode_rewards[-10:])\n","                mean_rewards.append(mean_reward)\n","\n","                std_reward = np.std(episode_rewards[-10:])\n","                std_rewards.append(std_reward)\n","\n","                break\n","        \n","        # Decay the exploration rate\n","        epsilon = max(epsilon_end, epsilon_decay * epsilon)\n","        \n","        # Print the training statistics\n","        print(f\"Episode: {episode+1}/{num_episodes}, Total Reward: {total_reward}\")\n","    \n","    return episode_rewards, mean_rewards, std_rewards"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["episode_rewards, mean_rewards, std_rewards = train_deep_q()"]},{"cell_type":"markdown","metadata":{},"source":["## Plot the rewards"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot training statistics\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(range(num_episodes), episode_rewards)\n","plt.xlabel('Episode')\n","plt.ylabel('Total Reward')\n","plt.title('Training Progress - Total Reward')"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the Deep Q-Learning agent üìà"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the output directory and filename for the video\n","output_directory = \"vid.mp4\"\n","\n","# Record the video for an episode\n","record_video_neuralnet(env, model, output_directory, max_steps=200, fps=30)\n","# show video\n","html = render_mp4(f\"video/vid.mp4\")\n","HTML(html)"]},{"cell_type":"markdown","metadata":{},"source":["# Lunar Lander\n","## Create and understand [LunarLander-v2 üöÄ](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n","The Lunar Lander is a popular environment in the OpenAI Gym library. It simulates the task of landing a spacecraft on the moon's surface. The goal is to control the spacecraft's thrusters to safely land it on a landing pad, while avoiding crashing or running out of fuel.\n","\n","The Lunar Lander environment provides the following observations:\n","- `state`: A 1D array of 8 continuous values representing the state of the spacecraft. The values include the x and y coordinates, the horizontal and vertical velocities, the angle, the angular velocity, and the contact points with the ground.\n","- `reward`: A scalar value indicating the reward obtained in the current step. The goal is to maximize the reward by successfully landing the spacecraft.\n","- `done`: A boolean value indicating whether the episode is finished. The episode ends if the spacecraft crashes or successfully lands on the landing pad.\n","- `info`: Additional information about the environment.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n","\n","# Set hyperparameters\n","num_episodes = 600\n","max_steps = 1000\n","batch_size = 32\n","gamma = 0.99\n","epsilon_start = 1.0\n","epsilon_end = 0.01\n","epsilon_decay = 0.995"]},{"cell_type":"markdown","metadata":{},"source":["## Initialize neural networks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize the target and policy networks\n","target_net = NeuralNetwork(input_size=env.observation_space.shape[0], output_size=env.action_space.n)\n","policy_net = NeuralNetwork(input_size=env.observation_space.shape[0], output_size=env.action_space.n)"]},{"cell_type":"markdown","metadata":{},"source":["## Render untrained agent"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the output directory and filename for the video\n","output_directory = \"vid.mp4\"\n","\n","# Record the video for an episode\n","record_video_neuralnet(env, policy_net, output_directory, max_steps=200, fps=30)\n","# show video\n","html = render_mp4(f\"video/vid.mp4\")\n","HTML(html)"]},{"cell_type":"markdown","metadata":{},"source":["## Train the agent"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["episode_rewards, mean_rewards, std_rewards = train_deep_q()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot training statistics\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(range(num_episodes), episode_rewards)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Render the episode"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the output directory and filename for the video\n","output_directory = \"vid.mp4\"\n","\n","# Record the video for an episode\n","record_video_neuralnet(env, policy_net, output_directory, max_steps=200, fps=30)\n","# show video\n","html = render_mp4(f\"video/vid.mp4\")\n","HTML(html)"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
