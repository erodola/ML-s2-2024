{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/erodola/DLAI-s2-2023/blob/main/labs/04/4_Logistic_Regression_and_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"4C5Ct9yoZKYa"},"source":["# Machine Learning\n","\n","# Tutorial 5: Principal Component Analysis\n","\n","In this tutorial, we will cover:\n","\n","- Principal components and dimensionality reduction\n","- Eigenvalues, SVD decomposition, and power iterations\n","- Learning parameter spaces\n","\n","Authors:\n","\n","- Prof. Emanuele Rodol√†\n","\n","Course:\n","\n","- Lectures and notebooks at https://github.com/erodola/ML-s2-2024/"]},{"cell_type":"markdown","source":["# Imports and utilities"],"metadata":{"id":"iXd3HJRDfLEO"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pRePt-K1_yw9"},"outputs":[],"source":["# @title import dependencies\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","from sklearn import decomposition\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","import plotly.graph_objects as go\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tGN_bJOcfd3","cellView":"form"},"outputs":[],"source":["# @title reproducibility stuff\n","\n","import random\n","np.random.seed(42)\n","random.seed(0)"]},{"cell_type":"code","source":["# @title visualization utils\n","\n","!pip install trimesh==4.2.4 rtree==1.2.0\n","\n","import trimesh\n","\n","def render(mesh, height_pix: int = 200):\n","  \"\"\" Renders a trimesh as a grayscale image.\n","\n","  :Authors:\n","    Emanuele Rodol√†\n","  \"\"\"\n","\n","  # center\n","  mesh.apply_translation(-mesh.centroid)\n","\n","  # rotate\n","  R = trimesh.transformations.rotation_matrix(-np.pi/10, [0, 1, 0], point=None)\n","  mesh.apply_transform(R)\n","\n","  # normalize\n","  mesh.apply_scale(1 / max(np.max(mesh.vertices, axis=0) - np.min(mesh.vertices, axis=0)))\n","\n","  # rescale\n","  mesh.apply_scale(height_pix)\n","\n","  # translate\n","  mesh.apply_translation(-np.min(mesh.vertices, axis=0))\n","\n","  Vq = np.round(mesh.vertices).astype(int)\n","  w, h, _ = 1 + np.max(Vq, axis=0).astype(int)\n","\n","  ray_origins = np.zeros((w*h, 3), dtype=np.float64)\n","  ray_origins[:, 0] = (np.arange(w)[:, None] * np.ones(h)).reshape(-1)\n","  ray_origins[:, 1] = (np.ones(w)[:, None] * np.arange(h)).reshape(-1)\n","  ray_origins[:, 2] = -10\n","\n","  view_dir = np.array([0, 0, 1], dtype=np.float64)\n","  view_dir /= np.sqrt(np.sum(view_dir**2))\n","\n","  ray_directions = np.tile(view_dir, (w*h, 1))\n","\n","  locations, _, index_tri = mesh.ray.intersects_location(ray_origins=ray_origins, ray_directions=ray_directions)\n","\n","  img = np.ones((w, h), dtype=np.float32)\n","  depths = np.empty((w, h), dtype=np.float32)\n","\n","  light_dir = np.array([0, 0, 1], dtype=np.float32)\n","  light_dir /= np.sqrt(np.sum(light_dir**2))\n","\n","  shades = np.dot(mesh.face_normals[index_tri], light_dir)\n","\n","  for idx, p in enumerate(locations):\n","      j, i, z = int(p[0]), int(p[1]), p[2]\n","      if depths[j, i] is None or depths[j, i] < z:\n","          depths[j, i] = z\n","          img[j, i] = shades[idx]\n","\n","  return img"],"metadata":{"id":"_u_h1ii327dn","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Principal components"],"metadata":{"id":"1mMJC63GohRC"}},{"cell_type":"markdown","source":["Let's start simple!\n","\n","We generate a bunch of 2D data points, and we'll use them in the first part of this notebook to play a bit."],"metadata":{"id":"zrjJ5GVn1v-0"}},{"cell_type":"code","source":["n = 50\n","xs = np.linspace(0, 1, n)\n","ys = 1.7 * xs + 2.1 + np.random.rand(n) * 1.1\n","X = np.stack((xs, ys), axis=0).T\n","\n","plt.figure(figsize=(5,2))\n","plt.scatter(X[:,0], X[:,1], s=5, color='red', zorder=2)\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"vKIqElHyal3c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Take note of the shape of `X`; here we are storing each data point as a _row_ in the **data matrix $\\mathbf{X}$**:"],"metadata":{"id":"UDvVIcgb2D3t"}},{"cell_type":"code","source":["X.shape"],"metadata":{"id":"92bdLrRx2JC4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In theory class, we derived the $k$ principal components of the data as the top-$k$ eigenvectors of the **covariance matrix $\\mathbf{C}=\\mathbf{X}^\\top \\mathbf{X}$**.\n","\n","If the dataset consists of $n$ points with $d$ dimensions, the size of the covariance matrix must be $d \\times d$. Therefore, **the size of $\\mathbf{C}$ does not depend on the number of data points**.\n","\n","This makes sense: since the principal components are eigenvectors of $\\mathbf{C}$, at most we can have as many components as the original dimensionality of the data."],"metadata":{"id":"ygLKeOBY4T40"}},{"cell_type":"code","source":["C = X.T @ X\n","C.shape"],"metadata":{"id":"2kxokcVs6UrD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Didn't we forget something?\n","\n","Before seeking the principal components, the data must be **centered**. This is because the principal components must not depend on the absolute position of the data in space, but rather by their distribution. Centering is as simple as:"],"metadata":{"id":"bVI9G3PB6dlH"}},{"cell_type":"code","source":["X = X - np.mean(X, axis=0)\n","C = X.T @ X"],"metadata":{"id":"Rasln0hs67kC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's now compute the first principal component of `X`:"],"metadata":{"id":"SKEe291Z7j2G"}},{"cell_type":"code","source":["evals, evecs = np.linalg.eig(C)\n","idx = np.argsort(evals)[::-1]  # necessary because eig() does not return sorted eigenpairs\n","evals = evals[idx]\n","evecs = evecs[:, idx]\n","\n","pc = evecs[:, 0]\n","pc"],"metadata":{"id":"djYSC30D7p3n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is the principal axis of our data. Two trivial but nonetheless important observations arise:\n","- It has as many dimensions as the original data space.\n","- Its length and sign do not really matter: what matters is the **direction**.\n","\n","Let's plot it:"],"metadata":{"id":"MthFeyAc7_SS"}},{"cell_type":"code","source":["plt.figure(figsize=(5,2))\n","plt.scatter(X[:,0], X[:,1], s=5, color='red', zorder=3)\n","xx = np.linspace(-1, 1, 2)\n","plt.plot(xx*pc[0], xx*pc[1], color='green')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"dZbgGQm48g90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Try to change the sign of `pc`, does it have any effect?"],"metadata":{"id":"0zxuUN5q8y-g"}},{"cell_type":"markdown","source":["> **EXERCISE:** Use the power interation method to compute the principal component of the data we used so far:\n",">\n","> - Initialize with a random $\\mathbf{v}_0$.\n","> - Iteratively compute:\n","> $$ \\mathbf{v}_{t+1} = \\frac{\\mathbf{Cv}_{t}}{\\|\\mathbf{Cv}_{t}\\|} $$"],"metadata":{"id":"gBjad_OuDVC8"}},{"cell_type":"code","source":["# ‚úèÔ∏è your solution here"],"metadata":{"id":"gRG9LlrxEJ-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title üëÄ Solution\n","\n","v = np.random.rand(2, 1)\n","for i in range(100):\n","  v = C @ v\n","  v /= np.sqrt(v.T @ v)\n","\n","pc, v"],"metadata":{"cellView":"form","id":"H7d9HZokENVs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SVD\n","\n","In fact, it turns out that principal components can be computed without constructing the covariance matrix at all!\n","\n","Let's apply the singular value decomposition to the data matrix $\\mathbf{X}$. We get the factorization:\n","\n","$$ \\mathbf{X} = \\mathbf{USV}^\\top$$\n","\n","where $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal (left and right singular vectors) and $\\mathbf{S}$ is diagonal (singular values)."],"metadata":{"id":"EnJextZe9_93"}},{"cell_type":"markdown","source":["> **EXERCISE:** _(pen and paper)_\n","> Consider the covariance matrix $\\mathbf{X}^\\top \\mathbf{X}$. Substitute $\\mathbf{X}$ with its SVD. What expression do you get?"],"metadata":{"id":"dFtbUOXe_cl5"}},{"cell_type":"markdown","source":["If you did the derivations correctly, you should be convinced that we can equivalently compute the _right singular vectors_ of $\\mathbf{X}$ instead of the eigenvectors of $\\mathbf{C}$.\n","\n","Let's verify it:"],"metadata":{"id":"Tq2aZEc7AE2r"}},{"cell_type":"code","source":["U, s, Vt = np.linalg.svd(X, full_matrices=False)\n","V = Vt.T  # svd() returns the transpose of V\n","\n","V[:,0], pc"],"metadata":{"id":"TuN5j5q_AWfJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Of course, the equivalence will always be up to sign."],"metadata":{"id":"Z5dSgF1nA3c-"}},{"cell_type":"markdown","source":["> **EXERCISE:** We have seen that you can get at most as many principal components as there are dimensions in the data space. But can you get more principal components than there are _points_ in the dataset?\n",">\n","> _Hint:_ Think of what it means to have a few points in many dimensions.\n",">\n","> _Hint #2:_ Check what `full_matrices=False` means."],"metadata":{"id":"723wCr4k6swN"}},{"cell_type":"markdown","source":["## Scikit-learn\n","\n","Any ML library with basic functionalities can compute the PCA, and Scikit-learn is obviously one of those."],"metadata":{"id":"EDWUaU-MA-AP"}},{"cell_type":"code","source":["from sklearn import decomposition\n","\n","pca = decomposition.PCA(n_components=1)\n","_ = pca.fit(X)\n","\n","pca.components_, pc"],"metadata":{"id":"0GE3wX3mimmj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Feel free to use whatever approach you prefer. As always, make sure to check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) of the libraries you use, not to miss any important details. For example, `sklearn`'s PCA also automatically centers the data, so we don't need to do it ourselves."],"metadata":{"id":"QyLu2XeoCDLY"}},{"cell_type":"markdown","source":["> **EXERCISE:** Generate a random dataset of 2D points, uniformly sampled from the interior of a 2D ellipse with different sample densities. Then:\n","> - Find its _two_ principal components and draw them as lines.\n","> - Do the same with a _circle_ instead of an ellipse.\n",">\n","> What conclusions can you draw from these tests?\n",">\n","> _Hint:_ to generate an ellipse, start from a circle and then rescale one of the two axes."],"metadata":{"id":"jYvZZWM8Fdj1"}},{"cell_type":"code","source":["density = [20, 100, 500, 1000]\n","\n","# ‚úèÔ∏è your solution here"],"metadata":{"id":"dJdsanPGHOeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title üëÄ Solution\n","\n","density = [20, 100, 500, 1000]\n","\n","for scale in [0.5, 1]:  # ellipse and circle\n","\n","  fig, ax = plt.subplots(1, len(density))\n","  fig.suptitle(\"circle\" if scale == 1 else \"ellipse\")\n","\n","  for i in range(len(density)):\n","\n","    n = density[i]\n","\n","    angles = np.random.uniform(low=0, high=2*np.pi, size=n)\n","    radii = np.sqrt(np.random.uniform(low=0, high=1, size=n))  # sqrt correction avoids higher density near the center\n","\n","    x = radii * np.cos(angles) * scale\n","    y = radii * np.sin(angles)\n","\n","    pca = decomposition.PCA(n_components=2)\n","    _ = pca.fit(np.hstack((x[:, None], y[:, None])))\n","\n","    pc1 = pca.components_[:, 0]\n","    pc2 = pca.components_[:, 1] * scale\n","\n","    ax[i].scatter(x, y, alpha=0.3, s=5)\n","    ax[i].plot([0, pc1[0]], [0, pc1[1]], color='red', zorder=3)\n","    ax[i].plot([0, pc2[0]], [0, pc2[1]], color='green', zorder=3)\n","    ax[i].set_title(f\"n = {n}\")\n","    ax[i].axis('equal')\n","    ax[i].axis('off')"],"metadata":{"id":"7yW0YbxqHKV7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dimensionality reduction\n","\n","PCA provides a means to project the input data onto a lower-dimensional subspace of the data space. So far we have only looked at the principal axes -- but what about the projections themselves?\n","\n","If we stack together the principal components in a matrix $\\mathbf{W}$, we can obtain lower-dimensional representations $\\mathbf{Z}$ via the orthogonal projection:\n","\n","$$ \\mathbf{Z} = \\mathbf{W}^\\top \\mathbf{X} $$\n","\n","In code:"],"metadata":{"id":"l4or4o1-BWJB"}},{"cell_type":"code","source":["# reusing the 2D data from the beginning of the notebook.\n","# go re-run the first cells if you overwrote X.\n","\n","pca = decomposition.PCA(n_components=1)\n","_ = pca.fit(X)\n","\n","Z = X @ pca.components_.T\n","Z.shape"],"metadata":{"id":"8FJJKyYsPbSO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Scikit-learn performs the operation for us by calling:"],"metadata":{"id":"AVuPS2ysRQnX"}},{"cell_type":"code","source":["Z = pca.transform(X)\n","Z.shape"],"metadata":{"id":"7vYRL2_6RVMh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We get a new representation of our original dataset, with **fewer dimensions for each data point** (one instead of two, in this example). We just did dimensionality reduction, which was our original motivation to study PCA -- mission accomplished!üéØ\n","\n","As we can expect, this reduction is **lossy**: we can't reconstruct the original data exactly, because some details were lost in the projection.\n","\n","Mathematically:\n","\n","$$ \\mathbf{X} \\approx \\mathbf{WZ} $$\n","\n","Let's try it:"],"metadata":{"id":"msXtERUMQWRd"}},{"cell_type":"code","source":["X_recon = Z @ pca.components_"],"metadata":{"id":"6LRFnv7PSVCI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We get back to two dimensions. If we plot them, we see that they correspond to the original points _projected_ onto the principal axis:"],"metadata":{"id":"RkoZkb_iSiRc"}},{"cell_type":"code","source":["plt.figure(figsize=(5,2))\n","xx = np.linspace(-1, 1, 2)\n","plt.plot(xx*pca.components_[0,0], xx*pca.components_[0,1], color='green')\n","plt.scatter(X[:,0], X[:,1], s=5, color='red', zorder=2, alpha=0.2, label='original')\n","plt.scatter(X_recon[:,0], X_recon[:,1], edgecolor='orange', facecolors='white', zorder=3, label='reconstructed')\n","plt.grid(True)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"SHy7J3EKo1AS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Intuitively, projecting onto the principal components allows us to **only keep the information that explains most of the variability in the data**, while discarding all the rest.\n","\n","It has a compression effect, because for each data point we only store one number per principal component. If the data is structured, the compression can be significant without having to sacrifice too much information!"],"metadata":{"id":"9Biw7LoqT6K5"}},{"cell_type":"markdown","source":["> **EXERCISE:** For the given set of 3D points given below, do the following:\n","> - Compute their projection onto the **first two** principal components.\n","> - How do you think they look like, if you plot the projections in 2D? Think about it, and then create the plot.\n","> - Re-do the same, but this time project onto the **first** principal component."],"metadata":{"id":"7CTi3Ws6VYIJ"}},{"cell_type":"code","source":["n = 70\n","t = np.linspace(0, 6*np.pi, n)\n","x = t*np.cos(t)\n","y = 2*t*np.sin(t)\n","z = 0.1*t\n","X = np.stack((x, y, z)).T\n","\n","# ‚úèÔ∏è your solution inbetween here\n","\n","fig = make_subplots(rows=1, cols=3, specs=[[{'type': 'scene'}, {'type': 'xy'}, {'type': 'xy'}]])\n","fig.add_trace(go.Scatter3d(x=X[:,0], y=X[:,1], z=X[:,2], mode='markers', marker=dict(opacity=1, size=5, color=t, colorscale=\"Agsunset\")), row=1, col=1)\n","fig.update_layout(showlegend=False)\n","fig.show()"],"metadata":{"id":"_WJ8vJkbXBNZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title üëÄ Solution\n","\n","n = 70\n","t = np.linspace(0, 6*np.pi, n)\n","x = t*np.cos(t)\n","y = 2*t*np.sin(t)\n","z = 0.1*t\n","X = np.stack((x, y, z)).T\n","\n","pca = decomposition.PCA(n_components=2)\n","_ = pca.fit(X)\n","Z2 = pca.transform(X)\n","\n","pca = decomposition.PCA(n_components=1)\n","_ = pca.fit(X)\n","Z1 = pca.transform(X)\n","\n","fig = make_subplots(rows=1, cols=3, specs=[[{'type': 'scene'}, {'type': 'xy'}, {'type': 'xy'}]])\n","fig.add_trace(go.Scatter3d(x=X[:,0], y=X[:,1], z=X[:,2], mode='markers', marker=dict(opacity=1, size=5, color=t, colorscale=\"Agsunset\")), row=1, col=1)\n","fig.add_trace(go.Scatter(x=Z2[:,0], y=Z2[:,1], mode='markers', marker=dict(opacity=1, size=10, color=t, colorscale=\"Agsunset\")), row=1, col=2)\n","fig.add_trace(go.Scatter(x=np.arange(n), y=Z1.reshape(-1), mode='markers', marker=dict(opacity=1, size=10, color=t, colorscale=\"Agsunset\")), row=1, col=3)\n","fig.update_layout(showlegend=False)\n","fig.show()"],"metadata":{"cellView":"form","id":"Oin_sbU2OGlz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Learning a parameter space\n","\n","Let's delve into one of the most fascinating aspects of PCA: its ability to **learn a parametric space**. This means that you can see the principal components as **parameters**. These parameters offer us the power to manipulate specific characteristics found in the original data space.\n","\n","To bring this concept to life, we'll learn a set of parameters to control facial expressions within a 3D dataset! üòÄüòÉüòÜ"],"metadata":{"id":"qc9Q71jfZgHW"}},{"cell_type":"markdown","source":["‚ö†Ô∏è This part requires using external data (find the links in the [course website](https://erodola.github.io/ML-s2-2024/)). Once you have downloaded the data, you can:\n","- Download the notebook itself and work locally, or...\n","- Mount your Google Drive, put the data there, and keep working from Colab.\n","\n","To mount your drive, do this:"],"metadata":{"id":"57cN4UtCxFfF"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"mu9Uik3kbtLp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Data loading from Colab will probably be slow at first because it must copy all the shape files to the server. But it will be faster at subsequent runs."],"metadata":{"id":"g-1YS3-O6qQp"}},{"cell_type":"code","source":["# @title Load shape data (change the paths if you work locally)\n","\n","id = 'FaceTalk_170731_00024_TA'\n","n_verts = 5023\n","\n","expressions = {\n","    'bareteeth': 122,\n","    'cheeks_in': 130,\n","    'eyebrow': 175,\n","    'high_smile': 140,\n","    'lips_back': 120,\n","    'lips_up': 136,\n","    'mouth_down': 176,\n","    'mouth_extreme': 28,\n","    'mouth_middle': 173,\n","    'mouth_open': 16,\n","    'mouth_side': 56,\n","    'mouth_up': 186\n","}\n","\n","n_total_shapes = np.sum(list(v for _, v in expressions.items()))\n","X_ = np.empty((n_verts * 3, n_total_shapes), dtype=np.float32)\n","tris = np.array([])\n","\n","j = 0\n","for k, v in expressions.items():\n","    for i in tqdm(range(1, v + 1)):\n","        fname = f\"./drive/MyDrive/Colab Notebooks/ML/{id}/{k}/{k}.{i:06d}.ply\"\n","        try:\n","            mesh = trimesh.load_mesh(fname)\n","            if tris.size == 0:\n","                tris = mesh.faces\n","            X_[:, j] = np.array(mesh.vertices).reshape(-1)\n","            j += 1\n","        except ValueError as e:\n","            print(f\"Missing file: {e}\")\n","X_ = X_[:, :j]  # in case a file was missing\n","\n","n_shapes = X_.shape[1]\n","\n","print('')\n","print('')\n","print(f\"Number of shapes: {n_shapes}\")\n","print(f\"Number of vertices per shape: {n_verts}\")"],"metadata":{"cellView":"form","id":"EY4mjqYTacV8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before we forget (I did, when preparing this notebook!) and do anything else, let's **center the data**:"],"metadata":{"id":"kShgY-nly70F"}},{"cell_type":"code","source":["X = np.empty((n_shapes, n_verts, 3))\n","\n","for i in range(n_shapes):\n","  Y = X_[:,i].reshape(n_verts,3)\n","  X[i, :, :] = Y - np.mean(Y, axis=0)\n","X = X.reshape([n_shapes, 3*n_verts])"],"metadata":{"id":"NcMEd8_Ty33D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have now created our data matrix, storing a shape per _row_. Each row contains all the (x, y, z) vertex coordinates of a shape, flattened into a 1D vector:"],"metadata":{"id":"JfR5SpKv1QMh"}},{"cell_type":"code","source":["X.shape"],"metadata":{"id":"5_-HkyaI1Q-r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's look at a random shape from the dataset. We'll render it as a 2D image to keep things as readable as possible:"],"metadata":{"id":"9rHBsSlT1TOW"}},{"cell_type":"code","source":["idx = 609\n","\n","# use this code to create a 3D mesh for plotting\n","mesh = trimesh.Trimesh(vertices=X[idx,:].reshape(n_verts, 3), faces=tris)\n","\n","# use this code to plot a rendered 3D mesh\n","plt.figure(figsize=(5,2))\n","plt.imshow(np.rot90(render(mesh)), cmap='gray')\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"9ne_ixYC1VIE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Imagine having a parameter that lets you adjust how wide the mouth opens. That's exactly what we're going for!"],"metadata":{"id":"QzQzKCxT228C"}},{"cell_type":"markdown","source":["But before we do that, we need to **center the data**. Again.\n","\n","Why are we recentering? Because what we did before was to center each shape with respect to its (x, y, z) barycenter. Instead, PCA demands to center the **data matrix** such that the mean over its rows (each representing a data point) is zero!"],"metadata":{"id":"sQd4-U-Q3I6E"}},{"cell_type":"code","source":["# center the flattened shape vectors\n","M = np.mean(X, axis=0)\n","X = X - M[None, :]"],"metadata":{"id":"PaEbPJDh1Mai"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In other words, while in our early examples we were computing the mean of a set of 2D points and subtracted it from all points, now we are computing the **mean shape** and subtracting it from all the shapes. We can actually look at the mean shape:"],"metadata":{"id":"lTWMCj9i5NXa"}},{"cell_type":"code","source":["mesh = trimesh.Trimesh(vertices=M.reshape(n_verts, 3), faces=tris)\n","\n","plt.figure(figsize=(5,2))\n","plt.imshow(np.rot90(render(mesh)), cmap='gray')\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"MwT_hoMx5gvA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE:** What do you think will happen if you now plot again a random shape, _after_ the data has been centered? Think about it, and check your intuition by visualizing a random shape."],"metadata":{"id":"P_ygctR659B2"}},{"cell_type":"markdown","source":["---\n","\n","**A note on dimensionality**\n","\n","You might have noticed that in our earlier toy examples we had $n$ data points in $d$ dimensions, with $n \\gg d$.\n","\n","Now it's quite the opposite: we are dealing with $n \\ll d$ data points (around 1400 shapes, each with thousands of dimensions). Yet, since we are dealing with _structured_ data, we can rightfully hope that there is something to learn.\n","\n","---"],"metadata":{"id":"eqKCdWqCr044"}},{"cell_type":"markdown","source":["We are now ready to compute the principal components of `X`!"],"metadata":{"id":"XADgD-Ga8F9g"}},{"cell_type":"code","source":["n_pc = 4  # principal components"],"metadata":{"id":"EFYgm3Fr-myv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pca = decomposition.PCA(n_components=n_pc)\n","_ = pca.fit(X)\n","V = pca.components_.T\n","\n","# equivalent, but slower\n","# _, s, Vt = np.linalg.svd(X, full_matrices=False)\n","# V = Vt.T\n","# V = V[:, :n_pc]\n","# s = s[:n_pc]"],"metadata":{"id":"m8LHkzamfo9o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's choose a shape and project it onto the principal components of the dataset:"],"metadata":{"id":"NOHHPYX_f39C"}},{"cell_type":"code","source":["shape_idx = 0\n","\n","Z = X[shape_idx] @ V\n","\n","X[shape_idx].shape, Z.shape"],"metadata":{"id":"F6ZwNABjgUSI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Think about it: we took a data point that originally had 15,069 dimensions (5023 vertices times 3 vertex coordinates), and reduced it to mere **4 dimensions**! This seems like a very convenient encoding. But is it working any good? Let's try to reconstruct the shape, from `Z` back to `X`:"],"metadata":{"id":"2sPvgFQQggcK"}},{"cell_type":"code","source":["X_recon = Z @ V.T\n","X_recon = X_recon + M[None, :]  # adding back the mean!\n","\n","mesh = trimesh.Trimesh(vertices=(X[shape_idx] + M[None, :]).reshape(n_verts, 3), faces=tris)\n","mesh_recon = trimesh.Trimesh(vertices=X_recon.reshape(n_verts, 3), faces=tris)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(5, 2))\n","\n","ax[0].imshow(np.rot90(render(mesh)), cmap='gray')\n","ax[0].axis('off')\n","ax[0].set_title('original')\n","\n","ax[1].imshow(np.rot90(render(mesh_recon)), cmap='gray')\n","ax[1].axis('off')\n","ax[1].set_title('reconstructed');"],"metadata":{"id":"-yFleL7xhUbh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Not bad, but if you look closely you'll see that the shape is not exactly the same. Some details are gone with the projection!\n","\n","We'll now focus on something more interesting: how can we interpret the $\\mathbf{Z}$ encoding?"],"metadata":{"id":"e_UPfxlcjGgi"}},{"cell_type":"markdown","source":["## Interpreting $\\mathbf{Z}$ as parameters üéõÔ∏è\n","\n","The idea is that the 4 values encoded inside $\\mathbf{Z}$ (the result of projecting the data $\\mathbf{X}$ onto the principal components) can be interpreted as shape parameters.\n","\n","Let's plot them:"],"metadata":{"id":"HtxmmGqrjg-w"}},{"cell_type":"code","source":["plt.figure(figsize=(4,2))\n","plt.bar(range(4), Z, zorder=2, width=0.5)\n","plt.grid('on')\n","plt.xlabel('Parameters')\n","plt.ylabel('Values')\n","plt.xticks(range(4), range(4))\n","plt.show()"],"metadata":{"id":"OHFvHPluj6i1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You might be getting negative values; but as we know, the sign does not matter since these values multiply the principal components, which are defined up to sign.\n","\n","Actually, it is very tempting to see these parameters as knobs that tune certain features in the data üéöÔ∏èüéõÔ∏è. For example, why is parameter 2 so small? And why is parameter 0 so big in comparison? What are they representing?\n","\n","Let's find out!"],"metadata":{"id":"gwP4Rcpklge4"}},{"cell_type":"markdown","source":["> **EXERCISE:** Introduce a scale factor $\\alpha$ to selectively amplify one of the principal components, and verify what happens to the reconstructed shape. Test with a range of $\\alpha$'s, and plot the resulting reconstructed shapes."],"metadata":{"id":"MGP7jeGeoVUR"}},{"cell_type":"code","source":["shape_idx = 0  # shape\n","pc_idx = 0     # principal component\n","\n","alphas = np.array([-6.5, -5, -2, 1, 2, 5, 6.5])\n","\n","# ‚úèÔ∏è your solution here"],"metadata":{"id":"GKIDeMsYobhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title üëÄ Solution\n","\n","shape_idx = 0  # shape\n","pc_idx = 0     # principal component\n","\n","alphas = np.array([-6.5, -5, -2, 1, 2, 5, 6.5])\n","\n","fig, ax = plt.subplots(1, 7, figsize=(10, 4))\n","\n","for i, alpha in enumerate(alphas):\n","\n","  Z = X[shape_idx] @ V\n","  Z[pc_idx] *= alpha\n","\n","  X_recon = Z @ V.T + M[None, :]\n","\n","  mesh = trimesh.Trimesh(vertices=X_recon.reshape(n_verts, 3), faces=tris)\n","\n","  ax[i].imshow(np.rot90(render(mesh)), cmap='gray')\n","  ax[i].axis('off')"],"metadata":{"cellView":"form","id":"zrFW3nlbmfds"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE:** Re-run the previous experiment with another shape (e.g. `shape_idx=1181`). Is your intuition confirmed?"],"metadata":{"id":"ev101qwJmhVh"}},{"cell_type":"markdown","source":["> **EXERCISE:** Test all the 4 components, including their combinations (e.g. scale all of them simultaneously, or only two of them, etc.). Are they all easy to interpret? _Do you need $\\alpha$ with different magnitudes to observe an effect with all parameters?_"],"metadata":{"id":"9dGSJFw8mtvv"}},{"cell_type":"markdown","source":["There is another way to analyze the learned principal components. Rather than choosing a specific shape and reconstructing it via a rescaled $\\mathbf{Z}$, a better idea is to directly analyze the effect of the principal components on the **mean shape**.\n","\n","The intuition is that we can modify the mean shape by adding individual components to it. Each component represents a specific direction of change, influencing certain shape features. By adjusting the scale of these added components, we can increase or decrease the variations captured by each one. This method is straightforward to implement ‚Äî **simply add the component to the mean shape**.\n","\n","But are all components equally important?"],"metadata":{"id":"zunjwe9Bwvtp"}},{"cell_type":"markdown","source":["## Interpreting the singular values\n","\n","(_Note: Before reading on, make sure you did the pen-and-paper exercise in the SVD section._)\n","\n","Admittedly we have neglected the singular values so far, as we concentrated on the principal components themselves. However, we know from theory that the singular values of $\\mathbf{X}$ (i.e. the square root of the eigenvalues of $\\mathbf{C}$) are related to the **variability of the data** along each principal direction.\n","\n","Let's plot the ones we have:"],"metadata":{"id":"lf_GL61A0dZu"}},{"cell_type":"code","source":["plt.figure(figsize=(4,2))\n","plt.bar(range(4), pca.singular_values_, zorder=2, width=0.5, color='skyblue')\n","plt.grid('on')\n","plt.xlabel('Singular values')\n","plt.xticks(range(4), range(4))\n","plt.show()"],"metadata":{"id":"roMCIxLb0tjp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is sure suggesting that the first principal component explains a lot of the variability in the data! In fact, let's have a look at the first 200, rather than just the first four:"],"metadata":{"id":"EHoPzdll2zS5"}},{"cell_type":"code","source":["pca_full = decomposition.PCA(n_components=200)\n","_ = pca_full.fit(X)\n","\n","plt.figure(figsize=(5,2))\n","plt.plot(pca_full.singular_values_, zorder=2, color='skyblue', linewidth=4)\n","plt.grid('on')\n","plt.xlabel('Singular values')\n","plt.show()"],"metadata":{"id":"gTGPA4GH26tn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["They drop very quickly, meaning that indeed most of the information about the data is contained in the first few principal components! Probably 25-50 components are more than enough to parametrize the shape space of these facial expressions. We'll keep using four, just for ease of visualization."],"metadata":{"id":"ulEXE67Z413e"}},{"cell_type":"code","source":["# recompute in case we overwrote something during our tests\n","pca = decomposition.PCA(n_components=n_pc)\n","_ = pca.fit(X)\n","V = pca.components_.T"],"metadata":{"id":"Q50_lYYPAWC_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When plotting deviations from the mean shape using principal components in PCA, we should then scale the principal component by its corresponding weight. This scaling ensures that modifications to the mean shape are realistic, consistent, and representative of actual variations observed in the dataset."],"metadata":{"id":"lxOKQv65CNMl"}},{"cell_type":"markdown","source":["More precisely, for each principal component we can obtain its **variance** from the corresponding singular value $\\sigma$ as:\n","\n","$$ \\sigma \\mapsto \\frac{\\sigma^2}{n-1} $$\n","\n","In fact, recall from the theory class that the variance is given by the _eigenvalues of the covariance matrix $\\mathbf{X}^\\top\\mathbf{X}$_, which are the square of the singular values of the data matrix $\\mathbf{X}$. We then divide by $n-1$, because the covariance matrix represents a raw sum of squares over $n$ data samples, rather than an average."],"metadata":{"id":"hBJMgXtkDya_"}},{"cell_type":"code","source":["var = (pca.singular_values_**2) / (n_shapes - 1)\n","\n","# sklearn already does the calculation for us, compare:\n","var, pca.explained_variance_"],"metadata":{"id":"U8kTsHEl-gKg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, let's plot each principal component at 2 standard deviations from the mean (each component using its own standard deviation value) to capture the variability across the majority of the data (95% in a normally distributed dataset):"],"metadata":{"id":"FY6nTnGcHsEn"}},{"cell_type":"code","source":["sigma = var**0.5\n","\n","fig, ax = plt.subplots(2, n_pc, figsize=(10, 4))\n","\n","for i in range(n_pc):\n","\n","  A = M - 2*sigma[i]*V[:,i]\n","  mesh = trimesh.Trimesh(vertices=A.reshape(n_verts, 3), faces=tris)\n","\n","  ax[0, i].imshow(np.rot90(render(mesh)), cmap='gray')\n","  ax[0, i].axis('off')\n","  ax[0, i].set_title(f\"pc {i}\")\n","\n","  A = M + 2*sigma[i]*V[:,i]\n","  mesh = trimesh.Trimesh(vertices=A.reshape(n_verts, 3), faces=tris)\n","\n","  ax[1, i].imshow(np.rot90(render(mesh)), cmap='gray')\n","  ax[1, i].axis('off')\n","  ax[1, i].set_title(f\"pc {i}\")"],"metadata":{"id":"9nHNdeuN9C_9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As a final note, Scikit-learn also computes the mean `M` for us, simplifying the overall pipeline to:\n","\n","```python\n","pca = decomposition.PCA(n_components=n_pc)\n","pca.fit(X)\n","A_plus = pca.mean_ + 2*np.sqrt(pca.explained_variance_)*pca.components_\n","A_minus = pca.mean_ - 2*np.sqrt(pca.explained_variance_)*pca.components_\n","```"],"metadata":{"id":"u_Bp-l1O0Eq2"}},{"cell_type":"markdown","source":["# Playground: The PCA Zoo ü¶í\n","\n","In this final part, you'll generate new animals weirder than the platypus!\n","\n","Follow the same approach as we did for the 3D faces, but use the 3D animal data from the [SMAL dataset](https://smal.is.tue.mpg.de/). The data is available on the [course website](https://erodola.github.io/ML-s2-2024/)."],"metadata":{"id":"QDsfg1VlZrWI"}},{"cell_type":"markdown","source":["If you mounted your Google Drive, you can use this code for loading:"],"metadata":{"id":"ETnSYLlkDxIe"}},{"cell_type":"code","source":["# @title Load shape data (change the paths if you work locally)\n","\n","id = 'animals'\n","\n","n_verts = 3889\n","n_shapes = 41\n","\n","X_ = np.empty((n_verts * 3, n_shapes), dtype=np.float32)\n","tris = np.array([])\n","\n","for i in tqdm(range(n_shapes)):\n","  fname = f\"./drive/MyDrive/Colab Notebooks/ML/{id}/toy_{i}.ply\"\n","  mesh = trimesh.load_mesh(fname)\n","\n","  # rotate the mesh for better visualization\n","  R = trimesh.transformations.rotation_matrix(-np.pi/2, [1, 0, 0], point=None)\n","  mesh.apply_transform(R)\n","  R = trimesh.transformations.rotation_matrix(-np.pi/5, [0, 1, 0], point=None)\n","  mesh.apply_transform(R)\n","\n","  if tris.size == 0:\n","    tris = mesh.faces\n","  X_[:, i] = np.array(mesh.vertices).reshape(-1)\n","\n","print('')\n","print('')\n","print(f\"Number of shapes: {n_shapes}\")\n","print(f\"Number of vertices per shape: {n_verts}\")"],"metadata":{"cellView":"form","id":"rWFqqCrjD04R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I'll also **recenter the 3D shapes** for you, but you'll have to do all the rest by yourself!"],"metadata":{"id":"KQgT14qsE_r9"}},{"cell_type":"code","source":["X = np.empty((n_shapes, n_verts, 3))\n","\n","for i in range(n_shapes):\n","  Y = X_[:,i].reshape(n_verts,3)\n","  X[i, :, :] = Y - np.mean(Y, axis=0)\n","X = X.reshape([n_shapes, 3*n_verts])\n","\n","X.shape"],"metadata":{"id":"UshWXcGgFAap"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And here's the code for plotting a shape (notice the `height_pix=350` parameter, to increase the render resolution a bit):"],"metadata":{"id":"W4JA3pYpGnh6"}},{"cell_type":"code","source":["idx = 12\n","\n","# use this code to create a 3D mesh for plotting\n","mesh = trimesh.Trimesh(vertices=X[idx,:].reshape(n_verts, 3), faces=tris)\n","\n","# use this code to plot a rendered 3D mesh\n","plt.figure(figsize=(5,3))\n","plt.imshow(np.rot90(render(mesh, height_pix=350)), cmap='gray')\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"1x9IgwQxFd5S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On to you: try to learn some interesting parameters and use them to generate funny animals üêí"],"metadata":{"id":"JhegX-nyHv3b"}},{"cell_type":"markdown","source":["> **EXERCISE:**\n",">\n","> - Train PCA on the SMAL dataset of 3D animals, and experiment with their principal components to control the generation of new animals.\n","> - How many components do we need to capture most of the variability of the data?\n","> - Try to **interpolate** the parameters between different shapes, can you get weird animal fusions?\n","> - Try to **extrapolate** the parameters by amplifying their values beyond boundaries: can you get exaggerated shape features?\n","> - Can you get a super fat lion? ü¶Å"],"metadata":{"id":"kQ7EqcpmH_cq"}}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}