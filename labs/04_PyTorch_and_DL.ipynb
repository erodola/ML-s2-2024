{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["FkefLsEoJS4Y","apkyGbXEbu9F","F7FZjuoT6Z9E","TZqlt4gxb4IK"],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5kgFt-wdVnXI"},"source":["# Machine Learning\n","\n","We recommend going through the notebook using Google Colaboratory.\n","\n","# Tutorial 4: PyTorch and Deep Learning\n","\n","In this tutorial, we will cover:\n","\n","- PyTorch\n","- Autograd, back-propagation\n","- Modules, `torch.nn`\n","\n","Authors:\n","\n","- Prof. Emanuele Rodolà\n","- Based in part on original material by Dr. Antonio Norelli and Dr. Luca Moschella\n","\n","Course:\n","\n","- Lectures and notebooks at https://github.com/erodola/ML-s2-2024/"]},{"cell_type":"markdown","source":["Today we'll cover a new library: [PyTorch](https://pytorch.org/), one of the most popular python frameworks for deep learning. We'll then use it to implement our first deep neural network (a Multi-Layer Perceptron). Brace yourselves for the ride!"],"metadata":{"id":"q-w7uDAzjm3v"}},{"cell_type":"markdown","source":["# PyTorch tensors\n","\n","Similarly to Numpy's multidimensional arrays we have used so far, PyTorch also provides a data structure for storing high-dimensional data. It's explictly called a `torch.tensor`, and it's a close cousin of our beloved `numpy.array`.\n","\n","In fact, since we already know how to manipulate Numpy's multidimensional arrays, shifting to tensors is not a problem! There are a few differences though, worth going through."],"metadata":{"id":"YLdeSXwekFp2"}},{"cell_type":"markdown","source":["## Tensor basics"],"metadata":{"id":"BrSa9ggamxin"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int32)"],"metadata":{"id":"Ne9MpJDNk5Uu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.zeros((3,5))"],"metadata":{"id":"kyBz2Us_lJnI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.ones((2,5), dtype=torch.float64)"],"metadata":{"id":"XbZsAF53lJ8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.eye(4)"],"metadata":{"id":"2QZ7KkaqlLci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.rand(2,2)  # note: in Numpy, we did np.random.rand(2,2)"],"metadata":{"id":"FLaCYBy1lOWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Pro tip**: Bookmark the [PyTorch docs](https://pytorch.org/docs/stable/)."],"metadata":{"id":"UwRdhePrll6l"}},{"cell_type":"code","source":["torch.randint(0, 100, (3,3))"],"metadata":{"id":"Lu20RJQOlrnk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t = torch.rand((3, 3))\n","torch.ones_like(t)"],"metadata":{"id":"GfpGObB5lr_F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["⚠️ The **transpose** operation has a different meaning in PyTorch if compared with NumPy!"],"metadata":{"id":"f3svQ8CFpFUY"}},{"cell_type":"code","source":["# in numpy, transpose() is actually permuting *all* the dimensions:\n","a = np.ones((2, 3, 6))\n","a.transpose(0, 2, 1).shape"],"metadata":{"id":"xonmEVbCpS45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# in pytorch, this is done by using permute():\n","a = torch.ones((2, 3, 6))\n","a.permute(0, 2, 1).shape"],"metadata":{"id":"91cIWRWnq3B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# instead, pytorch's transpose() is more intuitive and only swaps *two* dimensions:\n","a = torch.ones((2, 3, 6))\n","a.transpose(1, 2).shape"],"metadata":{"id":"iQ1PNoHuq-li"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# in numpy, you can call transpose() without arguments to transpose 2d arrays\n","a = np.ones((2, 3))\n","a.transpose()"],"metadata":{"id":"D4dSNZJSrmw9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# in pytorch, this doesn't work at all!\n","a = torch.ones((2, 3))\n","a.transpose()"],"metadata":{"id":"7EtpjjSJr8tb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use T instead:\n","a.T"],"metadata":{"id":"i2J_F0_EsEd0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you are a fan of `einsum`, you can still use it in PyTorch:"],"metadata":{"id":"CJ8K4eQYsTI_"}},{"cell_type":"code","source":["A = torch.rand((2, 3))\n","b = torch.rand(3)\n","torch.einsum('ij, j -> i', A, b)"],"metadata":{"id":"xu7DrhyssXAC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Reshaping, indexing, slicing, broadcasting, stacking, concatenating**, and **adding new dimensions** work as in Numpy:"],"metadata":{"id":"0cOx5bYCs2uS"}},{"cell_type":"code","source":["a = torch.arange(12)\n","a.reshape(2, 2, 3)"],"metadata":{"id":"NaDzqsLss-nV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.reshape(1, -1)"],"metadata":{"id":"fFYnCXf9uYX5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.reshape(-1)"],"metadata":{"id":"kLW5QOSRubcs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Differently from Numpy, you can add dimensions using `unsqueeze`:"],"metadata":{"id":"TB7fN5TLuedW"}},{"cell_type":"code","source":["b = a.unsqueeze(0)  # add a new dimension at the beginning\n","b.shape"],"metadata":{"id":"DJqemEfyukB1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b = a.unsqueeze(-1)  # add a new dimension at the end\n","b.shape"],"metadata":{"id":"GTBrcfmhutNO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In Numpy, we would have done it by using `np.newaxis` or `None`:"],"metadata":{"id":"Cwl5_aD7u-Oe"}},{"cell_type":"code","source":["b = a[:, None]\n","b.shape"],"metadata":{"id":"V-naxNWnvEX6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Also, the `axis` attribute of Numpy is called `dim` in Pytorch:"],"metadata":{"id":"RTG61WfWybcl"}},{"cell_type":"code","source":["a = np.random.rand(2, 3)\n","np.sum(a, axis=1)"],"metadata":{"id":"uIFmZKvLyfue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = torch.rand((2, 3))\n","torch.sum(a, dim=1)"],"metadata":{"id":"Dktwq0agynfL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Torch also adds new functions for tensor manipulation that we don't have in Numpy, such as [gather](https://pytorch.org/docs/stable/generated/torch.gather.html)."],"metadata":{"id":"Jfu-tRHTvWIq"}},{"cell_type":"markdown","source":["**Type conversion** of tensors is also made easier in PyTorch:"],"metadata":{"id":"SU52Mc6kzN3B"}},{"cell_type":"code","source":["a = torch.rand(3, 3) + 0.5"],"metadata":{"id":"pVeaKXT0zVOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.int()"],"metadata":{"id":"dMhWEYTZzWZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.long()"],"metadata":{"id":"Ssp5WtDYzX_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.float()"],"metadata":{"id":"HoFdt9j0zZSn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.double()"],"metadata":{"id":"TYDTAHFZzade"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.bool()"],"metadata":{"id":"tnhhu5IQzb5D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.to(torch.double)"],"metadata":{"id":"dgEWCjFSzdMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.to(torch.uint8)"],"metadata":{"id":"_uk_fSZnzeag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.bool().int()"],"metadata":{"id":"rVkl0MZNzfvm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, one can easily **convert** to/from Numpy tensors:"],"metadata":{"id":"BhkoGXM0lvM2"}},{"cell_type":"code","source":["t = torch.rand((3, 3), dtype=torch.float32)\n","t.numpy()"],"metadata":{"id":"mLfF5d_zlxCL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = np.random.rand(3,3).astype(np.float16)\n","torch.from_numpy(n)"],"metadata":{"id":"9mO_oE3Tlxa6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If Numpy's `ndarray` is so similar to Torch's `tensor`, why should we prefer the latter to do Deep Learning?\n","\n","In fact, there are two important distinctions:\n","\n","- ``Tensor`` supports GPU computations.\n","- ``Tensor`` may store extra information needed for **back-propagation**:\n","  - A `grad` attribute storing the gradient of the loss w.r.t. the tensor.\n","  - A node representing an operation in the computational graph that produced this tensor."],"metadata":{"id":"F6Fap-7WlDOz"}},{"cell_type":"markdown","source":["The **device** of a tensor indicates the memory in which the tensor is currently stored: RAM (denoted as ``cpu``) or GPU memory (denoted as ``cuda``)"],"metadata":{"id":"39SWnTpgmqD2"}},{"cell_type":"code","source":["t = torch.rand((3,5))\n","t.device"],"metadata":{"id":"EJaLaoTomr3W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using the GPU\n","\n","Thanks to the explosion of the videogame industry in the last 50 years, the performance of the chips specialized in rendering and processing graphics --known as GPUs-- has dramatically improved.\n","\n","In 2007 NVidia realized the potential of parallel GPU computing outside the videogame world, and released the first version of the CUDA framework, allowing  software developers to use GPUs for general purpose processing.\n","\n","Graphics operations are mostly linear algebra operations, and accelerating them can turn very useful in many other fields.\n","\n","In 2012 Hinton et al. [demonstrated](https://en.wikipedia.org/wiki/AlexNet) the huge potential of GPUs in training deep neural networks, starting *de facto* the glorious days of deep learning."],"metadata":{"id":"-o56nGVVm2FE"}},{"cell_type":"code","source":["# Check if the GPU is available\n","torch.cuda.is_available()"],"metadata":{"id":"1rwS7oz9ngZD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If available use the GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"metadata":{"id":"DnnV-Ek-nh23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t = torch.rand((2,3,7))\n","t = t.to(device)  # Note that we are assigning back to t, otherwise t won't be updated!\n","t"],"metadata":{"id":"PfTBZ3XLnjPg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Construct tensors directly on the GPU memory\n","t = torch.ones((5, 5), device='cuda')\n","t"],"metadata":{"id":"8qZXMJRBnks8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t = torch.rand((3,3))\n","\n","# you can also do this, but be careful: the code will not run if a GPU is not available\n","t = t.cuda()\n","t"],"metadata":{"id":"IK8ZrcA9nm48"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t = t.cpu()\n","t"],"metadata":{"id":"wjrZkU-wnoKU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE**\n",">\n","> Build a tensor $X \\in \\mathbb{R}^{k \\times k}$ **on GPU**, filled with zeros and the sequence $[0, ..., k-1]$ along the diagonal."],"metadata":{"id":"aAfmA9tcn2kc"}},{"cell_type":"code","source":["# ✏️ your solution\n","k = 12"],"metadata":{"id":"Ua5ovhprn53J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 👀 Solution\n","\n","k = 12\n","X = torch.diag(torch.arange(k)).to('cuda')\n","X"],"metadata":{"cellView":"form","id":"8eFrb7MBn7zx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reproducibility\n","\n","As mentioned in the previous notebooks, we are going to ensure that all the RNGs used in different parts of this notebook produce the same sequence of numbers each time. Let's add PyTorch's generators as well:"],"metadata":{"id":"LqNritfLCtsY"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import random\n","torch.manual_seed(42)      # PyTorch CPU\n","np.random.seed(42)         # NumPy\n","random.seed(0)             # Python built-in\n","torch.cuda.manual_seed(0)  # PyTorch GPU"],"metadata":{"id":"KlbEmV79C5cx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For some operations, cuDNN (NVIDIA's library for deep neural networks) uses algorithms that can produce different results on different runs, *even* with the same inputs and the same seed. Below we set `deterministic = True`, forcing cuDNN to use deterministic algorithms where possible. This might limit cuDNN to a subset of algorithms that might not be  the most efficient, but will produce the same results across different runs."],"metadata":{"id":"eRrkZo1kC76l"}},{"cell_type":"code","source":["torch.backends.cudnn.deterministic = True"],"metadata":{"id":"8bqXnOKSC9ow"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, when `benchmark = True`, cuDNN will automatically find the most efficient algorithms for your specific operations based on your network architecture and input sizes. This can greatly improve performance. However, since the selection of algorithms might change from one run to another, this can lead to non-deterministic behavior. Setting it to `benchmark = False` prevents cuDNN from dynamically selecting algorithms, thus improving reproducibility at the cost of potential performance gains."],"metadata":{"id":"vFfFkd-9C_Rn"}},{"cell_type":"code","source":["torch.backends.cudnn.benchmark = False"],"metadata":{"id":"Q4KZZeZRDAqw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PyTorch Datasets\n","\n","So far we have handled the data either ourselves by hand, or by using Scikit-learn's provided datasets. PyTorch provides convenient classes to handle the data and make its manipulation as painless as possible!"],"metadata":{"id":"LRhIan75zvFz"}},{"cell_type":"markdown","source":["[`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) is an abstract class representing a dataset. Your custom dataset should inherit `Dataset` and override the following methods:\n","\n","- `__len__`: so that `len(dataset)` returns the size of the dataset\n","- `__getitem__`: so that `dataset[i]` returns the $i$-th sample from the dataset.\n","\n","Let's create a toy dataset:"],"metadata":{"id":"H4T9Qk2FDXcs"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class ToyDataset(Dataset):\n","\n","  def __init__(self, n_points: int = 20, noise: float = .1):\n","    super().__init__()\n","\n","    self.n_points = n_points\n","\n","    # these two lines pre-load the entire dataset in memory\n","    self.x = torch.linspace(-1, 1, n_points)\n","    self.y = self.x ** 3 + noise * torch.randn(n_points)\n","\n","  def __len__(self):\n","    return self.n_points\n","\n","  def __getitem__(self, idx):\n","    return {\n","        'x': self.x[idx],\n","        'y': self.y[idx]\n","    }"],"metadata":{"id":"FZqOnHsXDbB-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this case the dataset is composed of simple pairs:"],"metadata":{"id":"DntBeAOKDeuM"}},{"cell_type":"code","source":["toydataset = ToyDataset(20, noise=.1)\n","toydataset[2]"],"metadata":{"id":"AdKyqLOsDgFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.express as px\n","fig = px.scatter(x=toydataset.x.numpy(), y=toydataset.y.numpy())\n","fig.update_layout(width=400, height=300)\n","fig.show()"],"metadata":{"id":"bITZXdmiDhZG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **NOTE**\n",">\n","> Small Python reminder. Every object that implements the `__getitem__` method follows the [iterator procotol](https://www.python.org/dev/peps/pep-0234/#python-api-specification). It means that you can **iterate** the dataset:"],"metadata":{"id":"psW-_t6MDjcK"}},{"cell_type":"code","source":["from tqdm.notebook import tqdm as tqdm   # just a progress bar\n","\n","for sample in tqdm(toydataset):  # wrap the iterable in tqdm and you're done\n","  pass"],"metadata":{"id":"ZFXFzmi5DllU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["⚠️ In `ToyDataset` we stored the whole dataset **in memory**, i.e. in the attributes `x`, `y` of the `ToyDataset` class.\n","This is the fastest and simplest way to implement a dataset, but it is **not always feasible**.\n","What if you must train a neural network on 500GB of images?\n","The whole dataset does not fit in memory!\n","\n","We can instead implement *lazy loading*: we load each item **only when it's needed** -- and even apply some preprocessing on the fly.\n","\n","Example:\n","\n","```python\n","class LazyDataset(Dataset):\n","\n","  def __init__(self, file_paths: Sequence[Path]):\n","    super().__init__()  \n","    self.file_paths = file_paths\n","\n","  def __len__(self):\n","    return len(self.file_paths)\n","\n","  def __getitem__(self, idx):\n","    sample_path = self.file_paths[idx]\n","\n","    # -> Load sample_path in memory\n","    # -> Perform some lightweight preprocessing\n","    # -> Generate (sample_input, sample_output)\n","\n","    return {\n","        'x': sample_input,\n","        'y': sample_output\n","    }\n","\n","```"],"metadata":{"id":"7OZ0ObtQDn2Q"}},{"cell_type":"markdown","source":["The dataset can return any type of object, i.e. you are *not* forced to return a dictionary of tensors:"],"metadata":{"id":"Aja97sLkDqag"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class AnotherDataset(Dataset):\n","  def __init__(self):\n","    super().__init__()\n","    self.myitems = torch.arange(100)\n","\n","  def __len__(self):\n","    return len(self.myitems)\n","\n","  def __getitem__(self, idx):\n","    return f'Sample{idx}', self.myitems[idx], None, 3.5\n","\n","dataset = AnotherDataset()\n","dataset[5]"],"metadata":{"id":"tBh3u8NDDsHu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["However, returning a dictionary makes your code more readable, and makes the creation of mini-batches (for SGD) way easier through the `DataLoader` (next chapter).\n","\n","⚠️ Do *not* return tensors that are stored on the GPU memory, as it [causes problems](https://pytorch.org/docs/stable/data.html#multi-process-data-loading) with the multiprocessing behavior of the DataLoader. There's a better way to achieve this via _memory pinning_, as we'll see further below."],"metadata":{"id":"ZAAwtyjuDuaY"}},{"cell_type":"markdown","source":["# PyTorch DataLoader"],"metadata":{"id":"tJYLOwwHD0ZA"}},{"cell_type":"markdown","source":["[`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) is an iterator that provides:\n","\n","- Data batching\n","- Data shuffling\n","- Parallel data loading using `multiprocessing` workers. Meaning that while the `GPU` is performing some computation on a batch, in parallel you can load the next batch.\n","\n","\n","Creating a dataloader from a dataset is straightforward. Here's an example that highlights some of the most used parameters:"],"metadata":{"id":"DPCY6t9OIIlV"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","toydataset = ToyDataset(200)\n","toyloader = DataLoader(toydataset,\n","                       batch_size=8,    # number of elements in each batch\n","                       shuffle=True,    # shuffle the dataset\n","                       num_workers=4,   # number of workers, i.e. batches to prefetch\n","                       pin_memory=True  # return memory-pinned tensors, see below for an explanation!\n","                       )\n","\n","# 200 iterations\n","for sample in tqdm(toydataset):\n","  pass\n","print(f\"SAMPLE: {sample}\")\n","\n","# 25 iterations\n","for batch in tqdm(toyloader):  # there is some overhead when using multiple workers!\n","  pass\n","print(f\"BATCH: {batch}\")"],"metadata":{"id":"NRMZjMSbIK5w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Did you notice how the batch samples were put together by the `DataLoader`? The batch is not simply a list of samples; rather, the data loader **collated** the samples into a dictionary with two keys (`x` and `y`) and populated these with tensors. This is why making your `DataSet` return dictionaries is useful!\n","\n","If your `DataSet` returns something else, you must manually specify *how* to put the samples together to form a batch. You do this by defining a custom collate function, and passing it to the `collate_fn` parameter of the `DataLoader` (see the [docs](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data))."],"metadata":{"id":"hO268fqFKUib"}},{"cell_type":"markdown","source":["Remember: a `DataSet` can be directly indexed:"],"metadata":{"id":"GRL5odDoIO0I"}},{"cell_type":"code","source":["toydataset = ToyDataset(20, noise=.1)\n","toydataset[2]"],"metadata":{"id":"LnTAhT3NL6d4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["But a `DataLoader` is an _iterator_ and can't be directly indexed:"],"metadata":{"id":"CP7qv8mJL8XW"}},{"cell_type":"code","source":["toyloader = DataLoader(toydataset, batch_size=8)\n","\n","try:\n","  toyloader[0]  # NOT OK\n","except Exception as e:\n","  print('Error:', e)\n","\n","print('')\n","\n","for b in toyloader:  # OK\n","  print(b)"],"metadata":{"id":"K41rZY4-IQVA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📖 Memory pinning\n","\n","Pytorch tensors support [memory pinning](https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/):\n","\n","![](https://devblogs.nvidia.com/wp-content/uploads/2012/12/pinned-1024x541.jpg)"],"metadata":{"id":"gw0aSqEMMg5I"}},{"cell_type":"markdown","source":["Pinned tensors enable:\n","- **Much faster copies** from CPU to GPU.\n","- **Aynchronous GPU copies**: *while* the tensor is being transferred, the CPU code continues if it doesn't need that tensor! To enable this, just pass an additional `non_blocking=True` argument to a `to()` or a `cuda()` call.  \n","\n","Note that differently from data transfer, GPU operations (e.g. tensor product) are [asynchronous by default](https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution).\n","\n","Let's see how to pin a tensor by hand:"],"metadata":{"id":"D7_3ChlkMw3k"}},{"cell_type":"code","source":["t = torch.rand(100)\n","t.is_pinned()"],"metadata":{"id":"95X99sm4MzZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# be sure to use a GPU runtime in Colab!\n","\n","t = t.pin_memory()  # reassigning to t, because pin_memory() is not in-place"],"metadata":{"id":"bo7YAOcUM0de"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t.is_pinned()"],"metadata":{"id":"d1SYyZ5zM1oq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From now on, whenever there is the necessity to transfer `t` from CPU to GPU, the transfer will be more efficient. In addition, if the transfer is done using the `non_blocking=True` option in a `to()` call, the transfer will happen asynchronously!\n","\n","Passing `pin_memory=True` to a `DataLoader` will automatically put the fetched data tensors in pinned memory. Note that `DataLoader` only knows how to pin standard types like Tensor, Map and Sequence of Tensors. If you want to pin some custom type, read more [here](https://pytorch.org/docs/stable/data.html#memory-pinning) (tldr: define a `pin_memory()` method on your custom type(s))."],"metadata":{"id":"PCXVf3ekCHUc"}},{"cell_type":"code","source":["batch['x'].is_pinned()"],"metadata":{"id":"0bIOObH1M42Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Potential bottlenecks\n","\n","In general, you want to make sure that data loading is not a bottleneck in your pipeline. **Your GPU must not wait for data**!\n","\n","- **Check resource usage**\n","\n","Check the GPU (or CPU) usage, if it is ~$100\\%$, it's being fully used. This is good!\n","\n","Otherwise you may have a bottleneck somewhere, or your data operations may not be GPU friendly (e.g. small batches).\n","\n","- **Check data loading speed**\n","\n","Iterate over the `Dataset` or `DataLoader` and check the data loading speed by counting the number of items that are loaded per second. Then compare this to how many items per second are processed by the rest of the pipeline.\n","\n","If you can load more items than you can process in the training loop, it means you _don't_ have a bottleneck in your data loading. The GPU is not waiting for you, good job!\n","\n","You can use the [tqdm](https://github.com/tqdm/tqdm) package to easily check the iteration speed of any iterable with a minimal overhead."],"metadata":{"id":"N79TidPQM7RP"}},{"cell_type":"code","source":["# Checking the loading speed\n","from tqdm.notebook import tqdm as tqdm\n","\n","toydataset = ToyDataset(20000)\n","toyloader = DataLoader(toydataset,\n","                       batch_size=8,\n","                       shuffle=True,\n","                       num_workers=4,\n","                       pin_memory=True\n","                       )\n","for batch in tqdm(toyloader):\n","  pass\n","\n","# Example:\n","# tqdm reports 350.00it/s (iterations per second) for the loader.\n","# tqdm reports 280.12it/s for the training step given the data.\n","# -> data loading is NOT a bottleneck!\n","#\n","# This scenario means your computation time is the primary factor in how long\n","# each iteration takes, and the data loading process is efficient enough to keep\n","# up with the computational demands."],"metadata":{"id":"n3JEAIwZYyZf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How to fix a bottleneck in the data loading?\n","\n","1) If your dataset fits on memory, load it beforehand.\n","\n","2) Tune the `num_workers` and `batch_size` parameter of the `DataLoader`, paying attention that the batch size will have a direct impact on the training. A good default for `num_workers` is the number of cores in your CPU.\n","\n","3) Do not preprocess data on the fly, but save the preprocessed files to disk.\n","\n","4) Consider changing the way in which the data files are stored on disk (e.g. another [format](https://www.h5py.org/) or a [database](https://github.com/google/leveldb))."],"metadata":{"id":"rPuLv1YlY4uz"}},{"cell_type":"markdown","source":["🌐 Keep in mind that the [`torchvision`](https://pytorch.org/vision/stable/index.html) package provides some common datasets and transforms. We'll use it in this notebook!"],"metadata":{"id":"G_CfboeXY-q5"}},{"cell_type":"markdown","source":["> **EXERCISE**\n",">\n","> Suppose you are creating a neural network to restore noisy images from the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. Given as input a corrupted image, the model will output a corresponding uncorrupted version.\n",">\n","> In this exercise, you are only concerned for the data loading steps.\n",">\n","> 1) Simulate the corruption by applying your favorite among: random pixel noise, random black patches, random crop, random reflections, or all the previous together.\n",">\n","> 2) Create the corresponding `Dataset` and `DataLoader`.\n",">\n","> 3) Plot the images in a **batch** to ensure you are doing everything right.\n",">\n",">  *hint*: you may use [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) and [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html)"],"metadata":{"id":"1S-hYuruZB8u"}},{"cell_type":"code","source":["# ✏️ your code here"],"metadata":{"id":"-kwRe8xWZDs8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Logistic regression with PyTorch\n","\n","We have seen how to do this with Scikit-learn, or by implementing everything by ourselves from scratch. Let's now do this with PyTorch. Why? Because it brings some interesting gimmicks along!"],"metadata":{"id":"ct6RMFgPFaO1"}},{"cell_type":"markdown","source":["## Data loading\n","\n","Instead of downloading the MNIST data and writing our own dataset code, we can use torchvision `datasets.MNIST`, which already inherits from torch `Dataset`, to do the job more quickly. Let's do that!"],"metadata":{"id":"aSRmH6uZF7pm"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","from torchvision import datasets, transforms\n","from tqdm import tqdm\n","\n","train_dataset = datasets.MNIST(\n","    './',\n","    train=True,\n","    download=True,\n","    transform=transforms.Compose([\n","        # tranforming images to pytorch tensors\n","        transforms.ToTensor(),\n","\n","        # normalizing the tensors, i.e. the distribution of values on each sample should have mean=0.1307 and stddev=0.3081,\n","        # corresponding to the mean and stddev of the whole MNIST dataset.\n","        # Check https://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current\n","        # for an intuition.\n","        transforms.Normalize((0.1307,), (0.3081,))\n","    ])\n",")\n","\n","test_dataset = datasets.MNIST('./', train=False,\n","                    transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1307,), (0.3081,))\n","                   ])\n","                )"],"metadata":{"id":"caZoqnHMGK_u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It is always a good idea to look at some entries in the dataset.\n","\n","**Note:** `train_dataset` and `test_dataset` are objects of the `torchvision.datasets.MNIST` class. Check the [docs](https://pytorch.org/vision/0.16/generated/torchvision.datasets.MNIST.html) to see how to use them."],"metadata":{"id":"2d211o58GQsW"}},{"cell_type":"code","source":["import plotly.express as px\n","\n","mnist_example = train_dataset[42][0][0].numpy()  # what are all those indices? investigate!\n","print('A MNIST sample has size', mnist_example.shape)\n","fig = px.imshow(mnist_example)\n","fig.update_layout(width=400, height=300)\n","fig.show()"],"metadata":{"id":"hirr6kcyGS5Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Is this a 1 or a 7?**\n","\n","This is the existential question we will try to answer today.\n","\n","Let's proceed by selecting only the 1 and 7 samples from the MNIST dataset."],"metadata":{"id":"BRsLLXIiHd46"}},{"cell_type":"code","source":["for dataset in [train_dataset, test_dataset]:\n","    mask_sevens = dataset.targets == 7\n","    mask_ones = dataset.targets == 1\n","\n","    # re-map 7s to have label 0 and 1s to have label 1\n","    dataset.targets[mask_sevens] = 0\n","    dataset.targets[mask_ones] = 1\n","\n","    # only keep 7s and 1s\n","    dataset.targets = dataset.targets[mask_sevens + mask_ones]\n","    dataset.data = dataset.data[mask_sevens + mask_ones]"],"metadata":{"id":"TZEkV3aLHpE4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's wrap the dataset in a pytorch DataLoader.\n","\n","Notice that by using `batch_size=len(dataset)`, each batch contains the entire dataset. This is not common, we are doing this here just because we don't really need smaller batches in this part of the notebook."],"metadata":{"id":"q8om0YQNHrZg"}},{"cell_type":"code","source":["train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)"],"metadata":{"id":"gWbRgqEOHw62"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**7 or 1**?\n","\n","How difficult is this task?\n","\n","It is always important to have an idea of how much *intelligence* you expect from your AI application.\n","\n",">**EXERCISE (warm-up)**: Combine several samples in a grid and plot a big picture. Then try to classify each one as a 1 or a 7, using your own judgement. What is your classification accuracy?"],"metadata":{"id":"A0bp3u8uIHFt"}},{"cell_type":"code","source":["# ✏️ your code here\n","# hint: use `torchvision.utils.make_grid` and `px.imshow`\n","\n","images = next(iter(train_dataloader))  # we only get one batch of data, since it contains the entire dataset\n","# ..."],"metadata":{"id":"sIzpj7_xIV-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 👀 Solution  { run: \"auto\" }\n","\n","cols = 4  #@param {type:\"slider\", min:1, max:8, step:1}\n","rows = 3 #@param {type:\"slider\", min:1, max:4, step:1}\n","\n","grid_images = images[0][:rows*cols, ...]\n","resolved_grid = torchvision.utils.make_grid(grid_images, padding=4, nrow=cols, normalize=True, value_range=(0, 1))\n","px.imshow(resolved_grid.permute(1, 2, 0))"],"metadata":{"cellView":"form","id":"NCHNlDzHIXEM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training\n","\n","What's PyTorch's counterpart for Scikit-learn's `fit()`?\n","\n","Before we find out, we are going to implement our own SGD, just like we did in the previous notebook -- however, this time around **we won't have to compute the gradient by hand**. Gradients will be computed automagically🪄 with the `backward()` method.\n","\n","We can reuse the `model` function from our previous notebook:"],"metadata":{"id":"t5qr0359Izq9"}},{"cell_type":"code","source":["def model(xb):\n","  return torch.nn.functional.log_softmax(xb @ weights + bias, dim=1)"],"metadata":{"id":"5gZ6fZ3FJ-0b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["See how we are using torch's implementation of `log_softmax`. This is convenient, since it also takes care of numerical errors that might arise when using exponential and logarithms (they did in my tests!). In fact, let's use torch's `nll_loss` as well, instead of our own implementation:"],"metadata":{"id":"zZWbSwYvaov_"}},{"cell_type":"code","source":["X_train = images[0].reshape(-1, 28*28)\n","y_train = images[1]\n","n_classes = 1 + y_train.max()\n","\n","weights = torch.rand(X_train.shape[1], n_classes)\n","bias = torch.rand(1, n_classes)\n","\n","preds = model(X_train)\n","torch.nn.functional.nll_loss(model(X_train), y_train)  # nll_loss wants log-probabilities as input"],"metadata":{"id":"0yn8tHFoamDD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE:** Adapt your NLL code from the previous notebook to work with torch tensors, and compare the loss value you get with the one obtained by torch `nll_loss`."],"metadata":{"id":"Jh2nNuUzbNOQ"}},{"cell_type":"markdown","source":["We only miss the gradient, and then we'll have all we need to start training with gradient descent. Let's restart by initializing the training with random weights and biases, this time with a small modification:"],"metadata":{"id":"7ZBYsteL0FT0"}},{"cell_type":"code","source":["weights = torch.rand((X_train.shape[1], n_classes), requires_grad=True)\n","bias = torch.rand((1, n_classes), requires_grad=True)"],"metadata":{"id":"5amkr2czNpbF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The parameter `requires_grad=True` is telling torch that later we will compute the gradient of some function (i.e. the loss) with respect to `weights` and `bias`. In other words, these will be leaves🍃 of a computational graph!\n","\n","Here's our implementation of gradient descent. Read the comments!"],"metadata":{"id":"Vu0XcpPL1WkC"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","lr = 0.01\n","max_iters = 200\n","losses = torch.zeros(max_iters)\n","\n","for it in tqdm(range(max_iters)):\n","\n","  # -> forward pass of autodiff\n","  loss = torch.nn.functional.nll_loss(model(X_train), y_train)\n","\n","  # <- backward pass of autodiff\n","  loss.backward()\n","\n","  # Note that loss is a rank-0 tensor (not a plain scalar).\n","\n","  # After executing backward(), the .grad attributes of weights ad bias\n","  # (which had requires_grad=True) contain the gradient of the loss wrt them.\n","\n","  # Clearly, we are not interested in computing the gradient *of the update operations themselves*.\n","  # It sounds obvious, but if we don't tell PyTorch, it will attempt to compute those derivatives!\n","  # We use the .no_grad() command to do so.\n","\n","  with torch.no_grad():  # Disable gradient computation since we are only updating the parameters\n","\n","    losses[it] = loss.item()\n","\n","    # downhill step\n","    weights -= lr * weights.grad\n","    bias -= lr * bias.grad\n","\n","    # reset the gradients. otherwise, the next backward() call will accumulate!\n","    weights.grad = None\n","    bias.grad = None\n","\n","plt.figure(figsize=(6, 3))\n","plt.plot(losses, color='black', label='GD')\n","plt.xlabel('epochs')\n","plt.ylabel('training loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"ac3spswo1TQ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE:** Implement SGD with mini-batches of size $m=50$ and add the SGD curve to the previous plot. Use `train_dataloader` to load the mini-batches, and remember to set the correct batch size."],"metadata":{"id":"JW4Oxk6Hzy0f"}},{"cell_type":"markdown","source":["### Optimizers\n","\n","While SGD is very effective, PyTorch also provides several alternative **optimizers** that turn out to be useful in many practical cases. We don't need to implement them as we did with SGD, but rather we can use the [`torch.optim`](https://pytorch.org/docs/stable/optim.html) library. We'll try the [Adam optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/), by many suggested as the default optimization method for deep learning:"],"metadata":{"id":"AIJBUhuC6e2g"}},{"cell_type":"code","source":["weights = torch.rand((X_train.shape[1], n_classes), requires_grad=True)\n","bias = torch.rand((1, n_classes), requires_grad=True)\n","\n","lr = 0.01\n","max_iters = 200\n","losses = torch.zeros(max_iters)\n","\n","adam = torch.optim.Adam([weights, bias], lr=lr)  # Instantiate the optimizer\n","\n","for it in tqdm(range(max_iters), desc='Training'):\n","\n","  # ->\n","  loss = torch.nn.functional.nll_loss(model(X_train), y_train)\n","  # <-\n","  loss.backward()\n","\n","  # no need to manually update the gradients!\n","  adam.step()\n","  adam.zero_grad()\n","\n","  losses[it] = loss.item()\n","\n","plt.figure(figsize=(6, 3))\n","plt.plot(losses, color='black', label='Adam')\n","plt.xlabel('epochs')\n","plt.ylabel('training loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"i1XSJMcz8QFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE:**\n","> - Add _momentum_ to the previous optimization.\n","> - Compare with the `RMSprop` optimizer, with and without momentum.\n",">\n","> _Hint:_ Check the docs!"],"metadata":{"id":"VWU8hz2qB7X-"}},{"cell_type":"markdown","source":["### Learning rate scheduler\n","\n","When using some optimizers it may be useful to introduce a learning rate **decay** policy. This way, the learning rate will not be fixed for each step but will vary throughout the training epochs.\n","\n","Some optimizers do this on their own (e.g. Adam), while others leave everything in our hands (e.g. SGD)."],"metadata":{"id":"w3OMhSdpDF4t"}},{"cell_type":"markdown","source":["PyTorch provides some easy-to-use classes to manage the decay policy. The [`torch.optim.lr_scheduler`](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) package provides several methods to adjust the learning rate based on the number of epochs.\n","\n","Learning rate scheduling should be applied **after the optimizer’s update**; e.g., you should write your code this way:\n","\n","```python\n","scheduler = ...\n","for epoch in range(n_epochs):\n","    out = fn(...)\n","    out.backward()\n","    opt.step()\n","    opt.zero_grad()\n","    scheduler.step() # AFTER opt.step(): breaking change with PyTorch 1.1.0\n","```\n","\n","If you are also testing your model on a validation dataset during the training loop, this is how it should look:\n","\n","```python\n","scheduler = ...\n","for epoch in range(n_epochs):\n","    train(...)\n","    validate(...)\n","    scheduler.step()\n","```\n","\n","Examples of such policies are [`lr_scheduler.ExponentialLR`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR) and [`lr_scheduler.CosineAnnealingLR`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR).\n","\n","Let's see the `ExponentialLR` in action! This is a multiplicative decay where, at each epoch, the learning rate is multiplied by a `gamma` value (smaller than one)."],"metadata":{"id":"n2V3Ca2eDgef"}},{"cell_type":"markdown","source":["> **EXERCISE:**\n","> - Complete the following code.\n","> - Also _validate_ your training model, and plot two loss curves (training and validation) across the iterations.\n",">\n","> Remember that we already loaded the validation data in `test_dataset` and we have the `test_dataloader`."],"metadata":{"id":"kX3MLFgTEiBG"}},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ExponentialLR\n","from torch.optim import SGD\n","\n","weights = torch.rand((X_train.shape[1], n_classes), requires_grad=True)\n","bias = torch.rand((1, n_classes), requires_grad=True)\n","\n","lr = 0.01\n","max_iters = 200\n","\n","opt = SGD([weights, bias], lr=lr)\n","scheduler = ExponentialLR(opt, gamma=.8)\n","\n","for i in tqdm(range(max_iters), desc=\"Training\"):\n","    #\n","    # ✏️ your code here\n","    #\n","    pass"],"metadata":{"id":"0yD2bkFwD1DM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE:** Time to switch to the full MNIST dataset! Run your latest code on the complete MNIST (not just 1 and 7). Can you reach **>90%** accuracy?"],"metadata":{"id":"cnnE5pgWHy7p"}},{"cell_type":"markdown","metadata":{"id":"OQteSeLHGq74"},"source":["# Autograd\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FkefLsEoJS4Y"},"source":["The `backward` call that computed the gradients for us in the previous parts of this notebook uses the ``autograd`` package. As you can imagine, it provides *automatic differentiation* for all operations on tensors.\n","\n","As we have seen in theory class, when we execute PyTorch operations on tensors, the framework is constructing a computational graph behind the curtains. The graph will then be used for *reverse-mode automatic differentation*, also called **backpropagation**."]},{"cell_type":"markdown","metadata":{"id":"IqrAmmOeSx_t"},"source":["## Basics\n","\n","Let's start by defining a tensor $x$ that may appear in some computation like $f(x) = x^2 + x^3$.\n","\n","Suppose we want to calculate its derivative at the point $x=42$:\n","$$\\frac{\\partial f}{\\partial x}\\Bigr\\rvert_{x=42}$$"]},{"cell_type":"code","metadata":{"id":"A2WafGqiQZSA"},"source":["x = torch.tensor(42., requires_grad=True)  # we'll compute the gradient w.r.t. this variable!\n","x2 = x ** 2\n","x3 = x ** 3\n","f = x2 + x3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DrM2IBJBU9BQ"},"source":["Now the backward pass, where we'll appreciate the *automatic* part of the differentation: you just need to call the `backward()` method from your output $f$.  "]},{"cell_type":"code","metadata":{"id":"yPzpQ5yaRlRm"},"source":["f.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-tFl09aaEgE"},"source":["Now you have $\\frac{\\partial f}{\\partial x}\\Bigr\\rvert_{x=42}$ in the `grad` of $x$"]},{"cell_type":"code","metadata":{"id":"3TYHvfmESIRG"},"source":["x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WJgwUuySJtC"},"source":["2 * 42 + 3 * 42**2  # Yep, it's correct."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juLqy9vAbMeA"},"source":["This is enough for training very standard models on PyTorch.\n","\n","Nevertheless, the design principles behind the PyTorch `autograd` package are not always as straightforward. For instance, what do you think will happen executing `backward()` a second time?\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"IjrsmwFblEVe"},"source":["# f.backward()  # try!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"222khA6jlBXI"},"source":["\n","To fully understand the world of the `Autograd` package we must go deeply down the rabbit hole.\n","\n","You do not need to get at the first pass everything we are going to mention from now on. There are explanations of advanced concepts and some PyTorch internals, which are usually not needed but can be useful (e.g. in debugging or complex implementations).\n","\n","Feel free to refer back to this notebook when needed!"]},{"cell_type":"markdown","metadata":{"id":"EKmBMm97mwim"},"source":["## Aggressive buffer freeing"]},{"cell_type":"markdown","metadata":{"id":"Q6_8luL9oo8U"},"source":["### The second backward\n","So, what was the problem with the second backward?\n","\n","When we computed the first `backward()`, the intermediate variables needed for the computation of $f$, as well as its gradient, were freed to save memory. So PyTorch does not have the necessary information to do backward from $f$ a second time.\n","\n","`Autograd` has an aggressive buffer freeing policy to be very memory efficient!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ytUCW4Kzpw-V"},"source":["If you want to prevent this, you can use `.backward(retain_graph=True)`.\n","\n","Let's redo from scratch the previous computation:"]},{"cell_type":"code","metadata":{"id":"ER5iDz7wj-hf"},"source":["x = torch.tensor(42., requires_grad=True)\n","x2 = x ** 2\n","x3 = x ** 3\n","f = x2 + x3\n","f.backward(retain_graph=True)\n","f.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jeS9KlJkzriT"},"source":["So we did backward two times. Let's check again the gradient of $x$:"]},{"cell_type":"code","metadata":{"id":"Oxy1Zgm6-5ne"},"source":["x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uv-d0Akf-7T2"},"source":["It's doubled!\n","\n","The reason is that `Autograd` keeps accumulating into the `grad` attribute. This means that multiple `backward()` calls **will sum up previously computed gradients** if they are not explicitly zeroed out."]},{"cell_type":"markdown","metadata":{"id":"5rg0ZSrmcAlB"},"source":["### Intermediate gradients are not kept by default\n","Intermediate gradients are other victims of PyTorch's aggressive buffer freeing policy.\n","\n","We do not have access to the gradient with respect to $x_2$, even if we actually computed it to calculate the one with respect to $x$."]},{"cell_type":"code","metadata":{"id":"JF65AN8eq1cR"},"source":["x2.requires_grad  # we *require* the grad w.r.t. x2, in order to compute the one w.r.t. x..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rXGqM4XhOyD"},"source":["x2.grad is None  # ...but we had asked Pytorch to only compute the gradient w.r.t. x, so the one wrt x2 is not maintained in memory!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Did you read the user warning up there? That should already give you an intuition of what a _leaf_ tensor is 🍃!"],"metadata":{"id":"cJM-ZS0V1Wda"}},{"cell_type":"markdown","metadata":{"id":"v6dqtbeo_3BO"},"source":["### Sick of being tracked? 🍪\n","\n","You can call `detach()` to **remove a tensor from the computational graph**. This means that the tensor will _not_ be used for computing the gradient and will not partake to the chain rule.\n","\n","We saw one example earlier in this notebook, where we were implementing gradient descent by ourselves and we didn't want to compute gradients of the descent steps! Another classical example is when you run a trained model just for inference, which means you already know you won't call `backward()` at all."]},{"cell_type":"code","metadata":{"id":"h-DpCaLcESlq"},"source":["x = torch.tensor(42., requires_grad=True)\n","x2 = x ** 2\n","x2sig = x2\n","print(x2sig.requires_grad)\n","x2nog = x2.detach()\n","print(x2nog.requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Of course, if a tensor is `detach()`ed, a gradient won't be computed for it and thus `requires_grad` will be `False`."],"metadata":{"id":"qE5ODGMpWx5B"}},{"cell_type":"markdown","source":["As a \"blanket solution\", you can also wrap the code block in a context `with torch.no_grad()`. This is equivalent to calling `detach()` everywhere:"],"metadata":{"id":"WRbY27Uu3qJP"}},{"cell_type":"code","metadata":{"id":"4PDmghK0BJqe"},"source":["x = torch.tensor(42., requires_grad=True)\n","x2 = x ** 2\n","print(x2.requires_grad)\n","with torch.no_grad():\n","    x2nog = x ** 2\n","    x3nog = (x2 + 7) ** 3\n","    print(x2nog.requires_grad)\n","    print(x3nog.requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`.no_grad()` is particularly useful for inference, when you are certain that you won't ever call `.backward()`."],"metadata":{"id":"2PgGv86R40Fs"}},{"cell_type":"markdown","metadata":{"id":"pln_4Rgv5Vek"},"source":["Clearly, you won't be able to backpropagate trough a detached tensor because it was removed from the graph:"]},{"cell_type":"code","metadata":{"id":"T3gojG2x5daC"},"source":["try:\n","  x2nog.sum().backward()\n","except Exception as e:\n","  print(e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLcCGIFQ5ozR"},"source":["# backward() still works for the tensor that we didn't detach:\n","x2sig.sum().backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ZH_ZFvsUjHx"},"source":["## Tensors 🎲\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_jnsJMh1D1aI"},"source":["``torch.Tensor`` is the central class of the `autograd` package.\n","\n","\n","In order to understand in detail how autograd works, it is necessary to dissect some of the most relevant attributes of the Tensors:\n","\n","---\n","\n","- **`data`**:\n","\n","It is the data stored in the tensor. Usually you do not need to access directly this attribute."]},{"cell_type":"code","metadata":{"id":"M0lm9gI0CXMs"},"source":["t = torch.rand(4, 4)\n","t.data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UupbCTeMCWo7"},"source":["\n","---\n","\n","- **`requires_grad`**:\n","\n","  - If `True`, the gradient with respect to this tensor will be computed.\n","  - If `True` and the tensor is a _leaf_, the gradient will also be saved in the `.grad` attribute.\n","  - If `False`, the gradient with respect to this tensor will _not_ be computed."]},{"cell_type":"code","source":["x = torch.tensor(42., requires_grad=True)\n","x2 = x ** 2\n","x3 = x ** 3\n","f = x2 + x3\n","\n","x.requires_grad, x2.requires_grad, x3.requires_grad, f.requires_grad"],"metadata":{"id":"irilpjS-ZI8O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note how all the tensors involved in the computation above have `requires_grad=True`. This means that a gradient will be computed for all of them; these intermediate gradients are all needed by the chain rule, when we will compute `f.backward()`!\n","\n","You can't force any of the intermediate tensors to _not_ have their gradient computed, because this would break the computation of the entire gradient from `f` back to `x`."],"metadata":{"id":"HY2WzdIMblUe"}},{"cell_type":"code","source":["x = torch.tensor(42., requires_grad=True)\n","x2 = x ** 2\n","x3 = x ** 3\n","try:\n","  x3.requires_grad = False\n","except RuntimeError as e:\n","  print(f\"Error: {e}\")\n","f = x2 + x3\n","f.backward()"],"metadata":{"id":"b4_fYQnpcJbh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VaaoNH1aCwDO"},"source":["---\n","\n","- **`grad`**:\n","\n","This attribute is `None` by default; it actually becomes a Tensor when `backward()` is called. The attribute will then contain the computed gradient, and future calls to `backward()` will accumulate (add) gradients into it. Only the leaf nodes of the computational graph with `requires_grad=True` will have the `grad` attribute populated.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uJoahnb9Es0G"},"source":["\n","---\n","\n","- **`grad_fn`**:\n","\n","The backward function that `autograd` will use to use to compute the gradient. For example, if we sum two tensors during the forward pass, then the `grad_fn` attribute of the result will indicate that it was created as a result of an addition operation."]},{"cell_type":"code","metadata":{"id":"XwTMO09rE8Sr"},"source":["t3 = x + x2\n","t3.grad_fn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When we call `backward()` on a tensor, PyTorch will traverse the computational graph from the tensor backward to its inputs, using these `grad_fn` functions to calculate gradients along the way."],"metadata":{"id":"aIP8RYBgF15W"}},{"cell_type":"markdown","metadata":{"id":"qw3kKSuSE7_m"},"source":["\n","---\n","\n","- **`is_leaf`**: a boolean. You can _not_ set it, this is read-only.\n","\n","🍃 **Only *leaf* tensors with `requires_grad=True` will have their `grad` populated during a call to `backward()`**. To get `grad` populated for non-leaf tensors, you can use `retain_grad()`.\n","Keep in mind that:\n","  - All tensors that have `requires_grad=False` will be leaf tensors by default.\n","  - For tensors that have `requires_grad=True`, they will be leaf tensors if their `grad_fn` is `None`. This means that they are not the result of an operation of tracked tensors, but rather they were created directly by the user."]},{"cell_type":"markdown","source":["**NOTE:** Make sure you are on a GPU runtime before running the following."],"metadata":{"id":"DGspX1szHSjE"}},{"cell_type":"code","metadata":{"id":"0xWCeqfp-A1h"},"source":["a = torch.rand(10, requires_grad=True)\n","a.is_leaf, a.requires_grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = torch.rand(10, requires_grad=True) + 2\n","a.is_leaf, a.requires_grad  # was created by the addition operation"],"metadata":{"id":"NnIZtOePHNzc"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BU6DUjE0-PBL"},"source":["a = torch.rand(10, requires_grad=True, device=\"cuda\")\n","a.is_leaf, a.requires_grad  # requires grad, directly created by the user"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-v0q8vXh-NY1"},"source":["a = torch.rand(10).cuda()\n","a.is_leaf, a.requires_grad  # requires_grad=False, thus it is a leaf by default"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = torch.rand(10, requires_grad=True).cuda()\n","a.is_leaf, a.requires_grad  # Was created by the operation that casts a cpu tensor into a cuda tensor.\n","                            # Since we are moving a cpu tensor that requires gradients, this is creating a new version of the tensor in GPU.\n","                            # Therefore 'a' is not a leaf, but the cpu tensor was."],"metadata":{"id":"EOlhAcaHIXPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wi8mQKs1-OWY"},"source":["a = torch.rand(10).cuda().requires_grad_()  # Here we move a cpu tensor that does not require gradients, so it stays a leaf, and then modify it.\n","a.is_leaf, a.requires_grad  # requires gradients and has `grad_fn=None`"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NZgZvzGRMv2E"},"source":["a = torch.rand(10, requires_grad=True, device=\"cuda\")\n","b = a + 2                          # non leaf, since requires grad and it is produced by an operation\n","print(b.is_leaf, b.requires_grad)\n","c = b.detach()                     # leaf, it has been detached and now has requires_grad=False\n","print(c.is_leaf, c.requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRUeDcG1n2oJ"},"source":["---\n","\n","- **`backward()`**:\n","\n","Computes the gradient of current tensor w.r.t. computational graph leaves."]},{"cell_type":"markdown","source":["> 🧠 **MEMO**: Remember, the graph is created on the fly during the forward pass, as operations are performed on tensors. When you call `backward()` on the final tensor (usually the rank-0 tensor representing the loss value), Pytorch traverses the computational graph back to the leaf tensors (usually the network parameters), calculating the gradient with respect to them and storing it in their `.grad` attribute."],"metadata":{"id":"knA4ybp1Port"}},{"cell_type":"markdown","metadata":{"id":"apkyGbXEbu9F"},"source":["> ### Leaves recap\n",">\n","> Let's recap the answer to the following question:\n",">\n","> *What are the nodes that will have the `.grad` attribute populated?*\n",">\n","> Here's a computational graph:\n",">\n","> ![](https://raw.githubusercontent.com/erodola/DLAI-s2-2021/main/labs/05/pics/leaves.svg)\n",">\n","> 1. Take the subgraph of nodes with `requires_grad=True` *(green and blue nodes)*\n","> 2. Take the leaves of this subgraph *(green nodes)*\n",">\n","> The nodes selected with this procedure *(green nodes)* will have their `.grad` attribute populated."]},{"cell_type":"markdown","metadata":{"id":"riJ-TeznY5Pr"},"source":["## Gradients\n","\n","Let's look at one last example."]},{"cell_type":"markdown","metadata":{"id":"bm5WbGkZX2om"},"source":["Create a tensor and set ``requires_grad=True`` to track operations:"]},{"cell_type":"code","metadata":{"id":"fgnwModnX_Li"},"source":["x = torch.ones(2, 2, requires_grad=True)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wC7rds5YYBqF"},"source":["Do some operation:\n"]},{"cell_type":"code","metadata":{"id":"B0fTL53zYIKv"},"source":["y = x + 2\n","y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwdL-EHbYGZh"},"source":["``y`` was created as a result of a tracked operation, so it has a ``grad_fn``:\n","\n"]},{"cell_type":"code","metadata":{"id":"HJo_kWbbYOJE"},"source":["y.grad_fn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3arTzZIvYQKS"},"source":["Do more operations on `y`:"]},{"cell_type":"code","metadata":{"id":"3KxkovOQYT5Y"},"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z, out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x0A7egnAY9i2"},"source":["out.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uWAp289WZIpS"},"source":["With this operation we computed $\\frac{\\partial \\, \\text{out}}{\\partial \\, x}$ as well as all the intermediate partial derivatives, but the only one we can actually read is $\\frac{\\partial \\, \\text{out}}{\\partial \\, x}$:\n"]},{"cell_type":"code","metadata":{"id":"L5OxpRZxaSPG"},"source":["x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RAO08WUF3fWM"},"source":["Let's double-check why `x.grad` is a `2x2` tensor full of $4.5$.\n","\n","The output is defined as:\n","\n","$$ \\mathrm{out} = \\frac{1}{4} \\sum_i 3(x_i + 2)^2 \\: \\text{ with } x_i = 1 \\, \\forall i$$\n","\n","We have the partial derivatives:\n","\n","$$\n","\\frac{\\partial \\mathrm{out}}{\\partial x_i}\n","= \\frac{3 \\times 2}{4} (x_i + 2)\n","= \\frac{3}{2} (x_i + 2)\n","$$\n","\n","*(Note: the derivative for every $x_j$ with $j \\neq i$ is zero)*\n","\n","\n","Thus, since $x_i=1$ for all $i$ in the input, we obtain $\\frac{\\partial \\mathrm{out}}{\\partial x_i} = \\frac{9}{2} = 4.5$."]},{"cell_type":"markdown","metadata":{"id":"StzZE8g8KNgI"},"source":["> **EXERCISE:**\n",">\n","> Understanding if a tensor is a leaf or not is suprisingly tricky, but it is very important to be able to distinguish leaf tensors: **only leaves with `requires_grad=True` tensors will have the grad attribute populated**. The leaves will be the parameters of our neural networks.\n",">\n","> Consider the two following scenarios and try to understand if `a.grad` and/or `b.grad` will be populated.\n",">\n","> **Scenario 1**\n",">\n","> ```python\n","> a = torch.randn(2, 2, requires_grad=True)\n","> b = a ** 2                                \n","> b.requires_grad_(True)                    \n","> b.sum().backward()                        \n","> ```\n","> - [ ] `a.grad` is populated (it is not `None`)\n","> - [ ] `b.grad` is populated (it is not `None`)\n",">\n",">\n","> **Scenario 2**\n",">\n","> ```python\n","> a = torch.randn(2, 2, requires_grad=False)\n","> b = a ** 2                                \n","> b.requires_grad_(True)                    \n","> b.sum().backward()                        \n","> ```\n","> - [ ] `a.grad` is populated (it is not `None`)\n","> - [ ] `b.grad` is populated (it is not `None`)"]},{"cell_type":"code","metadata":{"id":"OVGyQnomGpqG","cellView":"form"},"source":["# @title Solution 👀\n","\n","if False:  # Change to true to enable the prints\n","  # 1)\n","  a = torch.randn(2, 2, requires_grad=True)  # leaf tensor that requires grad\n","\n","  b = a ** 2                                 # non leaf tensor: requires grad and produced by an op\n","  b.requires_grad_(True)                     # it already requires a grad!\n","\n","  print(f'a.is_leaf: {a.is_leaf} \\t a.requires_grad: {a.requires_grad}  \\t a.grad_fn: {a.grad_fn}')\n","  print(f'b.is_leaf: {b.is_leaf} \\t b.requires_grad: {b.requires_grad}  \\t b.grad_fn: {b.grad_fn}')\n","\n","  b.sum().backward()                         # just a sample backprop\n","\n","  print(\"\\nGradients:\")\n","  print(f'a.grad: {a.grad}')                 # a is a leaf, thus it will have .grad\n","  print(f'b.grad: {b.grad}')                 # b is not a leaf, thus it will not have .grad\n","\n","  print('\\n\\n---\\n\\n')\n","\n","  # 2)\n","  a = torch.randn(2, 2, requires_grad=False) # leaf tensor that does not requires grad\n","\n","  b = a ** 2                                 # leaf tensor, because not requires grad\n","  b.requires_grad_(True)                     # now it requires a grad and has grad_fn=None! It is a leaf\n","\n","  print(f'a.is_leaf: {a.is_leaf} \\t a.requires_grad: {a.requires_grad}  \\t a.grad_fn: {a.grad_fn}')\n","  print(f'b.is_leaf: {b.is_leaf} \\t b.requires_grad: {b.requires_grad}  \\t b.grad_fn: {b.grad_fn}')\n","\n","  b.sum().backward()                         # just a sample backprop\n","\n","  print(\"\\nGradients:\")\n","  print(f'a.grad: {a.grad}')                 # a is a leaf but does not require grad, thus it will not have .grad\n","  print(f'b.grad: {b.grad}')                 # b is a leaf and requires grad, thus it will have .grad\n","\n","  print('\\n\\n---\\n\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"30r6CUdnjVXq"},"source":["> **EXERCISE:**\n",">\n","> Consider the following expression:\n",">\n","> $$ z = \\frac{\\sqrt{x^2 +1} - \\sqrt{y - 1}}{\\sqrt{x^2 + y^2}} + \\sqrt{y - 1} $$\n",">\n","> Compute the gradients $\\frac{\\partial z}{\\partial x}$, $\\frac{\\partial z}{\\partial y}$, $\\frac{\\partial z}{\\partial \\sqrt{x^2 +1}}$ and $\\frac{\\partial z}{\\partial \\sqrt{y-1}}$ at $x=2$, $y=10$"]},{"cell_type":"code","metadata":{"id":"Qlfee_OzDrM2"},"source":["# Expected results, respectively:\n","# x.grad: 0.08914636820554733\n","# y.grad: 0.15752650797367096\n","# x3.grad: 0.0980580672621727\n","# y2.grad: 0.9019419550895691\n","\n","# ✏️ your solution here\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"desrsm7KWlw2"},"source":["## Autograd Mechanics 🧑‍🔧\n","\n"]},{"cell_type":"markdown","metadata":{"id":"htzNwsSH9_7O"},"source":["### Custom `Function` 📖\n","\n","Look at this simple example:\n"]},{"cell_type":"code","source":["t = torch.rand(4, 4, requires_grad=True)\n","t2 = torch.rand(4, 4)\n","\n","t3 = t + t2\n","t3.grad_fn"],"metadata":{"id":"AAV-HcBfwoqp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That `AddBackward0` is an object of the `Function` class. It indicates that `t3` was created by a sum operation, but not only! Together with the `Tensor` class, `Function` makes up the graph that encodes a complete history of computation.\n","\n","All mathematical operations in PyTorch are implemented as objects of the `torch.nn.Autograd.Function` class."],"metadata":{"id":"B61GCHoGw6xO"}},{"cell_type":"markdown","source":["📜 **Story time**\n","\n","Once upon a time, we needed to backpropagate through the operation `lambda = eig(X)`, which computes the eigenvalues of a matrix `X`. But the `eig()` operation was not a `Function`! 😱\n","\n","So we implemented our own `Function` and defeated the evil derivative.\n","\n","**Good ending!** Our heroes make their way directly into the sun 🌅.\n"],"metadata":{"id":"dV1k-llJwnqI"}},{"cell_type":"markdown","source":["Our heroes had to implement these two methods:\n","\n","- `forward()`: the code that performs the operation. It can take as many arguments as you want. All Python objects are accepted as input. _Any input of the `Tensor` type should be explicitly `detach()`ed inside the `forward()` call, so that whatever happens inside the function will not affect the computational graph_; recall that we are going to manually implement the gradient anyway! You can return either a single `Tensor` or a tuple of `Tensor`. Refer to the docs of `Function` to find descriptions of useful methods that can be called only from `forward()`.\n","\n","- `backward()`: gradient formula. The size of its input matches the size of `forward()`'s output. It should return as many `Tensor` s as there were inputs in `forward()`, with each of them containing the gradient w.r.t. its corresponding input. If your inputs didn't require a gradient (`needs_input_grad`, in the `ctx` argument, is a tuple of booleans indicating whether each input needs gradient computation), or were non-Tensor objects, you can return `None`. Also, if you have optional arguments to `forward()` you can return more gradients than there were inputs, as long as they're all `None`.\n","\n","Confused? Let's see an example.\n"],"metadata":{"id":"0PPJADKoAin2"}},{"cell_type":"markdown","source":["We are going to implement our own ReLU from scratch.\n","\n","$$f(x) = \\max \\{0, x \\} $$\n","\n","The _forward_ pass is easy to implement: just write the operation above, and return the result. We'll also need the value of $x$ for computing the derivative $\\frac{\\partial f}{\\partial x}$, so `forward()` must save $x$ for later use. Scroll down to see how we implemented the forward.\n","\n","The _backward_ pass is a bit more tricky. Reverse-mode autodiff requires us to compute the _derivative of the **loss** with respect to $x$_:\n","\n","$$ {\\color{blue}{\\frac{\\partial\\ell}{\\partial x}}} = {\\color{green}{\\frac{\\partial \\ell}{\\partial f}}} {\\color{red}{\\frac{\\partial f}{\\partial x}}} $$\n","\n","In particular, `backward()` will receive ${\\color{green}{\\frac{\\partial \\ell}{\\partial f}}}$ as input, and must produce ${\\color{blue}{\\frac{\\partial\\ell}{\\partial x}}}$ in the output. All we must do is compute the portion:\n","\n","$${\\color{red}{\\frac{\\partial f}{ \\partial x}}} =  \\begin{cases} 1 & \\text{if } x > 0\\\\ 0 & \\text{if } x \\le 0 \\end{cases}$$\n","\n","and simply output the product ${\\color{green}{\\frac{\\partial \\ell}{\\partial f}}} {\\color{red}{\\frac{\\partial f}{\\partial x}}}$. Note how, as promised, we are also using $x$ for this calculation.\n","\n"],"metadata":{"id":"4OB-EEqJ0fUX"}},{"cell_type":"code","metadata":{"id":"5YNiK9zZw7JL"},"source":["class MyReLU(torch.autograd.Function):\n","    \"\"\"\n","    We can implement our own custom autograd Functions by subclassing\n","    torch.autograd.Function and implementing the forward and backward passes\n","    which operate on Tensors.\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, x):\n","        \"\"\"\n","        In the forward pass we receive a Tensor containing the input and return\n","        a Tensor containing the output. ctx is a \"context\" object that can be used\n","        to stash information for backward computation. You can cache arbitrary\n","        objects for use in the backward pass using the ctx.save_for_backward method.\n","        \"\"\"\n","        ctx.save_for_backward(x)\n","\n","        # The operation we do here can be even external to PyTorch, like playing a Mario🥸 level and recording the final score.\n","        # We're going simple here: let's implement a standard ReLU.\n","        x_device = x.device\n","        x_dtype = x.dtype\n","        xnumpy = x.cpu().detach().numpy()  # detach() ensures that operations done here do not interfere with the autograd\n","        xnumpy = xnumpy.clip(min=0)\n","\n","        return torch.tensor(xnumpy, dtype=x_dtype, device=x_device)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        \"\"\"\n","        In the backward pass we receive a Tensor containing the gradient of the loss\n","        with respect to the output, and we need to compute the gradient of the loss\n","        with respect to the input.\n","        \"\"\"\n","        input, = ctx.saved_tensors  # unpack the tuple to its only element\n","\n","        grad_input = torch.zeros_like(grad_output)\n","        grad_input[input > 0] = 1\n","        grad_input *= grad_output\n","\n","        # Alternatively, to avoid the element-wise product:\n","        # grad_input = grad_output.clone()  # deep copy\n","        # grad_input[input <= 0] = 0\n","\n","        return grad_input\n","\n","myrelu = MyReLU.apply  # not really needed, but useful to have an alias for future use"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test this out:"],"metadata":{"id":"d9jLZRig98Dn"}},{"cell_type":"code","metadata":{"id":"VIFmHOz00J1v"},"source":["x = torch.rand(50, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sotv6iQXFZa9"},"source":["out = myrelu(x - 0.5)\n","print(out)  # grad_fn=<MyReLUBackward>\n","out.sum().backward()\n","x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_WtEDM2FZqU"},"source":["x.grad.zero_()  # usually you should not use this method\n","\n","# -> Let's check our implementation against torch.relu\n","out = torch.relu(x - 0.5)\n","print(out)  # grad_fn=<MyReLUBackward>\n","out.sum().backward()\n","x.grad      # Negative numbers get zeroed, and their grad is zero"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE**\n",">\n","> Implement your own \"ReCU\", defined as:\n",">\n","> $$ f(x) = \\max \\{0, x^3\\} $$\n",">\n","> Write the `forward()` and `backward()` functions, and test them out."],"metadata":{"id":"vJyGzR7j9-yh"}},{"cell_type":"code","source":["# your solution here ✏️\n"],"metadata":{"id":"EYje40FnHVwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 👀 Solution\n","\n","class MyReCU(torch.autograd.Function):\n","\n","    @staticmethod\n","    def forward(ctx, x):\n","\n","        ctx.save_for_backward(x)\n","\n","        x_device = x.device\n","        x_dtype = x.dtype\n","        xnumpy = x.cpu().detach().numpy() ** 3\n","        xnumpy = xnumpy.clip(min=0)\n","\n","        return torch.tensor(xnumpy, dtype=x_dtype, device=x_device)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        input, = ctx.saved_tensors\n","        # no cloning necessary, since we are not modifying grad_output directly\n","        grad_input = grad_output * 3 * (input**2) * (input > 0).float()\n","        return grad_input\n","\n","myrecu = MyReCU.apply\n","\n","# testing\n","\n","x = torch.rand(50, requires_grad=True)\n","\n","out = myrecu(10 * x - 5)\n","print(out)\n","\n","out.sum().backward()\n","x.grad"],"metadata":{"cellView":"form","id":"P8lngSXB-UUj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fnFpy_nQ8OVc"},"source":["### Excluding subgraphs from backward\n","\n","The `requires_grad` flag allows for fine-grained exclusion of subgraphs from gradient computation and can increase efficiency. As a reminder, if any input tensor of an operation has `requires_grad=True`, the output tensor automatically gets `requires_grad=True` as well."]},{"cell_type":"code","metadata":{"id":"6zeW6UGQ8dh1"},"source":["x = torch.randn(5, 5)  # requires_grad=False by default\n","y = torch.randn(5, 5)  # requires_grad=False by default\n","z = torch.randn((5, 5), requires_grad=True)\n","\n","a = x + y\n","b = a + z\n","\n","a.requires_grad, b.requires_grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ciywagdm8vxz"},"source":["Explicitly setting certain tensors to `requires_grad=False` is useful when you want to **❄️ freeze a subset of parameters of your model** so they are not updated during training. This would be done, for instance, to **finetune** the last layer of a pretrained CNN: simply set `requires_grad=False` for all the parameter tensors except the ones in the last layer.\n","\n","Let's do it:"]},{"cell_type":"code","metadata":{"id":"WKstBTYz81Mc"},"source":["import torchvision\n","model = torchvision.models.resnet18(pretrained=True)  # no need to understand this right now"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZ4f-QrzfpYQ"},"source":["# compute some random prediction from this pretrained network\n","random_prediction = model(torch.rand(2, 3, 224, 224))\n","\n","# dummy loss, just to get some gradients\n","f = random_prediction.sum()\n","\n","# compute the gradients\n","f.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model's parameters, together with the gradient of `f` with respect to them, are stored in..."],"metadata":{"id":"enwFdhlsk-Xh"}},{"cell_type":"code","source":["model.parameters()"],"metadata":{"id":"bRgpZtsdljbJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For example, we can look for all the parameters having a nonzero gradient (based on our dummy loss function):"],"metadata":{"id":"7-jCJH60lnTH"}},{"cell_type":"code","metadata":{"id":"6g8KRjZZe-sI"},"source":["grads = list(x.grad for x in model.parameters() if x.grad.bool().any())\n","len(grads)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's now freeze the pretrained model except for the last layer:"],"metadata":{"id":"NKUefHUgmC3N"}},{"cell_type":"code","metadata":{"id":"MUl9vXuve-ZM"},"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","# Clear the previous gradients to avoid undue accumulation later\n","model.zero_grad()\n","\n","# Freeze the pretrained model\n","for param in model.parameters():\n","    param.requires_grad = False  # you can do this, because they are all leaves!\n","\n","# Replace the last fully-connected layer\n","# These parameters have requires_grad=True by default\n","model.fc = nn.Linear(512, 100)\n","\n","# Configure an optimizer for the last layer only.\n","# NOTE: we don't actually optimize, this is just to show you how we would setup the training.\n","optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vt6ZCLWl9aIG"},"source":["### In-place operations\n","\n","From our discussion so far, you might suppose that in-place operations on Pytorch tensors can potentially **overwrite values required to compute gradients**. This is true: with an in-place operation, we may break the backpropagation mechanism.\n","\n","Here's an example:\n","\n","```python\n","x = torch.rand(5, requires_grad=True)\n","y = x * 2\n","y.add_(torch.sqrt(y * x))\n","```\n","\n","What happens to the internal attributes of `y` as we keep overwriting it?\n","\n","Each in-place operation actually rewrites the computational graph. This can be tricky, especially if there are many `Tensors` that reference the same storage (e.g. created by indexing or transposing), and in-place functions will actually raise an error if the storage of modified inputs is referenced by any other `Tensor`. In contrast, **out-of-place versions simply allocate new objects and keep references** to the old graph."]},{"cell_type":"markdown","metadata":{"id":"zejrsiY79wnT"},"source":["#### In-place correctness checks 📖\n","\n","Every tensor keeps a _version counter_, incremented each time the tensor is marked as \"dirty\" by an in-place operation. When a `Function` uses `save_for_backward()` to save references of any tensors for its backward pass, a version counter of their containing `Tensor` is saved as well. Once you access `self.saved_tensors`, the version is checked. If it is greater than the saved value, an error is raised. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct."]},{"cell_type":"code","metadata":{"id":"4ZSN9j7uNhUF"},"source":["x = torch.rand(10, requires_grad=True)\n","o = x * 10\n","o.retain_grad()\n","o2 = o + 10\n","o2.retain_grad()\n","y = torch.rand(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOoIoXCbONCY"},"source":["o._version  # the version counter is initialized to zero"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhC3kaUyOBKu"},"source":["o.add_(-1)  # dirty edit, increase the version counter\n","o._version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cb2vqqAfOBVs"},"source":["z = x + y  # it does not modify x in place\n","x._version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGWUFDNgOBbg"},"source":["x = x + x  # x is a new tensor\n","x._version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRrF9bMSKNTz"},"source":["# 😈 Let's break autodiff with in-place operations\n","\n","try:\n","  x = torch.ones(5, requires_grad=True)\n","  x2 = (x + 1).sqrt()\n","  z = (x2 - 10)\n","  x2[0] = -1\n","  z.sum().backward()\n","except Exception as e:\n","  print(e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3kVgPlPH7nug"},"source":["References:\n","\n","- [PyTorch docs](https://pytorch.org/docs/stable/index.html)\n","- [Autograd tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n","- [Autograd mechanics](https://pytorch.org/docs/stable/notes/autograd.html)\n","- [Extending PyTorch](https://pytorch.org/docs/stable/notes/extending.html)\n","- Nice [blogpost](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)\n","- Nice [blogpost](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95) number  two"]},{"cell_type":"markdown","metadata":{"id":"Xwkalx3GSpmG"},"source":["# The `torch.nn` package\n","\n","Finally, let's implement a **Deep Neural Network**, beyond the simple logistic regression model 🚀"]},{"cell_type":"markdown","source":["\n","PyTorch provides the elegantly designed modules and classes\n","[`torch.nn`](https://pytorch.org/docs/stable/nn.html),\n","[`torch.optim`](https://pytorch.org/docs/stable/optim.html),\n","[`Dataset`](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset),\n","and [`DataLoader`](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n","to help you create and train neural networks.\n","You have already seen how to use `torch.optim`, `Dataset` and `DataLoader`. In this section we will review all these classes together with the new `torch.nn` package to understand how they work together to simplify our life.\n","\n","To develop this understanding, we will first train a basic neural net on the MNIST dataset _without_ using any of these modules: we will just use the most basic PyTorch tensor functionality.\n","\n","---\n","\n","Our final goal is to reach an elegant, general structure suitable for most problems and models with minor tweaks:\n","\n","```python\n","# load data\n","# instantiate model\n","# instantiate optimizer\n","\n","# for each epoch:\n","  # train the model on the training set\n","  # evaluate the model on one or more evaluation sets\n","  # log metrics (e.g. accuracy)\n","```"],"metadata":{"id":"7gNTB1k5_zEn"}},{"cell_type":"markdown","metadata":{"id":"_osaceZPDLNq"},"source":["For the weights, we set `requires_grad` **after** the initialization, since we don't want the initialization function to be included in the gradient computation. (remember that a trailling `_` in PyTorch means that the operation is performed in-place.)\n","\n","We are initializing the weights with a simplified version of\n","[Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), i.e. by multiplying with $\\frac{1}{\\sqrt{n}}$\n"]},{"cell_type":"code","metadata":{"id":"QBGRtDolDOb7"},"source":["import math\n","import torch\n","\n","weights = torch.randn(784, 10) / math.sqrt(784)  # Xavier init.\n","weights.requires_grad_()                         # Start to track the weights\n","bias = torch.zeros(10, requires_grad=True)       # Initialize the bias with zeros"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1pMn2kbHEQ6X"},"source":["For these tests we are going to use the entire MNIST dataset, where each image has 784 pixels (check the size of `weights` above).\n","\n","We are also going to use PyTorch's implementations of the loss and activation functions. Previously we used `torch.nn.functional.log_softmax` and `torch.nn.functional.nll_loss`; we are now going to simplify this further:"]},{"cell_type":"code","metadata":{"id":"E8mrfSyALH7N"},"source":["import torch.nn.functional as F\n","\n","loss_func = F.cross_entropy  # log_softmax and nll_loss all in one, for better numerical stability!\n","\n","def model(xb):\n","  return xb @ weights + bias  # we don't explicitly apply log-softmax anymore"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VmFJ7hRMMFRL"},"source":["## Refactor: use `nn.Module`\n","\n","Next up, we'll use ``nn.Module`` and ``nn.Parameter``, for a clearer and more\n","concise training loop. We subclass ``nn.Module`` to create a class that\n","holds our weights, bias, and method for the forward step.  ``nn.Module`` has a\n","number of attributes and methods (such as ``.parameters()`` and ``.zero_grad()``)\n","which we will be using."]},{"cell_type":"code","metadata":{"id":"L2dUPFzqOKJQ"},"source":["from torch import nn\n","\n","class Mnist_Logistic(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n","    self.bias = nn.Parameter(torch.zeros(10))\n","\n","  def forward(self, xb):\n","    return xb @ self.weights + self.bias"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GjgeOoIIOK8Y"},"source":["Since we're now using an object instead of just using a function (our old `model()`), we first have to instantiate our model:\n","\n"]},{"cell_type":"code","metadata":{"id":"QW-63GBxOOrM"},"source":["model = Mnist_Logistic()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ro2IHPQBOSu-"},"source":["Now we can calculate the loss in the same way as before. Note that\n","``nn.Module`` objects are used as if they are functions (i.e they are\n","*callable*), but behind the scenes Pytorch will call our ``forward``\n","method automatically.\n","\n","> The `__call__` method of the Modules, internally calls the `forward` method and *does other stuff* (e.g. registers some hooks, you can check the implementation [here](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module)). Thus, you should always call the forward with `model(inputs)` and never directly `model.forward(inputs)`."]},{"cell_type":"markdown","metadata":{"id":"qZIQ7m6zOWkp"},"source":["Previously in our training loop we had to update `weights` and `bias` explicitly and manually zero out the grads, like this:\n","\n","```python\n","  with torch.no_grad():\n","      weights -= weights.grad * lr\n","      bias -= bias.grad * lr\n","      weights.grad.zero_()\n","      bias.grad.zero_()\n","```\n","\n","Now we can take advantage of `model.parameters()` and `model.zero_grad()` (which are both defined by PyTorch for ``nn.Module``) to make these steps more concise and less prone to error:\n","\n","```python\n","  with torch.no_grad():\n","      for p in model.parameters(): p -= p.grad * lr\n","      model.zero_grad()\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"8s550An7OwDj"},"source":["## Refactor: use `nn.Linear`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MiDLxIksO6IU"},"source":["We continue to refactor our code.  Instead of manually defining and\n","initializing ``self.weights`` and ``self.bias``, and calculating ``xb  @\n","self.weights + self.bias``, we will instead use the Pytorch class\n","[`nn.Linear`](https://pytorch.org/docs/stable/nn.html#linear-layers) for a\n","linear layer, which does all that for us.\n","\n","Pytorch has many predefined layers that can greatly simplify our code, often making it faster too."]},{"cell_type":"code","metadata":{"id":"tS-fO2BUPCnk"},"source":["class Mnist_Logistic(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.lin = nn.Linear(784, 10)\n","\n","  def forward(self, xb):\n","    return self.lin(xb)\n","\n","model = Mnist_Logistic()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pIYrW4yJPKNG"},"source":["## Refactor: use `torch.optim`\n","\n","We already know the `torch.optim` package; we can use the ``step`` method to take a forward step, instead of manually updating each parameter.\n","\n","This will let us replace our previous custom optimization step:\n","\n","```python\n","with torch.no_grad():\n","  for p in model.parameters(): p -= p.grad * lr\n","  model.zero_grad()\n","```\n","\n","and instead use just:\n","```python\n","opt.step()\n","opt.zero_grad()\n","```\n","where `opt` can be any fancy optimizer."]},{"cell_type":"code","metadata":{"id":"3LQLPJOtPVMe"},"source":["from torch import optim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that we will **not replace the training loop**. The optimizer only helps us for a single step; we still have to run the training loop ourselves."],"metadata":{"id":"wUkm0SdXLm39"}},{"cell_type":"markdown","metadata":{"id":"G7O345TQQBDx"},"source":["## Refactor: use `Dataset` and `DataLoader`\n","\n","We already know this as well. PyTorch's [`TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)\n","is a `Dataset` that wraps tensors. This also gives us a way to easily iterate, index, and slice the dataset with a more compact code."]},{"cell_type":"code","source":["import numpy as np\n","\n","!wget https://s3.amazonaws.com/img-datasets/mnist.npz\n","\n","def load_data_impl():\n","    # file retrieved by:\n","    #   wget https://s3.amazonaws.com/img-datasets/mnist.npz -O code/dlgo/nn/mnist.npz\n","    # code based on:\n","    #   site-packages/keras/datasets/mnist.py\n","    path = 'mnist.npz'\n","    f = np.load(path)\n","    x_train, y_train = f['x_train'].reshape(-1, 784), f['y_train']\n","    x_test, y_test = f['x_test'].reshape(-1, 784), f['y_test']\n","    f.close()\n","    return (x_train.astype(np.float32), y_train), (x_test.astype(np.float32), y_test)\n","\n","(x_train, y_train), (x_valid, y_valid) = load_data_impl()\n","\n","x_train = (x_train / 255 - 0.13) / 0.3  # data normalization\n","x_valid = (x_valid / 255 - 0.13) / 0.3\n","\n","# Convert to PyTorch tensors\n","x_train, y_train, x_valid, y_valid = map(\n","  torch.tensor, (x_train, y_train, x_valid, y_valid)\n",")\n","y_train = y_train.long()  # PyTorch wants int64 as indices\n","y_valid = y_valid.long()\n","print(x_train, y_train)\n","print(x_train.shape)\n","print(y_train.min(), y_train.max())"],"metadata":{"id":"_p59i6Xuia1_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmy9HEaKRFSk"},"source":["Both ``x_train`` and ``y_train`` can be combined in a single ``TensorDataset``,\n","which will be easier to iterate over and slice:\n","\n"]},{"cell_type":"code","metadata":{"id":"da2JI5n7RHhW"},"source":["from torch.utils.data import TensorDataset\n","train_ds = TensorDataset(x_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EGh9MSe9RgRh"},"source":["``DataLoader`` can then provide us with mini-batches automatically.\n"]},{"cell_type":"code","metadata":{"id":"mxdaL230Ty0i"},"source":["from torch.utils.data import DataLoader\n","\n","bs = 64\n","train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KIydhfapTzjg"},"source":["Thanks to Pytorch's ``nn.Module``, ``nn.Parameter``, ``Dataset``, and ``DataLoader``, our training loop is now dramatically smaller and easier to understand:"]},{"cell_type":"code","metadata":{"id":"Zskh-TRNT_vl"},"source":["epochs = 3\n","lr = 0.5\n","\n","opt = optim.SGD(model.parameters(), lr=lr)\n","\n","for epoch in range(epochs):\n","\n","  for xb, yb in train_dl:\n","\n","    pred = model(xb)\n","    loss = loss_func(pred, yb)\n","\n","    loss.backward()\n","    opt.step()\n","    opt.zero_grad()\n","\n","print(loss_func(model(xb), yb))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jT_IS5bbUI83"},"source":["## Add: validation set\n","\n","As usual, we should use the validation set to check the quality of our model and potentially identify overfitting.\n","\n","**A note about shuffling:** Shuffling the training data is important to prevent correlation between batches and overfitting. On the other hand, the validation loss will be identical whether we shuffle the validation set or not. Since shuffling takes extra time and makes qualitative comparisons more difficult, _it makes no sense to shuffle the validation data_.\n","\n","Still, we'll build mini-batches for the validation set as well, for efficiency reasons (e.g. avoid a memory bottleneck of loading the entire validation set at once). We'll use a batch size for the validation set that is twice as large as\n","that for the training set, because it doesn't need to\n","store any gradients.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"gf1C5uehV5vN"},"source":["train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n","\n","valid_ds = TensorDataset(x_valid, y_valid)\n","valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WlTH6IbNV6Mt"},"source":["We will calculate and print the validation loss at the end of each epoch.\n","\n","In the code below, we also call `model.train()` before training, and `model.eval()` before inference; this will be required by layers such as ``nn.BatchNorm2d``\n","and ``nn.Dropout`` to ensure appropriate behavior, and it's good practice to do this always to be safe."]},{"cell_type":"code","metadata":{"id":"NOr_UhEWWMwB"},"source":["for epoch in range(epochs):\n","\n","  model.train()\n","\n","  for xb, yb in train_dl:\n","\n","    pred = model(xb)\n","    loss = loss_func(pred, yb)\n","\n","    loss.backward()\n","    opt.step()\n","    opt.zero_grad()\n","\n","  model.eval()\n","\n","  with torch.no_grad():\n","    valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl) / len(valid_dl)\n","\n","  print(epoch, valid_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Is the loss always going down? Try a few times!"],"metadata":{"id":"dcxliWu2Ove1"}},{"cell_type":"markdown","source":["> **EXERCISE:** Plot training and validation curves for our latest model."],"metadata":{"id":"XXWVACJltZKn"}},{"cell_type":"markdown","metadata":{"id":"kaJphSnyXDp6"},"source":["## 🎉 Our first MLP\n","\n","We are now going to build a deep network with two fully-connected layers. Let's start with a single layer:"]},{"cell_type":"code","metadata":{"id":"BEUjxfJJYGaa"},"source":["class Mnist_MLP(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.lin1 = nn.Linear(784, 10)\n","\n","  def forward(self, xb):\n","    xb = self.lin1(xb)\n","    return xb\n","\n","model = Mnist_MLP()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bs = 50\n","\n","train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n","\n","valid_ds = TensorDataset(x_valid, y_valid)\n","valid_dl = DataLoader(valid_ds, batch_size=128)"],"metadata":{"id":"7YcF0_Z7RY4Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQCB2rTFYH1V"},"source":["import matplotlib.pyplot as plt\n","\n","epochs = 20\n","lr = 0.01\n","\n","opt = optim.Adam(model.parameters(), lr=lr)\n","\n","valid_accuracy = torch.zeros(epochs)\n","\n","for epoch in range(epochs):\n","\n","  model.train()\n","\n","  for xb, yb in train_dl:\n","\n","    pred = model(xb)\n","    loss = loss_func(pred, yb)\n","\n","    loss.backward()\n","    opt.step()\n","    opt.zero_grad()\n","\n","  model.eval()\n","\n","  with torch.no_grad():\n","    valid_accuracy[epoch] = sum((model(xb).argmax(dim=1) == yb).int().sum() for xb, yb in valid_dl) / len(valid_ds)\n","\n","  if not epoch % 10:\n","    print(epoch, loss.item())\n","\n","plt.figure(figsize=(6, 3))\n","plt.plot(valid_accuracy, label='validation', color='red')\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.xlim(-1, epochs + 1)\n","plt.ylim(0.7, 1)\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","print(f\"final validation accuracy: {valid_accuracy[-1]*100:.2f}%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You should get something around ~88% accuracy on the validation set. Looks ok, but we can definitely do better. Instead of playing with parameters such as batch size, optimizer, learning rate, and so forth, let's do one simple modification: **change the network architecture**📐."],"metadata":{"id":"fpj4sQ1ceteo"}},{"cell_type":"markdown","source":["> **EXERCISE:** Now it's your turn to appreciate the power of _deep_ networks. Right now we have a one-layer network going from 784 (i.e. number of pixels per image) to 10 features (i.e. the class predictions). Add a layer to the previous network, such that the feature dimensions change as 784 → 512 → 10. Use ReLU as an activation function after the first layer. What validation accuracy do you reach?"],"metadata":{"id":"bfrlIeOmeCqD"}},{"cell_type":"markdown","metadata":{"id":"nMdlDhLCYQzW"},"source":["``torch.nn`` has another handy class we can use to simplify our code: [`Sequential`](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential). A ``Sequential`` object runs each of the modules contained within it, in a\n","sequential manner. This is a simpler way of writing a neural network.\n","\n","For example:"]},{"cell_type":"code","metadata":{"id":"PEwvSOXBaOSu"},"source":["model = nn.Sequential(\n","  nn.Linear(784, 100),\n","  nn.ReLU(),\n","  nn.Linear(100, 50),\n","  nn.Tanh(),\n","  nn.Linear(50, 10)\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["⚠️ `nn.Sequential` takes _modules_ as input. Therefore, we are passing the module `nn.ReLU` rather than the functional `F.relu`, and similarly for tanh."],"metadata":{"id":"-DsUn-Gejpdz"}},{"cell_type":"markdown","metadata":{"id":"F7FZjuoT6Z9E"},"source":["## Use: your GPU\n","\n","If you're lucky enough to have access to a CUDA-capable GPU, you can\n","use it to speed up your code. First check that your GPU is working in\n","Pytorch:\n","\n"]},{"cell_type":"code","metadata":{"id":"GApMyXwe6Z9F"},"source":["print(torch.cuda.is_available())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lmZr8ET_6Z9G"},"source":["And then create a device object for it:\n","\n"]},{"cell_type":"code","metadata":{"id":"E7YAOdXh6Z9H"},"source":["dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J6FQIUUy6Z9O"},"source":["We can then move our model to the GPU:\n","\n"]},{"cell_type":"code","metadata":{"id":"plAiM1d76Z9O"},"source":["model.to(dev)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7kbRyLkW6Z9Q"},"source":["Further, we should also move our data to the GPU, and then re-initialize the optimizer with the GPU-stored model. If we did everything correctly, the training should run faster!\n","\n"]},{"cell_type":"markdown","source":["> **EXERCISE:** Measure the training time of your MLP from the previous exercise. Then move it to the GPU and measure the training time again. What's the gain in performance?\n",">\n","> You can use python's builtin `time` module for timing."],"metadata":{"id":"aKfkkFBukxZJ"}}]}