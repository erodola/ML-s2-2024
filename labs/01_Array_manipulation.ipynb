{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["i51GvUnwSc0U"],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AhTm-wTJNJEJ"},"source":["# Machine Learning\n","\n","# Tutorial 1: Array manipulation\n","\n","In this tutorial, we will cover:\n","\n","- Multidimensional arrays (tensors) in NumPy\n","- Broadcasting\n","- Einsum\n","\n","Prerequisites:\n","\n","- Python\n","\n","Authors:\n","\n","- Prof. Emanuele RodolÃ \n","- Based in part on original material by Dr. Antonio Norelli and Dr. Luca Moschella\n","\n","Course:\n","\n","- Lectures and notebooks at https://erodola.github.io/ML-s2-2024/\n","\n"]},{"cell_type":"markdown","source":["## Welcome to the Machine Learning lab sessions!\n","During the lab sessions, you will be guided through one or more Python notebooks that teach you ML tools and provide opportunities to apply what you have learned in class.\n","\n","We encourage you to form small groups of 2-3 people to read and discuss the notebooks together.\n","\n","Run the code and play with it! It is very easy to edit the code locally and make small experiments. Try whatever comes to your mind, this is the best way to learn! Python notebooks are designed to be used in this way, that's why we chose them for the ML lab sessions.\n","\n","There will be some exercises, try to do them by yourself, and when everyone in your group has finished, compare the solutions with each other.\n","\n","When something is not clear or you have a question, raise your hand and we will come to you.\n","\n","Some sections in the notebooks are marked with ðŸ“–. This is deepening content for further reading outside of class. You may want to go through it at home or during class if you finish early. (Some sections are \"more optional\" than others, those are marked with more books ðŸ“–ðŸ“–)\n","\n","Let's start!"],"metadata":{"id":"PVQCoYhRizlI"}},{"cell_type":"markdown","metadata":{"id":"MDJI_JVTPMRc"},"source":["## Introduction\n","\n","Many ML frameworks have emerged for python, including **Scikit-learn**, **PyTorch** (focused on deep learning), and **Jax**.\n","We will use whichever framework is most appropriate, depending on the topic at hand.\n","\n","The fundamental data structure of these frameworks is the **multidimensional array** or **tensor**, which is more or less the same everywhere. _A solid understanding of how tensors work is required_ and will definitely come in handy in other areas.\n","\n","This tutorial will give you solid basics of tensors and operations between tensors."]},{"cell_type":"markdown","metadata":{"id":"5fCEDCU_qrC0"},"source":["## Wait, wait, wait... what is this strange web page with code and text cells all around?\n","\n","It is called Colab, an environment to play with python notebooks directly in your web browser, made by Google. If you never used Colab before, take a look to the following cells, adapted from the official [Colab guide](https://colab.research.google.com/notebooks/welcome.ipynb).\n","\n"]},{"cell_type":"markdown","source":["### Getting started with Colab\n","\n","Colab, or \"Colaboratory\", allows you to write and execute Python in your browser, with\n","- Zero configuration required\n","- Access to GPUs free of charge\n","- Easy sharing"],"metadata":{"id":"IYafT_OGiBAl"}},{"cell_type":"markdown","metadata":{"id":"GJBs_flRovLc"},"source":["The document you are reading is not a static web page, but an interactive environment called a **Colab notebook** that lets you write and execute code.\n","\n","For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJr_9dXGpJ05"},"outputs":[],"source":["seconds_in_a_day = 24 * 60 * 60\n","seconds_in_a_day"]},{"cell_type":"markdown","metadata":{"id":"2fhs6GZ4qFMx"},"source":["To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\". To edit the code, just click the cell and start editing.\n","\n","Variables that you define in one cell can later be used in other cells:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gE-Ez1qtyIA"},"outputs":[],"source":["seconds_in_a_week = 7 * seconds_in_a_day\n","seconds_in_a_week"]},{"cell_type":"markdown","metadata":{"id":"lSrWNr3MuFUS"},"source":["Colab notebooks allow you to combine **executable code** and **rich text** in a single document, along with **images**, **HTML**, **LaTeX** and more. When you create your own Colab notebooks, they are stored in your Google Drive account. You can easily share your Colab notebooks with co-workers or friends, allowing them to comment on your notebooks or even edit them. To learn more, see [Overview of Colab](/notebooks/basic_features_overview.ipynb). To create a new Colab notebook you can use the File menu above, or use the following link: [create a new Colab notebook](http://colab.research.google.com#create=true).\n","\n","Colab notebooks are Jupyter notebooks that are hosted by Colab. To learn more about the Jupyter project, see [jupyter.org](https://www.jupyter.org)."]},{"cell_type":"markdown","metadata":{"id":"_ptdrqwQQAi0"},"source":["## Numpy\n","\n","Numpy is a historical library which added support for large, multi-dimensional arrays and matrices to Python.\n","\n","For example, modern deep learning frameworks (such as PyTorch) have drawn largely from Numpy's API, while at the same time overcoming its limitations such as the absence of GPU support or automatic differentiation.\n","\n","For example, **Numpy arrays** and **PyTorch tensors** are very similar, and we can seamlessly convert one to the other.\n","\n","If you have prior knowledge of matrix manipulation in Matlab, we recommend the [numpy for Matlab users page](https://numpy.org/doc/stable/user/numpy-for-matlab-users.html) as a useful resource.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"eRzmry5qj8DD"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dkkmkYGDXFFU"},"source":["#### **Tensor instantiation**\n","\n","A tensor (or multidimensional array) represents an n-dimensional grid of values, **all of the same type**."]},{"cell_type":"code","metadata":{"id":"F-i2-H7QU7DH"},"source":["# Basic tensor creation from python lists\n","np.array([[1, 2, 3], [4, 5, 6]], dtype=np.int32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5LNGnqk3VbkU"},"source":["# Some other tensor construction methods\n","np.zeros((3, 5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8et3bE93WLBR"},"source":["np.ones((2, 5), dtype=np.float64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2Ehsm4WcML0"},"source":["np.eye(4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jokiBKanWFIE"},"source":["np.random.rand(2, 3)  # from which distribution are these random numbers sampled? Check the Numpy documentation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Pro tip**: Bookmark the [Numpy docs](https://numpy.org/doc/1.26/reference/index.html)."],"metadata":{"id":"hLz-HYi5rd5-"}},{"cell_type":"code","metadata":{"id":"DHV8h3LVWRaI"},"source":["np.random.randint(0, 100, (3, 4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zsf9dn4CWl2V"},"source":["t = np.random.rand(2, 3)\n","np.ones_like(t)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6FYkcuhs84Sp"},"source":["There are many other functions available to create tensors!"]},{"cell_type":"markdown","metadata":{"id":"RPEmhFYd5TK-"},"source":["> **EXERCISE**\n",">\n","> Create a matrix $M \\in \\mathbb{R}^{3 \\times 3}$ that is filled with 2 along the diagonal and 1 elsewhere, that is:\n",">\n","> $$\n","m_{ij} =\n","\\begin{cases}\n","2 & \\text{if } i = j \\\\\n","1 & \\text{otherwise}\n","\\end{cases}\n","$$"]},{"cell_type":"code","source":["# ðŸ“ write your solution in this cell"],"metadata":{"id":"pAnntNcu9otu"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2tjmfWbYk9P","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","np.ones((3, 3)) + np.eye(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_c7X85hbWs0c"},"source":["#### **Tensor properties**"]},{"cell_type":"markdown","metadata":{"id":"1XCHWcMlbhH0"},"source":["The **type** of a tensor is the type of each element contained in the tensor:"]},{"cell_type":"code","metadata":{"id":"XP4CyRTmXYpd"},"source":["t = np.random.rand(3, 3)\n","t.dtype"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TBoZHIlybZUV"},"source":["\n","The **shape** of a tensor is a tuple of integers giving the size of the tensor along each dimension, e.g. for a matrix $M \\in \\mathbb{R}^{3 \\times 5}$:"]},{"cell_type":"code","metadata":{"id":"PjuSpqbvXbqs"},"source":["t = np.random.rand(3, 5)\n","t.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UM0UCkZ49DPk"},"source":["> **EXERCISE**\n",">\n","> Given a matrix $X \\in \\mathbb{R}^{m \\times n}$, create another matrix $Y \\in \\mathbb{R}^{m \\times 3}$ filled with ones using $X$."]},{"cell_type":"code","metadata":{"id":"_GJinHgLzs11"},"source":["# Exercise variables\n","X = np.random.rand(100, 42)\n","\n","# Your solution:\n","# Y = ?"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","np.ones((X.shape[0], 3))"],"metadata":{"cellView":"form","id":"A_UxL8zE-n8z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1g8l8j4deyZ2"},"source":["#### ðŸ“– **Tensor rank**\n","\n","In Numpy and other frameworks such as PyTorch, the **rank of a tensor** denotes the number of dimensions. For example, any matrix is a tensor of rank 2.\n","\n","Don't confuse this with the rank of a matrix, which has a completely different meaning in linear algebra!"]},{"cell_type":"markdown","metadata":{"id":"pF8r6t6VTbrf"},"source":["- **rank-0** tensors are just scalars"]},{"cell_type":"code","metadata":{"id":"lfSjfJ7bTHlZ"},"source":["t0 = np.array(3.12, dtype=np.double)\n","t0, t0.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4O2GLaL0SxHn"},"source":["item = t0.item()  # convert the tensor scalar to a python base type\n","item, type(item)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPcQfgGbaEqv"},"source":["# Be careful, a non-scalar tensor cannot be converted with an .item() call\n","try:\n","  x = np.ones(3).item()\n","except ValueError as e:\n","  print('Error:', e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"El2afq3-Sujl"},"source":["- **rank-1** tensors are sequences of numbers. A sequence of length ``n`` has the shape ``(n,)``"]},{"cell_type":"code","metadata":{"id":"Twmbub4VSrvt"},"source":["# A rank-1 tensor\n","t1 = np.array([1, 2, 3])\n","t1, t1.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3LA-5C4kSHXv"},"source":["# A rank-1 tensor with a single scalar\n","np.array([42]).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Frameworks like NumPy and Pytorch are smart: if a tensor is not rank-0 but can be converted to a rank-0 tensor, then the .item() will work.\n","\n","This operation is called **broadcasting**, we will see it in detail very soon!"],"metadata":{"id":"PEGUO2rcxtwi"}},{"cell_type":"code","metadata":{"id":"7xuBu8bKcbgl"},"source":["# A rank-1 tensor with a single element can be converted to a rank-0 tensor\n","np.array([42]).item()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **NOTE**\n",">\n","> Don't be too hopeful about mapping Numpy/Pytorch concepts onto mathematical concepts.\n",">\n","> There isnâ€™t a distinction in Numpy/Pytorch between row vectors and column vectors: both are just rank-1 tensors!"],"metadata":{"id":"gU-LyxtryotQ"}},{"cell_type":"markdown","metadata":{"id":"Lj0NVuY3Rzp0"},"source":["- **rank-2** tensors have the shape ``(n, m)``"]},{"cell_type":"code","metadata":{"id":"wui6OfMFPdnM"},"source":["t2 = np.array([[1, 2, 3], [4, 5, 6]])\n","t2, t2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again, it doesn't make sense to talk about the \"rows\" and \"columns\" of rank-2 tensors."],"metadata":{"id":"Z_W9Pi50zMD-"}},{"cell_type":"code","source":["# element (i,j) of a rank-2 tensor just means the j-th element of the i-th rank-1 tensor\n","t2[1, 2].item()"],"metadata":{"id":"o6dSV3eWzvm-"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hBNed-yOmmQ"},"source":["# To mimick the notion of a column vector from linear algebra, we can use a rank-2 tensor\n","t_col = t1.reshape(-1, 1)\n","t_col, t_col.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qad_RigUjEtA"},"source":["# ...and similarly for row vectors\n","t_row = t1.reshape(1, -1)\n","t_row, t_row.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hold on, what's that `-1` used as a dimensional argument?\n","\n","When you reshape an array and use `-1` for one of the dimensions, NumPy will automatically infer the size of this dimension based on the array's total size and the specified sizes of the other dimensions!\n","\n","Therefore, the instruction `t_col = t1.reshape(-1, 1)` is telling NumPy to use one column, and automatically determine the number of rows."],"metadata":{"id":"Ta82Dzc3oDT7"}},{"cell_type":"code","source":["# After reshaping, the standard matrix product will work only if the dimensions match\n","\n","t = np.ones(10)  # rank-1 tensor\n","\n","t_row = t.reshape(1, -1)  # rank-2 'row vector'\n","t_col = t.reshape(-1, 1)  # rank-2 'column vector'\n","\n","_ = t_row @ np.ones((10, 3))  # does not work with t_col\n","_ = np.ones((3, 10)) @ t_col  # does not work with t_row\n","\n","# Notice that the matrix product still does the right thing if we multiply by the rank-1 tensor\n","\n","_ = t @ np.ones((10, 3))\n","_ = np.ones((3, 10)) @ t"],"metadata":{"id":"sOdxEQFwBpgy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Yes, we know: using `@` as a matrix product is a bit weird, but this what the developers chose as an operator. You'll get used to it!"],"metadata":{"id":"qhVkP0sBo5j4"}},{"cell_type":"markdown","metadata":{"id":"BeGPcSYMOlv0"},"source":["- **rank-k** tensors have a shape of $(n_1, \\dots, n_k)$"]},{"cell_type":"code","metadata":{"id":"hg5bbeCIjuL_"},"source":["np.zeros((2, 3, 4)).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3O68PwBGOjpe"},"source":["np.ones((2, 2, 2, 2)).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xkugOzEKi3wk"},"source":["> **EXERCISE**\n",">\n","> Build a tensor $X \\in \\mathbb{R}^{k \\times k}$ filled with zeros and the sequence $[0, ..., k-1]$ along the diagonal"]},{"cell_type":"code","metadata":{"id":"v_8mTH8pdzHz"},"source":["# your solution\n","k = 12"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","k = 12\n","X = np.diag(np.arange(k))\n","X, X.shape"],"metadata":{"id":"38L1P8aCCXed","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Dixdu5yjZ0g"},"source":["> **EXERCISE**\n",">\n","> What is the shape of the following array?\n",">\n","> ```python\n","> np.array(\n",">     [\n",">         [[1.0, 1.0, 1.0],\n",">          [1.0, 1.0, 1.0]],\n",">\n",">         [[1.0, 1.0, 1.0],\n",">          [1.0, 1.0, 1.0]],\n",">\n",">         [[1.0, 1.0, 1.0],\n",">          [1.0, 1.0, 1.0]],\n",">\n",">         [[1.0, 1.0, 1.0],\n",">          [1.0, 1.0, 1.0]],\n",">     ]\n","> )\n","> ```\n","\n"]},{"cell_type":"code","metadata":{"id":"PXIhMoIO1DJA"},"source":["# Think about it, then confirm your answer by writing code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tfGGF8mmSJkq"},"source":["### **Changing and adding dimensions**\n","\n","Numpy provides several functions to manipulate tensor shapes.\n"]},{"cell_type":"markdown","metadata":{"id":"i51GvUnwSc0U"},"source":["#### **Transpose dimension**"]},{"cell_type":"code","metadata":{"id":"g1kH1K7XS6KS"},"source":["a = np.ones((3, 5))\n","a[0, -1] = 0  # here -1 denotes the last element, as in common python indexing\n","a, a.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DUXAlWb7fJn9"},"source":["a.T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqEfj7tuTAqK"},"source":["a.transpose(1, 0)  # Swap dimension 1 and 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wvUahrTS9uJ"},"source":["np.einsum('ij -> ji', a)  # transpose using Einstein notation\n","\n","# Don't fret, we will explain the Einstein notation in detail"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9nXnyRKSfPSw"},"source":["#### ðŸ“– Transpose in k-dimensions\n","\n","The best way to think of the `.transpose()` method in Numpy is as a **permutation** of the dimensions of the tensor. In the example above, `a.transpose(1, 0)` tells Numpy to use the original dimension `1` as the new first dimension, and the original dimension `0` as the new second dimension.\n","\n","âš ï¸ This is quite different from the transpose operation PyTorch tensors, where this behavior is instead obtained with the method `.permute()`."]},{"cell_type":"code","metadata":{"id":"Sww5AtujTPiJ"},"source":["a = np.ones((2, 3, 6))\n","a[1, 2, 4] = 42\n","a, a.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bFezEV5uTc1W"},"source":["a.transpose(0, 2, 1)  #  the first dimension stays in place, while the other two swap"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pIYTVMyRUC1e"},"source":["np.einsum('ijk->ikj', a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We all know that shortcuts are handy, but sometimes they make your code less readable.\n","Most of the time readability is the most important goal to aim for!\n","\n","What do you think `a.T` will do to our rank-3 tensor? Once you have your hypothesis, test it here:"],"metadata":{"id":"iF0q66cg-M6c"}},{"cell_type":"code","source":["a.T"],"metadata":{"id":"hgp1QJWK-cX3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is confusing even after reading the docs!\n","\n","Lesson learned: Prefer readable code to short code."],"metadata":{"id":"-7zn8PWh-toL"}},{"cell_type":"markdown","metadata":{"id":"a9Jshcp9Ul5O"},"source":["> **NOTE**\n",">\n","> We have seen that in NumPy you can specify a complete mapping to transpose and swap any dimensions.\n",">\n","> You can do this in two ways: by using the array's method `.transpose()`, or by calling `np.transpose()` **with a different syntax**. Compare:\n",">\n","> `a.transpose(2, 0, 1)`\n",">\n","> `np.transpose(a, axes=(2, 0, 1))`\n",">\n","> The two instructions above give exactly the same results!"]},{"cell_type":"markdown","source":["âš ï¸ Be careful: most operations on Numpy arrays **are not in-place**!\n","\n","This means that said operations will _not_ modify the array. If you want to store the result of an operation, you'll have to explicitly assign it."],"metadata":{"id":"b_tHKUib41qr"}},{"cell_type":"code","metadata":{"id":"K9ktDbO5Uswj"},"source":["a = np.arange(10).reshape(2, 5)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h0QGO8d_hFmQ"},"source":["a.transpose(1, 0)  # does not modify a!\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LLh0Qd7U2xT"},"source":["a = a.transpose(1, 0)  # a is reassigned\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fsU83yRwU9VW"},"source":["# The einsum is cross platform. It works with consistent semantics\n","# pretty much everywhere: PyTorch, NumPy, TensorFlow, Jax, ...\n","# We will see the power of einsum in the next lab\n","np.einsum('ij -> ji', a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RWaicg6uVIXQ"},"source":["#### **Reshape**\n","\n","Above we have seen a few calls to the `reshape()` method, leaving it to your intuition. Let's delve a bit deeper.\n","\n","**Reshaping** a tensor into different dimensions is an important feature that greatly aids our implementation tasks! Keep these two points in mind when reshaping:\n","\n","- We need to make sure to **preserve the same number of elements**.\n","- `-1` in one of the dimensions means **\"figure it out\"**.\n"]},{"cell_type":"markdown","metadata":{"id":"L5uTNjKI15kx"},"source":["âŒâŒâŒ Pay attention that **transposing and reshaping are two fundamentally different operations**:"]},{"cell_type":"code","metadata":{"id":"W01aXqIZwfCu"},"source":["a = np.arange(12).reshape(3, 4)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c67xp19WwmGP"},"source":["# The classical transpose\n","a.T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EenpJs6gwoLd"},"source":["# Reshape into the transpose shape\n","a.reshape(4, 3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### ðŸ“– **What is `reshape` really doing?**\n"],"metadata":{"id":"VoIngdEzwQNY"}},{"cell_type":"markdown","metadata":{"id":"wJPn4ArY2Q6E"},"source":["\n","Think of the `reshape` operation as unrolling the tensor **row-wise**, to obtain a rank-1 tensor *(matlab users: matlab unrolls **column-wise**, pay attention when converting code!)*. Then it stores the values in this tensor following the specified dimensions.\n","\n","```python\n","array([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]])\n","```\n","$-$ unrolling $ \\to $\n","\n","```python\n","array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n","```\n","\n","Then, reading the target shape from right to left, organize the values into the dimensions:\n","\n","- e.g. reshape into `[4, 3]`:\n","\n","```python\n","array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n","```\n","\n","$-$ organize in groups of $3$ $ \\to $\n","\n","```python\n","array([[0,  1,  2],  [3,  4,  5],  [6,  7,  8],  [9, 10, 11]])\n","```\n","\n","$-$ organize in groups of $4$ $ \\to $\n","\n","```python\n","array([[ 0,  1,  2],\n","        [ 3,  4,  5],\n","        [ 6,  7,  8],\n","        [ 9, 10, 11]])\n","\n","# same shape of corresponding transpose, but the values are stored differently!\n","```\n","\n","- e.g. reshape into `[2, 2, 3]`:\n","\n","```python\n","array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n","```\n","\n","$-$ organize in groups of $3$ $ \\to $\n","\n","```python\n","array([[0,  1,  2],  [3,  4,  5],  [6,  7,  8],  [9, 10, 11]])\n","```\n","\n","$-$ organize in groups of $2$ $ \\to $\n","\n","```python\n","array([[[0,  1,  2],  [3,  4,  5]],  [[6,  7,  8],  [9, 10, 11]]])\n","```\n","\n","$-$ organize in groups of $2$ $ \\to $\n","\n","```python\n","array([[[ 0,  1,  2],\n","         [ 3,  4,  5]],\n","\n","        [[ 6,  7,  8],\n","         [ 9, 10, 11]]])\n","```"]},{"cell_type":"code","metadata":{"id":"U3U_yh07Vf7m"},"source":["a = np.arange(12)\n","a, a.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mqUJve9VjUQ"},"source":["a.reshape(6, 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i-qtMeW0VnTu"},"source":["a.reshape(2, 6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S9miZ3XUqxvc"},"source":["a.reshape(2, 2, 3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rUiVJQgWibl"},"source":["try:\n","  a.reshape(5, -1)\n","except ValueError as e:\n","  print('Error:', e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9STd1HDBWkym"},"source":["a.reshape(1, -1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zjpBpctWr-D"},"source":["a.reshape(-1, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qWpSkDFAWo_Q"},"source":["a.reshape(-1)  # we are flattening the rank-k tensor into a rank-1 tensor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RS6Bg3gg-X-F"},"source":["> **NOTE**\n",">\n","> Sometimes, we want to add or remove dimensions of size `1` to an existing array. For example, you have a 1-dimensional array constructed from a list, and you want to make it into a 2-dimensional array with only one row.\n",">\n","> You can do this by using `np.newaxis`, as follows."]},{"cell_type":"code","metadata":{"id":"WuRS4c8HmUNB"},"source":["a"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.shape"],"metadata":{"id":"Gz9-eqQtAR_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F2Z05GiFmUpt"},"source":["a[np.newaxis, ...].shape  # adds a new dimension at the front"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UXZUg0P_mcha"},"source":["a[..., np.newaxis].shape  # adds a new dimension at the end"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b = np.ones((3, 4, 2))\n","b.shape\n","b[:, :, np.newaxis, :].shape  # adds a new dimension inbetween"],"metadata":{"id":"5b8liT86DeST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OYWdXFvqrbqS"},"source":["> **NOTE**\n",">\n","> Often the reshape does not require a physical copy of the data, but just a logical\n","> reorganization.\n",">\n","> If you are curious about the NumPy/PyTorch tensor internals, a good starting point to learn about *strides* is this [SO answer](https://stackoverflow.com/questions/53097952/how-to-understand-numpy-strides-for-layman).\n","> tldr: often you can reshape tensors by changing only its strides and shape. The strides  are the byte-separation between consecutive items for each dimension."]},{"cell_type":"markdown","source":["While `np.newaxis` adds dimensions, it is perhaps even more common to _remove_ dimensions from a Numpy array. Use `.squeeze()` for this operation."],"metadata":{"id":"VBSwRuOswS2V"}},{"cell_type":"code","source":["a = np.array([[[[[1, 2, 3]]]]])\n","print(f\"{a} has shape {a.shape}\")\n","a = a.squeeze()\n","print(f\"{a} has shape {a.shape}\")"],"metadata":{"id":"szZJNTiuwiQT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VYocF4AUBJN4"},"source":["> **EXERCISE**\n",">\n","> Given a sequence of increasing numbers from `0` to `9`, defined as:\n",">\n","> ```python\n","> a = np.arange(10)\n","> ```\n",">\n","> Use only the `reshape` and `transpose` functions to obtain the following array from `a`:\n",">\n","> ```python\n","> array([0, 2, 4, 6, 8, 1, 3, 5, 7, 9])\n","> ```"]},{"cell_type":"code","metadata":{"id":"leLESEEVD-Eq"},"source":["# Your solution\n","\n","a = np.arange(10)\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","a.reshape(5, 2).transpose(1, 0).reshape(1, -1).squeeze()"],"metadata":{"cellView":"form","id":"3KUBAb3mCYOS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjvDvo4yXAk-"},"source":["#### **Concatenation**\n","\n","NumPy provides many functions to manipulate tensors.\n","Two of the most common functions are:\n","\n","- `np.stack`: Adds a **new** dimension, and concatenates the given tensors along that dimension.\n","- `np.concatenate`: Concatenates the given tensors along one of the **existing** dimensions."]},{"cell_type":"code","metadata":{"id":"Oq1IMbFvXLPk"},"source":["a = np.arange(12).reshape(3, 4)\n","b = np.arange(12).reshape(3, 4) + 100\n","print(a)\n","print(b)\n","print(a.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# stacks along a new dimension\n","out = np.stack((a, b))\n","print(out)\n","print(out.shape)"],"metadata":{"id":"idTJRE6oDRpi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# you can also specify the axis along which the stacking will take place.\n","# a new dimension is always added!\n","# axis=0 corresponds to stacking along the new dimension.\n","out = np.stack((a, b), axis=1)\n","print(out)\n","print(out.shape)"],"metadata":{"id":"QwTgqZg2Mmwk"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EsyXtEm4XRf1"},"source":["# concatenate along an existing dimension, just specify which.\n","out = np.concatenate((a, b), axis=0)\n","print(out)\n","print(out.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out = np.concatenate((a, b), axis=1)\n","print(out)\n","print(out.shape)"],"metadata":{"id":"upRrgqeZILbb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qHQFWwzCn8k7"},"source":["> **EXERCISE**\n",">\n","> Given a tensor $X \\in \\mathbb{R}^{3 \\times 1920 \\times 5 \\times 1080}$ reorganize it in order to obtain a tensor $Y \\in \\mathbb{R}^{5 \\times 1920 \\times 1080 \\times 3}$\n",">\n","> Think of $X$ as a tensor that represents $5$ RGB images of size $1080\\times 1920$. Your goal is to reorganize this tensor in a sensible (and usable) way.\n"]},{"cell_type":"code","metadata":{"id":"PrMb_KB_ogMs"},"source":["a = np.random.rand(3, 1920, 5, 1080)\n","a.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Your solution\n"],"metadata":{"id":"b2twlGrOJ0VO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","a.transpose(2, 1, 3, 0).shape"],"metadata":{"cellView":"form","id":"nLpeXAQ1I1zR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QPHnbIH_rn_C"},"source":["### **Tensor indexing**\n","\n","NumPy offers several ways to index tensors.\n"]},{"cell_type":"markdown","metadata":{"id":"U68ToAl1r_dG"},"source":["#### **Standard indexing**\n","\n","As a standard Python list, NumPy arrays support the python indexing conventions:"]},{"cell_type":"code","metadata":{"id":"PWzGd3vYsb8e"},"source":["a = np.arange(10)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vTn3_9F3tAVl"},"source":["print(a[0])  # first element\n","print(a[5])  # sixth element"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDGstnvrs6wf"},"source":["print(a[-1])  # last element\n","print(a[-2])  # second last element"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t_uRKJqor2Ty"},"source":["#### **Multidimensional indexing**\n","\n","Since tensors may be multidimensional, you can specify **one index for each dimension**:"]},{"cell_type":"code","metadata":{"id":"4oSg1hGQtyT8"},"source":["a = np.arange(10).reshape(2, 5)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QfPPgXguqeT9"},"source":["a[1, 3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_d7tUsVuKNM"},"source":["a[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6zErRdLjud3S"},"source":["a[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j40QvNuLugWX"},"source":["a[0, -1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE**\n",">\n","> What element is at position `a[1, -1]`?"],"metadata":{"id":"ei6RS_wkKJp6"}},{"cell_type":"markdown","metadata":{"id":"XY76VPqCuhc1"},"source":["#### **Slicing**\n","\n","Numpy arrays can be easily sliced using the slice notation:\n","\n","```python\n","a[start:stop]  # items from start to stop-1 (i.e. the last element is excluded)\n","a[start:]      # items from start through the rest of the array\n","a[:stop]       # items from the beginning through stop-1\n","a[:]           # a shallow copy of the whole array\n","```\n","\n","There is also an optional step value, which can be used with any of the above:\n","\n","```python\n","a[start:stop:step] # from start to at most stop-1, by step\n","```"]},{"cell_type":"code","metadata":{"id":"ku-xaZ7LvjR5"},"source":["# Sum with scalar acts element-wise\n","a = np.arange(10) + 10\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Take the elements in positions 5..6\n","a[5:7]"],"metadata":{"id":"7TRwjulxKo_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aut0rgIEwIG_"},"source":["# Take the last 5 elements\n","a[-5:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZ3vLffywvQt"},"source":["# Select every element having an even index\n","a[::2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMgTjK6yyRCl"},"source":["With multidimensional arrays we can perform **multidimensional slicing**:"]},{"cell_type":"code","metadata":{"id":"ZJYcchmxykd6"},"source":["a = np.arange(10).reshape(2, 5)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gkhWYbVHyq-H"},"source":["# Take the second column\n","a[:, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5PzkxXpdyMoL"},"source":["# Take the last column\n","a[:, -1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7q0GVL_YxVRK"},"source":["# Take a slice from the last row\n","a[-1, -3:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6oeDEvEzvhQ"},"source":["You can **assign** to sliced tensors, therefore *modifying the original tensor*.\n","\n","This means that sliced tensors are **shallow copies**: the resulting tensors **share the underlying data** with the original tensor."]},{"cell_type":"code","metadata":{"id":"R2XdMXUUzLFV"},"source":["a = np.arange(10).reshape(2, 5)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aiM4-1Je1C3y"},"source":["b = a[0:2, 1:3]\n","b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Evbxee1A1GFo"},"source":["b[-1, :] = -999\n","b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"44A4NAYewvYd"},"source":["# The original tensor has been modified\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"chqxaQU63oxh"},"source":["a[-1, -1] = -1\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_yFS2q51zdn"},"source":["> **NOTE**\n",">\n","> Indexing with integers **yields lower rank tensors**\n",">\n","> Integer indexing simply means we don't use slices (:) or boolean masks for indexing."]},{"cell_type":"code","metadata":{"id":"CqLGPTtlu22k"},"source":["a = np.arange(12).reshape(3, 4)\n","a, a.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u2vHVDozvCag"},"source":["# Rank-1 view of the second row of a\n","row_r1 = a[1, :]\n","row_r1, row_r1.shape  # notice the size of the resulting tensor, which is now lower than the original tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTsk6eG325Ib"},"source":["# Rank-2 view of the second row of a\n","row_r2 = a[1:2, :]\n","row_r2, row_r2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUVPrn7l3A5j"},"source":["# Rank-2 view of the second row of a\n","row_r3 = a[[1], :]\n","row_r3, row_r3.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TL0qs9Gw3Mfu"},"source":["# Same with the columns\n","print(a[:, 1])\n","print(a[:, [1]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TjIcC1HqwvU1"},"source":["#### ðŸ“–ðŸ“– **Slice Object**\n","\n","The **slice syntax** is just a shortand.\n","\n","In Python everything is an object, even a ``slice``.\n","It is possible to explicitly create a ``Slice`` object and reuse it to **index multiple tensors in the same way**:\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"3D8qk7q0xUAb"},"source":["# The signature follows the same pattern as above: (begin, end, step)\n","\n","s1 = slice(3)  # equivalent to the slice [:3]\n","s1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMqp5ZCEp8uu"},"source":["type(s1)  # Slice is a python built-in type!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIxqkduyp470"},"source":["out = a[s1]  # equivalent to a[:3]\n","\n","print(f\"{a} has shape{a.shape}\")\n","print(f\"{out} has shape{out.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JWr9dLVqxziR"},"source":["mystring = 'this is just a string'\n","mystring[s1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RNehWUQn2t4H"},"source":["s2 = slice(None, None, -1)  # equivalent to [::-1]\n","mystring[s2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvkH5XOCyJQx"},"source":["a[s2]  # NumPy also supports negative steps"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UUsBgzbl33Qc"},"source":["#### ðŸ“– **Integer array indexing**\n","\n","When we use slices (:), the resulting tensor view will always be a subarray of the original tensor.\n","\n","In contrast, if we index with integers only, we can construct arbitrary tensors using the data from another tensor."]},{"cell_type":"code","metadata":{"id":"8yJ7eft44Shf"},"source":["a = np.arange(1, 7).reshape(3, 2)\n","a, a.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDGTkvIQ4qaO"},"source":["# Example of integer array indexing\n","# The returned array will have shape (3,)\n","b = a[[0, 1, 2], [0, 1, 0]]\n","b, b.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8AjjBKF4nos"},"source":["# The above is equivalent to:\n","v1, v2, v3 = a[0, 0], a[1, 1], a[2, 0]\n","b = np.array([v1, v2, v3])\n","b, b.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnVB223B5Nf9"},"source":["# You can re-use the same element of the source tensor multiple times!\n","print(a[[0, 0], [1, 1]])\n","print(np.array([a[0, 1], a[0, 1]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NG8PUPue5ntQ"},"source":["# You can use another tensor to perform the indexing,\n","# as long as they have dtype=np.int64 (synonym for np.long)\n","i = np.ones(3, dtype=np.int64)\n","i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yp--PCJN5uvA"},"source":["j = np.array([0, 1, 0])\n","j"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RrlnZbw25w3H"},"source":["out = a[i, j]\n","\n","print(a)\n","print(out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzr5hn34sjko"},"source":["> **EXERCISE**\n",">\n","> Using a single assignment, change the elements of a tensor $X \\in \\mathbb{R}^{4 \\times 3}$ as follows:\n",">\n","> `X[0,2] = -1`\n",">\n","> `X[1,1] = 0`\n",">\n","> `X[2,0] = 1`\n",">\n","> `X[3,1] = 2`\n","\n"]},{"cell_type":"code","metadata":{"id":"F_3bZ5xHwmGb"},"source":["# Mutate one element from each row of a matrix\n","a = np.arange(12).reshape(4, 3)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","a[[0, 1, 2, 3], [2, 1, 0, 1]] = np.array([-1, 0, 1, 2])\n","a"],"metadata":{"cellView":"form","id":"A62V7F_PI_Cn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"544F9pAf-qRY"},"source":["> âŒâŒâŒ **NOTE**\n",">\n","> **Slice indexing vs Array indexing**\n",">\n","> Be careful, since slice indexing and array indexing are different operations!"]},{"cell_type":"code","metadata":{"id":"b-GYMhxA-tOY"},"source":["a = np.arange(16).reshape(4, 4)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Piy-JUdv-x0_"},"source":["a[0:3, 0:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXJJAq9G-0rX"},"source":["a[[0, 1, 2], [0, 1, 2]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKjSW5ePxzLW"},"source":["a[np.arange(0,3), np.arange(0,3)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hkmiQgQT_k82"},"source":["a[0:5:2, 0:5:2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MUf_I1xjx8f9"},"source":["# With *slice indexing* you return a sub-tensor."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QVMo8E2W_wAw"},"source":["#### **Boolean array indexing**\n","\n","This type of indexing is used to select the elements of a tensor that satisfy some condition (similar to MATLAB's logical indexing):"]},{"cell_type":"code","metadata":{"id":"OFIFupWJAnI9"},"source":["a = np.arange(6).reshape(3, 2)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMaNbpVPAtMm"},"source":["bool_idx = (a > 2)\n","bool_idx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbYWFo8TArod"},"source":["a[bool_idx]  # remember that NumPy and PyTorch unroll row-wise and not column-wise like Matlab"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kzVyXjJgBMKa"},"source":["If you want to know more about indexing in PyTorch and Numpy read the [docs](https://numpy.org/doc/stable/user/basics.indexing.html#basics-indexing)"]},{"cell_type":"markdown","source":["##### ðŸ“–ðŸ“– **Graph use case**\n","\n","Suppose you have a weighted adjacency matrix for a directed graph. We want to obtain a list of edges with weight greater than 0.5.\n","\n","How can we do that?\n","\n","\n","\n"],"metadata":{"id":"cZUsTLVGPXmc"}},{"cell_type":"code","source":["# Let's define a random adjacency\n","a = np.random.randint(0, 2, size=(5, 5)).astype(bool)\n","adj_matrix = ((a + a.T) > 0.5) * np.random.rand(*a.shape)\n","adj_matrix\n","\n","# Note on unpacking:\n","#   np.random.rand(*a.shape)\n","# is equivalent to\n","#   np.random.rand(a.shape[0], a.shape[1])"],"metadata":{"id":"zReQ6XJdQHfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The list of edges\n","(adj_matrix > 0.5).nonzero()"],"metadata":{"id":"dmHUoUUBP7AU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The asssociated weights for each edge\n","adj_matrix[adj_matrix > 0.5]"],"metadata":{"id":"6zyU9SBOR-JQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercises"],"metadata":{"id":"mSAHCIYePoB5"}},{"cell_type":"markdown","metadata":{"id":"fbUID2DlLuhq"},"source":["> **EXERCISE**\n",">\n","> Build a 3D tensor in $X \\in \\mathbb{R}^{3 \\times 3 \\times 3}$ that has ones along the 3D-diagonal and zeros elsewhere, i.e. a 3D identity."]},{"cell_type":"code","source":["# Write here your solution\n","# X = ?"],"metadata":{"id":"trx7jDTuLe2D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","X = np.zeros((3, 3, 3))\n","X[np.arange(3), np.arange(3), np.arange(3)] = 1\n","X"],"metadata":{"cellView":"form","id":"G2lUX3SbLS3W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5uLHb9jWM0jM"},"source":["> **EXERCISE**\n",">\n","> You are given a 3D tensor $X \\in \\mathbb{R}^{w \\times h \\times 3}$ representing a $w \\times h$ image with `(r, g, b)` color channels. Assume that colors take values in $[0, 1]$.\n",">\n","> Color the image $X$ completely by red, i.e. `(1, 0, 0)` in the `(r, g, b)` format."]},{"cell_type":"code","metadata":{"id":"g_HG8I5zNnpJ"},"source":["# Create and visualize a black image\n","x = np.zeros((100, 200, 3))\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","img = plt.imshow(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gqqm2zZUybB7"},"source":["# Write here your solution"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","x[:, :, 0] = 1\n","img = plt.imshow(x)\n","\n","# alternative solution using broadcasting -- we'll explain broadcasting in a bit!\n","# plt.imshow(x + (1, 0, 0))"],"metadata":{"cellView":"form","id":"Iu4XPwmkNEtg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ps_j8BUcQmWu"},"source":["> **EXERCISE**\n",">\n","> You are given the GitHub logo $X \\in \\mathbb{R}^{560 \\times 560}$.  Assume the logo is in gray scale, with the color $c \\in [0, 1]$ (remember 0 $\\to$ black).\n",">\n","> 1. Change the black-ish color into light gray: $0.8$.\n","> 2. Then draw a diagonal and anti-diagonal black line (i.e. an X) on the new image, to mark that the new logo is wrong."]},{"cell_type":"code","metadata":{"id":"X0082ZNOP-EK"},"source":["from skimage import io\n","\n","image = io.imread('https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png', as_gray=True)\n","_ = plt.imshow(image, cmap='gray', vmin=0, vmax=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"thBZCe_3QyxM"},"source":["# Change the black into light-gray\n","X = image.copy()\n","# # ?\n","\n","_ = plt.imshow(X, cmap='gray', vmin=0, vmax=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvjjGYmkX8m7"},"source":["# # Mark the new image as wrong with a big black X\n","# # ?\n","\n","_ = plt.imshow(X, cmap='gray', vmin=0, vmax=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","X[X < 1] = 0.8\n","X[np.arange(X.shape[0]), np.arange(X.shape[1])] = 0\n","X[np.arange(X.shape[0] - 1, -1, -1), np.arange(X.shape[1])] = 0\n","\n","_ = plt.imshow(X, cmap='gray', vmin=0, vmax=1)"],"metadata":{"cellView":"form","id":"IRYlOz1CO6qZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It is now a good moment for a break! â˜•ðŸ©\n","\n","We'll move on to some more important operations in NumPy with the remainder of this notebook. We know it's a bit exhausting, but remember: this notebook will unlock important skills that you'll probably use throughout the next few years in your career!"],"metadata":{"id":"4IO1A-rr8V7O"}},{"cell_type":"markdown","metadata":{"id":"SUl8vYRv_yuG"},"source":["### **Tensor operations**\n","\n"]},{"cell_type":"code","metadata":{"id":"bFFU6Xw7_yuA"},"source":["t = np.random.rand(3, 3)\n","t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5pMboSXXogDd"},"source":["Functions that operate on tensors are often accessible in different ways:\n","\n","- From the **`numpy` module**...:"]},{"cell_type":"code","metadata":{"id":"Cpff92U3obK5"},"source":["np.add(t, t)  # does not operate in-place!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zD99wCT8_yt0"},"source":["- ...or by means of overloaded **operators**:"]},{"cell_type":"code","metadata":{"id":"_zkKPFHo_ytw"},"source":["t += t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PuJlpLP-gWMj"},"source":["These functions are equivalent, they are *aliases* of the same method.\n","Personal preference, code consistency, and readability should guide your decision of which one to use.\n","\n","> e.g. `np.add(...)` may be too verbose, but in some cases it may be preferable since it makes explicit to the code-reader that you are dealing with tensors. Nevertheless, if you are using [types](https://docs.python.org/3/library/typing.html) -- and you should be using types -- it will be rarely necessary."]},{"cell_type":"markdown","metadata":{"id":"aQ9ChdjH_ytv"},"source":["#### **Basic operations and broadcasting**\n","\n","Basic mathematical operations $(+, -, *, /, **)$ are applied **element-wise**: for example, if `x` and `y` are two tensors, the product `x*y` is a tensor with the same size, and its values are the element-wise products of the two tensors. In mathematics, this is also called a Hadamard product.\n","\n","**Broadcasting** is another powerful mechanism that allows NumPy to perform operations on tensors of different shapes. The most basic example is summing a scalar (a rank-0 tensor) to a matrix (a rank-2 tensor)."]},{"cell_type":"code","metadata":{"id":"doNfhKA5_ytq"},"source":["x = np.array([[1, 2], [3, 4]], dtype=np.float64)\n","y = np.array([[5, 6], [7, 8]], dtype=np.float64)\n","\n","print(x + y)  # element-wise sum\n","print(x + 4.2)  # broadcasting"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GlZG4i_D_ytj"},"source":["# other examples\n","print(x * y - 5)\n","print((x - y) / y)  # element-wise division!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2rNTtIs2NMKt"},"source":["Broadcasting is quite powerful! When you perform an operation between two tensors with different shape, NumPy automatically \"broadcasts\" the smaller tensor across the larger tensor so that they have compatible shapes.\n","\n","In the example below, the sequence `v` is replicated (_without actually copying data!_) along the missing dimension so that it fits the shape of matrix `m`:"]},{"cell_type":"code","metadata":{"id":"4XzPdOKXNLV-"},"source":["m = np.arange(12).reshape(4, 3)\n","v = np.array([100, 0, 200])\n","n = m + v\n","m, v, n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this other example `m` and `u` are both rank-2, but the smaller one (`u`) is expanded along the dimension where it has size 1 to fit `m`:"],"metadata":{"id":"D4IlFl_ZjybT"}},{"cell_type":"code","metadata":{"id":"CYREKQAuQNYy"},"source":["m = np.arange(12).reshape(4, 3)\n","u = np.array([0, 10, 0, 20]).reshape(4,1)\n","n = m + u\n","m, u, n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the following example, both tensors are expanded along their size-1 dimensions, so that the sum makes sense:"],"metadata":{"id":"XZoseFdYkTpG"}},{"cell_type":"code","metadata":{"id":"ReNq-RtKg1Dy"},"source":["w = u + v\n","u, v, w"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sYNysSHoQ6dX"},"source":["Mastering broadcasting is hard!\n","\n","However, it is very convenient as it allows writing **vectorized** code, i.e., code that avoids explicit python loops which can not be efficiently parallelized.\n","\n","Technically, broadcasting takes advantage of the underlying C implementation of Numpy. Here's a take-home illustration for your convenience:\n","\n","![broadcasting](https://jakevdp.github.io/PythonDataScienceHandbook/figures/02.05-broadcasting.png)"]},{"cell_type":"markdown","metadata":{"id":"PqyMVYPL_yoC"},"source":["##### **EXERCISE**\n",">\n","> Given two vectors $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^m$, compute the differences between all possible pairs of their elements, and organize these differences in a matrix $Z \\in \\mathbb{R}^{n \\times m}$:\n","> $$ z_{ij} = x_i - y_j $$"]},{"cell_type":"code","metadata":{"id":"-WqFqMkksOBg"},"source":["x = np.array([1, 2, 3])\n","y = np.array([4, 5])\n","\n","# âœï¸ your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gtF2--mB_yn1","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","out = x[..., np.newaxis] - y\n","\n","# equivalent to:\n","# x.reshape([-1, 1]) - y\n","\n","# equivalent to:\n","# x[:, None] - y\n","\n","# equivalent to:\n","# x[:, None] - y[None, :]\n","\n","x, y, out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Using `None` to add new dimensions**\n",">\n","> In NumPy, a convenient syntax for adding new dimensions and reshaping (which you can see adopted in the solution above) makes use of the `None` object in indexing.\n",">\n","> Here's an example to help you remember the syntax:\n",">\n","> `x[None, None, :, None]`\n",">\n","> The above simply states: \"put `x` in the third dimension of a new tensor, adding two dimensions at the beginning and one at the end\".\n",">\n","> This means, for instance, that if `x` is a list then `x[:, None]` will be a rank-2 tensor with `x` along the first dimension.\n","\n","Try it out!"],"metadata":{"id":"GeysH9tx_q-Z"}},{"cell_type":"code","source":["x = np.arange(5)\n","x[None, None, :, None]"],"metadata":{"id":"Eu7CUi4FBuE2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ezcEIBPFSlFa"},"source":["#### ðŸ“– **Broadcasting, let's take a peek under the hood**\n","\n","To recap: if a NumPy operation supports broadcast, then **its tensor arguments can be implicitly expanded to be of equal sizes** (without making copies of the data)."]},{"cell_type":"markdown","metadata":{"id":"CfTF3SiYSlFg"},"source":["###### **Broadcastable tensors**\n","\n","Two tensors are \"broadcastable\" if:\n","- Each tensor has at least one dimension\n","- When iterating over the dimension sizes, starting at the trailing dimension, the dimension **sizes** must either **be equal**, **one of them is 1**, or **one of them does not exist**.\n"]},{"cell_type":"markdown","metadata":{"id":"Mdgd4qM5SlFh"},"source":["###### **Broadcasting rules**\n","\n","Broadcasting two tensors together follows these rules:\n","\n","1. If the input tensors have different ranks, **singleton dimensions are prepended to the shape** of the smaller one until it has the same rank as the other\n","2. The size in each dimension of the **output shape** is the maximum size in that dimension between the two tensors\n","3. An input can be used in the computation if its size in a particular **dimension either matches** the output size in that dimension, **or is a singleton dimension**\n","4. If an input has a dimension size of 1 in its shape, the **first data entry in that dimension will be used for all calculations** along that dimension."]},{"cell_type":"markdown","metadata":{"id":"dcj42Be5SlFi"},"source":["**Example**:\n","\n","- `m` has shape `[4, 3]`\n","- `v` has shape `[3,]`.\n"]},{"cell_type":"code","metadata":{"id":"lXQWIJtoSlFj"},"source":["m, v"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = m + v\n","n"],"metadata":{"id":"MEdjjDButCVa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZLeIamSSlFn"},"source":["\n","Following the Broadcasting logic, this is what happened:\n","\n","- `v` has less dims than `m` so a dimension of `1` is **prepended** $\\to$ `v` is now `[1, 3]`.\n","- Output shape will be `[max(1, 4), max(3, 3)] = [4, 3]`.\n","- `dim 1` of `v` matches exactly `3`; `dim 0` is `1`, so we can use the first data entry in that dimension (i.e. the whole `row 0` of `v`) each time any row is accessed. This is effectively like converting `v` from `[1, 3]` to `[4, 3]` by stacking the repeated row four times."]},{"cell_type":"markdown","metadata":{"id":"xeoux-PvSlFp"},"source":["\n","For more on broadcasting, see the [documentation](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n","\n","Functions that support broadcasting are known as universal functions (i.e. ufuncs). For Numpy you can find the list of all universal functions in the [documentation](https://docs.scipy.org/doc/numpy/reference/ufuncs.html#available-ufuncs)."]},{"cell_type":"markdown","metadata":{"id":"L_L_GYRgsnLo"},"source":["#### **EXERCISE**\n",">\n","> Given a tensor $Y \\in \\mathbb{R}^{n \\times m}$ and an index pair $(a,b)$, for each element of $Y$ compute its $L_p$ distance to $(a,b)$, and store the resulting distance value in the corresponding cell of $Y$.\n",">\n","> In brief, compute:\n","> $$ y_{ij} = d_{L_p}\\left( (i,j), (a,b) \\right) \\text{ for all }  i,j$$\n",">\n","> and visualize the resulting $Y$.\n",">\n","> Try different values of $p>0$ to see what happens.\n",">\n","> ---\n",">\n","> The [$L_p$ distance](https://en.wikipedia.org/wiki/Lp_space#The_p-norm_in_finite_dimensions) between two points $x$ and $y$ can be computed as: $d_{L_p}(x, y)=\\left( \\sum_{i=1}^n|x_i - y_i|^p\\right)^{1/p}$\n",">\n","> Example: The $L_1$ distance between $(i,j) = (3, 5)$ and $(a,b) = (14, 20)$ is:\n","> $$ y_{3,5} = d_{L_1}( (3, 5), (14, 20) ) = |3 - 14| + |5 - 20| $$"]},{"cell_type":"code","metadata":{"id":"ApPt8XdAuK7R"},"source":["import plotly.express as px\n","\n","x = np.zeros((300, 300))\n","a = 150\n","b = 150"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mFwiTbVV7iho"},"source":["# âœï¸ your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MUDYSUxLuoAK","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","rows = np.arange(x.shape[0])\n","cols = np.arange(x.shape[1])\n","\n","# Manual computation of L1\n","y = (np.abs(rows - a)[:, None] + np.abs(cols - b)[None, :])\n","px.imshow(y).show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PYF38G6dwAbT","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","# Parametric computation of Lp\n","p = 8\n","y = ((np.abs(rows - a ) ** p )[:, None] +\n","     (np.abs(cols - b) ** p)[None, :]) ** (1/p)\n","px.imshow(y).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0O530uju9a0h"},"source":["Try Solution 2 with `p=10`. What happens, and why?"]},{"cell_type":"code","metadata":{"id":"NYZfn5gJ5kOM","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","# This works even with p=10. Why?\n","p = 10\n","y = ((np.abs(rows.astype(np.float64) - a ) ** p )[:, None] +\n","     (np.abs(cols.astype(np.float64) - b) ** p)[None, :]) ** (1/p)\n","px.imshow(y).show()\n","\n","# -> Write your own explanation here\n","p = 10\n","print(np.array(10, dtype=np.int32) ** p)\n","print(np.array(10, dtype=np.double) ** p)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vpE0yZGF_ytT"},"source":["#### **Non-elementwise operations**\n","\n","\n","NumPy provides many useful functions to perform computations on tensors:"]},{"cell_type":"code","metadata":{"id":"EI33i1Df_ytN"},"source":["x = np.array([[1, 2, 3], [3, 4, 5]], dtype=np.float32)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6x4rhtfI_ytI"},"source":["# Sum up all the elements\n","np.sum(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OvndASPe_ytD"},"source":["# Compute the mean of each column\n","np.mean(x, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HUwZxvZP_ysy"},"source":["> **REMEMBER!**\n",">\n","> In order to avoid confusion with the `axis` parameter, you can think of it as an **index over the list returned by `np.shape`**. The operation is performed by iterating over that dimension.\n",">\n","> Example above: since our tensor `x` has shape `[2, 3]`, the dimension `axis=0` operates along the `2`.\n",">\n","><img src=\"https://qph.fs.quoracdn.net/main-qimg-30be20ab9458b5865b526d287b4fef9a\" width=\"500\" >"]},{"cell_type":"code","source":["x"],"metadata":{"id":"puZDDaDfgDK8"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4K-4z5pL_ys-"},"source":["# Compute the product of each row\n","np.prod(x, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRtgzF33_ys4"},"source":["# Max along the rows (i.e. max value in each column)\n","values = np.max(x, axis=0)\n","values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hd0zbRp0_ys0"},"source":["# Max along the columns (i.e. max value in each row)\n","values = np.max(x, axis=1)\n","values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D1xEs8jkkk4f"},"source":["##### ðŸ“– **Axis parameter, let's take a peek under the hood**\n"]},{"cell_type":"markdown","metadata":{"id":"gwORY20xmzq8"},"source":["Let's see what the `axis` parameter exactly does:"]},{"cell_type":"code","metadata":{"id":"DoDvtWHq_ysu"},"source":["axis = 2\n","\n","a = np.arange(2*3*4).reshape(2, 3, 4)\n","out = a.sum(axis=axis)\n","out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-KbxoTK_ysq"},"source":["# It is summing over the `dim` dimension, i.e.:\n","a.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mZcG-q5R_ysm"},"source":["# The `axis` dimension has 4 elements\n","a.shape[axis]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zqT4jSkW_ysi"},"source":["# The dimension axis collapses, the output tensor will have shape:\n","new_shape = a.shape[:axis] + a.shape[axis + 1:]\n","new_shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GHakpxbl_ysd"},"source":["# Explicitly compute the sum over dim\n","out = np.zeros(new_shape)\n","\n","# iterate over all the rows\n","for r in range(a.shape[0]):\n","  # iterate over all the columns in the r-th row\n","  for c in range(a.shape[1]):\n","\n","    for i in range(a.shape[axis]): # <- sum over 'dim'\n","\n","      out[r, c] += a[r, c, i]\n","\n","out\n","\n","# **DO NOT** use for loops in your code"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQA3ngqoHsEt"},"source":["###### **EXERCISE**\n",">\n","> Given a matrix $X \\in R^{k \\times k}$:\n","> - Compute the mean of the values along its diagonal.\n",">\n","> Perform this computation in at least two different ways, then check that the results are the same."]},{"cell_type":"code","metadata":{"id":"evQYg9-GH-Td"},"source":["x = np.random.rand(4, 4)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3x8-6wyGcB_R"},"source":["# âœï¸ your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOEhv8X0ILC2","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","a = np.mean(x[np.arange(x.shape[0]), np.arange(x.shape[1])])\n","b = np.sum(np.eye(x.shape[0]) * x) / x.shape[0]\n","c = np.trace(x) / x.shape[0]\n","d = np.mean(np.diag(x))\n","\n","print(np.allclose(a, b) and np.allclose(a, c) and np.allclose(a, d))\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YePwf4ok_ysb"},"source":["##### **EXERCISE**\n",">\n","> Given a binary non-symmetric matrix $X \\in \\{0, 1\\}^{n\\times n}$, build the symmetric matrix $Y \\in \\{0, 1\\}^{n \\times n}$ defined as:\n","> $$\n","y_{ij} =\n","\\begin{cases}\n","1 & \\text{if } x_{ij} = 1 \\\\\n","1 & \\text{if } x_{ji} = 1 \\\\\n","0 & \\text{otherwise}\n","\\end{cases}\n","$$\n",">\n","> *Hint*: search for `clip` in the [docs](https://numpy.org/doc/stable/reference/index.html)"]},{"cell_type":"code","metadata":{"id":"2Dv-OmnV_ysU"},"source":["x = np.random.randint(0, 2, (5, 5))  # Non-symmetric matrix\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6HT6OSElDic"},"source":["# âœï¸ your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4tM6Yd14JrAB","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","(x + x.transpose(1, 0)).clip(max=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cVC3YH8H_ysT"},"source":["#### **Tensor contractions**"]},{"cell_type":"markdown","metadata":{"id":"QewYLJq-_yr5"},"source":["##### **Matrix multiplication**\n","\n","Given $X \\in R^{n \\times d}$ and $Y \\in R^{d \\times v}$, their matrix multiplication $Z \\in R^{n \\times v}$ is defined as:\n","\n","$$ \\sum_{k=0}^{d} x_{ik} y_{kj} = z_{ij} $$\n"]},{"cell_type":"code","metadata":{"id":"qu16frkd_yru"},"source":["x = np.array([[1, 2], [3, 4], [5, 6]])\n","y = np.array([[1, 2], [2, 1]])\n","x, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# as we will see, matmul's functionality is not limited to matrix-matrix multiplication\n","np.matmul(x, y)"],"metadata":{"id":"ag_EbhhOH1Xp"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PbVbXIzz_yrl"},"source":["x @ y  # Operator overload for matmul"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tTqIXFNU_yrZ"},"source":["np.einsum('ik, kj -> ij', x, y)  # Einsum notation!\n","\n","# It summed up dimension labeled with the index `k`"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V-KPmcQg_ysS"},"source":["##### **Dot product**\n","Also known as scalar product or inner product.\n","Given $x \\in \\mathbb{R}^k$ and $y \\in \\mathbb{R}^k$, the dot product $z \\in \\mathbb{R}$ is defined as:\n","\n","$$ \\sum_{i=0}^{k} x_i y_i = z $$"]},{"cell_type":"code","metadata":{"id":"6DxmdUoC_ysM"},"source":["x = np.array([1, 2, 3])\n","y = np.array([4, 5, 6])\n","x, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixf_iXF3KbOv"},"source":["# We want to perform:\n","(1 * 4) + (2 * 5) + (3 * 6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0jRMizc_ysG"},"source":["np.dot(x, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xzISunbpeUi"},"source":["x.dot(y) # as a method"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GrW2utM9_ysA"},"source":["x @ y  # operator overloading matmul"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWV3hiAU_yr7"},"source":["np.einsum('i, i ->', x, y)  # Einstein notation!\n","\n","# Read it as:\n","# - iterate with i along x\n","# - iterate with i along y\n","# - compute the product at each iteration\n","# - sum the products and return a scalar (-> means return a scalar)\n","\n","# More in general, Einstein notation:\n","# Multiply point-wise repeated indices in the input\n","# Sum up along the indices that `do not` appear in the output\n","\n","# More on this below!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S-aYQyHz_yrA"},"source":["##### ðŸ“– **Broadcast matrix multiplication**\n","\n","Given a matrix $Y \\in \\mathbb{R}^{m \\times p}$ and $b$ matrices of size $n \\times m$ organized in a 3D tensor $X \\in \\mathbb{R}^{b \\times n \\times m}$, we want to multiply together each matrix $X_{i,:,:}$ with $Y$, obtaining a tensor $Z \\in R^{b \\times n \\times p}$ defined as:\n","\n","$$ z_{bij} = \\sum_{k=0}^m x_{bik} y_{kj} $$\n"]},{"cell_type":"code","metadata":{"id":"awE5anPp_yq7"},"source":["x = np.array([[[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4], [5, 6]]])\n","y = np.array([[1, 2], [2, 1]])\n","x, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvZyCiYN_yqz"},"source":["np.matmul(x, y)  # always uses the last two dimensions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7fxtm64_yq3"},"source":["x @ y   # still using the last two dimensions since @ overloads matmul"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iod-0Z04_yqx"},"source":["##### **EXERCISE**\n",">\n","> Use the einsum notation to compute the equivalent broadcast matrix multiplication!"]},{"cell_type":"markdown","metadata":{"id":"LsgxdJnP_yqv"},"source":["### **Einsum notation**\n","\n","Einstein notation is a way to express complex operations on tensors.\n","\n","- It is **concise but expressive enough** to perform almost every operation you will need in building your neural networks, allowing you to think of the only thing that matters... **dimensions!**\n","- You will **not need to check your dimensions** after an einsum operation, since the dimensions themselves are *defining* the tensor operation.\n","- You will **not need to shape-comment** your tensors. Those comments do not work: they are bound to get outdated.\n","-  You will not need to explicitly code **intermediate operations** such as reshaping, transposing and intermediate tensors.\n","- It is **not library-specific**, being avaiable in ``numpy``, ``pytorch``, ``tensorflow`` and ``jax`` with the same signature. So you do not need to remember the functions signature in all the frameworks.\n","- It can sometimes be compiled to high-performing code (e.g. [Tensor Comprehensions](https://pytorch.org/blog/tensor-comprehensions/))\n","\n","Check [this blog post by Olexa Bilaniuk](https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/) to take a peek under the hood of einsum and [this one by Tim RocktÃ¤schel](https://rockt.github.io/2018/04/30/einsum) for several examples.\n","\n","Its formal behavior is well described in the [Numpy documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).\n","However, it is very intuitive and better explained through examples.\n","\n","![alt text](https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-fmtstring.png?w=676)\n","\n","> *Historical note (taken from O.Bilaniuk's post)*\n",">\n","> Einstein had no part in the development of this notation. He merely popularized it, by expressing his entire theory of General Relativity in it. In a letter to [Tullio Levi-Civita](https://en.wikipedia.org/wiki/Tullio_Levi-Civita), co-developer alongside [Gregorio Ricci-Curbastro](https://en.wikipedia.org/wiki/Gregorio_Ricci-Curbastro) of Ricci calculus (of which this summation notation was only a part), Einstein wrote:\n",">\n","> \" *I admire the elegance of your method of computation; it must be nice to ride through these fields upon the horse of true mathematics while the like of us have to make our way laboriously on foot.* \""]},{"cell_type":"code","metadata":{"id":"UdyM0vLB_yqq"},"source":["a = np.arange(6).reshape(2, 3)  # will use this in the examples below"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ow4puwv4_yqp"},"source":["###### **Matrix transpose**\n","\n","$$ B_{ji} = A_{ij} $$"]},{"cell_type":"code","metadata":{"id":"LmE5nSrt_yqk"},"source":["# The characters are indices along each dimension\n","b = np.einsum('ij -> ji', a)\n","a, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Z8j-iVH_yqi"},"source":["###### **Sum**\n","\n","$$ b = \\sum_i \\sum_j A_{ij} := A_{ij} $$\n"]},{"cell_type":"code","metadata":{"id":"HFjp7cOb_yqd"},"source":["# Indices that do not appear in the output tensor are summed up\n","b = np.einsum('ij -> ', a)\n","a, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hks-z_NN_yqb"},"source":["###### **Column sum**\n","\n","$$ b_j = \\sum_i A_{ij} := A_{ij} $$"]},{"cell_type":"code","metadata":{"id":"MXbTLNtL_yqX"},"source":["# Indices that do not appear in the output tensor are summed up,\n","# ...even if some other index appears\n","b = np.einsum('ij -> j', a)\n","a, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yDSB2iy0Mjf3"},"source":["###### **EXERCISE**\n",">\n","> Given a binary tensor $X \\in \\{0, 1\\}^{n \\times m}$ return a tensor $y \\in \\mathbb{R}^{n}$ that has in the $i$-th position the **number of ones** in the $i$-th row of $X$.\n",">\n",">Give a solution using `einsum`, and a solution using standard manipulation."]},{"cell_type":"code","source":["x = (np.random.rand(100, 200) > 0.5).astype(np.int32)"],"metadata":{"id":"Mj-Vw04DMAyn"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sewLwDHNdQq"},"source":["# Display a binary matrix with plotly\n","\n","fig = px.imshow(x)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-YO6eOmrUkK"},"source":["# âœï¸ your code here\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVQEYEThQn-I","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","# Count the number of ones in each row\n","row_ones = np.einsum('ij -> i', x)\n","\n","row_ones2 = np.sum(x, axis=-1)  # recall that -1 refers to the last dimension\n","\n","np.allclose(row_ones, row_ones2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bUlzmmfTQ_p8"},"source":["px.imshow(row_ones[:, None]).show()\n","print(f'Sum up the row counts: {row_ones.sum()}\\nSum directly all the ones in the matrix: {x.sum()}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JA5cWXv3_yqN"},"source":["###### **Matrix-vector multiplication**\n","\n","$$ c_i = \\sum_k A_{ik}b_k := A_{ik}b_k $$"]},{"cell_type":"code","metadata":{"id":"zYF4uNUC_yqJ"},"source":["# Repeated indices in different input tensors indicate pointwise multiplication\n","a = np.arange(6).reshape(2, 3)\n","b = np.arange(3)\n","c = np.einsum('ik, k -> i', a, b)  # Multiply on k, then sum up on k\n","a, b, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qDsla37O_yqH"},"source":["###### **Matrix-matrix multiplication**\n","\n","$$ C_{ij} = \\sum_k A_{ik}B_{kj} := A_{ik}B_{kj} $$"]},{"cell_type":"markdown","metadata":{"id":"G8khafOAKElM"},"source":["ðŸ“– Understanding einsum, what happens inside?\n","\n","![alt text](https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-matrixmul.png?w=676)"]},{"cell_type":"code","metadata":{"id":"7mcYlmu5_yqD"},"source":["a = np.arange(6).reshape(2, 3)\n","b = np.arange(15).reshape(3, 5)\n","c = np.einsum('ik, kj -> ij', a, b)\n","a, b, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wVAJP6Ma_yqC"},"source":["###### **Dot product multiplication**\n","\n","$$ c = \\sum_i a_i b_i := a_i b_i $$"]},{"cell_type":"code","metadata":{"id":"2x-XwGOy_yp6"},"source":["a = np.arange(3)\n","b = np.arange(3,6)\n","c = np.einsum('i,i->', a, b)\n","a, b, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xhLZ4Gl__yp4"},"source":["###### **Point-wise multiplication**\n","Also known as Hadamard product:\n","\n","$$ C_{ij} = A_{ij} B_{ij} $$"]},{"cell_type":"code","metadata":{"id":"QTUH61Ft_yp0"},"source":["a = np.arange(6).reshape(2, 3)\n","b = np.arange(6,12).reshape(2, 3)\n","c = np.einsum('ij, ij -> ij', a, b)\n","a, b, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dZoiSCsn_ypz"},"source":["###### **Outer product**\n","Given two column vectors of length $m$ and $n$ respectively,\n","\\begin{align*}\n","\\mathbf{a}=\\left[\\begin{array}{c}\n","a_{1} &\n","a_{2} &\n","\\dots &\n","a_{m}\n","\\end{array}\\right]^\\top, \\quad \\mathbf{b}=\\left[\\begin{array}{c}\n","b_{1} &\n","b_{2} &\n","\\dots &\n","b_{n}\n","\\end{array}\\right]^\\top\n","\\end{align*}\n","their outer product, denoted $\\mathbf{a} \\otimes \\mathbf{b}$, is defined as the $m \\times n$ matrix $\\mathbf{C}$ obtained by multiplying each element of $\\mathbf{a}$ by each element of $\\mathbf{b}$:\n","\\begin{align*}\n","\\mathbf{a} \\otimes \\mathbf{b}=\\mathbf{C}=\\left[\\begin{array}{cccc}\n","a_{1} b_{1} & a_{1} b_{2} & \\ldots & a_{1} b_{n} \\\\\n","a_{2} b_{1} & a_{2} b_{2} & \\ldots & a_{2} b_{n} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","a_{m} b_{1} & a_{m} b_{2} & \\ldots & a_{m} b_{n}\n","\\end{array}\\right]\n","\\end{align*}\n","Or, in index notation,\n","$$ C_{ij} = a_i b_j $$"]},{"cell_type":"code","metadata":{"id":"k71BkJbf_ypu"},"source":["a = np.arange(3)\n","b = np.arange(3,7)\n","c = np.einsum('i, j -> ij', a, b)\n","a, b, c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1MQYFeM2_ypo"},"source":["# Using the standard NumPy API\n","np.outer(a, b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using broadcasting black magic\n","a[:, None] * b[None, :]"],"metadata":{"id":"vuElOQ9YkjiW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HXQHwUet_yph"},"source":["###### ðŸ“– **Batch matrix multiplication**\n","\n","$$ c_{bij} = \\sum_k a_{bik} b_{bkj} $$"]},{"cell_type":"code","metadata":{"id":"6vpbY37H_ypb"},"source":["a = np.random.randn(2, 2, 5)\n","b = np.random.randn(2, 5, 3)\n","c = np.einsum('bik,bkj->bij', a, b)\n","a, b, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2fm8KVyoyoNh"},"source":["###### **EXERCISE**\n","> Implement:\n","> - Matrix transpose with einsum, in particular assume you have a batch of images of shape $(B, C, H, W)$ and you want to turn it into having shape $(B, H, W, C)$\n","> - Quadratic form with einsum  ($y = v^TMv$)"]},{"cell_type":"markdown","metadata":{"id":"2InvGnAtJ3nJ"},"source":["**Pro tip:** Do not try to memorize all the NumPy API!\n","\n","> Learn to understand what operation should already exist and search for it, when you need it. If it is something common, and it usually is, chances are it already exists.\n","\n","Google, StackOverflow and the docs are your friends!"]},{"cell_type":"markdown","metadata":{"id":"7x0_oylwuIgD"},"source":["### ðŸ“– Einops\n","\n","If you liked the `einsum` operation, have fun with the [einops](https://github.com/arogozhnikov/einops) package! ðŸš€\n","\n","It is a third-party library, compatible with most frameworks, that brings superpowers to `einsum`. We will not use the `einops` library in the tutorials, however, feel free to read the [docs](https://github.com/arogozhnikov/einops) and use it.\n","\n","![](http://arogozhnikov.github.io/images/einops/einops_video.gif)\n"]},{"cell_type":"markdown","metadata":{"id":"lafVikgz_ypX"},"source":["### Final exercises"]},{"cell_type":"markdown","source":["These final exercises are designed to showcase the elegant solutions of einsum.\n","\n","Feel free to also write down solutions that do _not_ use einsum, but rather with standard tensor manipulation!"],"metadata":{"id":"yKKthCbr_dHf"}},{"cell_type":"markdown","metadata":{"id":"VPeJMqEACs8z"},"source":["\n","#### **EXERCISE 1**\n",">\n","> You are given $b$ images with size $w \\times h$. Each pixel in each image has three color channels, `(r, g, b)`. These images are organized in a tensor $X \\in \\mathbb{R}^{w \\times b \\times c \\times h}$.\n",">\n","> You want to apply a linear trasformation to the color channel of each single image. In particular, you want to :\n","> - **Convert each image into a grey scale image**.\n","> - **Afterthat, transpose the images** to swap the height and width.\n",">\n","> The linear traformation that converts from `(r, g, b)` to grey scale is simply a linear combination of `r`, `g` and `b`. It can be encoded in the following 1-rank tensor $y \\in \\mathbb{R}^3$:"]},{"cell_type":"code","metadata":{"id":"0BRybv63oysl"},"source":["y = np.array([0.2989, 0.5870, 0.1140], dtype=np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ON5HBjD2oyLi"},"source":["\n","> At the end, you want to obtain a tensor $Z \\in \\mathbb{R}^{b \\times w \\times h}$.\n",">\n","> Write the NumPy code that performs this operation."]},{"cell_type":"code","metadata":{"id":"If137gBApDcR"},"source":["# Create the input tensors for the exercise\n","# Execute and ignore this cell\n","\n","from skimage import io\n","from skimage.transform import resize\n","\n","size = 100\n","\n","image1 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Earth_Eastern_Hemisphere.jpg/260px-Earth_Eastern_Hemisphere.jpg')\n","image1 = (resize(image1, (size, size), anti_aliasing=True)).astype(np.float32)  # Covert  to float type\n","image1 = image1[..., :3]  # remove alpha channel\n","\n","image2 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/The_Sun_by_the_Atmospheric_Imaging_Assembly_of_NASA%27s_Solar_Dynamics_Observatory_-_20100819.jpg/628px-The_Sun_by_the_Atmospheric_Imaging_Assembly_of_NASA%27s_Solar_Dynamics_Observatory_-_20100819.jpg')\n","image2 = (resize(image2, (size, size), anti_aliasing=True)).astype(np.float32)\n","image2 = image2[..., :3]  # remove alpha channel\n","\n","image3 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Wikipedia-logo-v2.svg/1920px-Wikipedia-logo-v2.svg.png')\n","image3 = (resize(image3, (size, size), anti_aliasing=True)).astype(np.float32)\n","image3 = image3[..., :3]  # remove alpha channel\n","\n","source_images = np.stack((image1, image2, image3), axis=0)\n","images = np.einsum('bwhc -> wbch', source_images)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DGgBC-dkqwVo"},"source":["# Plot source images\n","# âœï¸ your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Flm4DsjMww90"},"source":["# âœï¸ your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NwGXAyrVryTz","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","# Grey-fy all images together, using the `images` tensor\n","gray_images = np.einsum('wbch, c -> bwh', images, y)\n","\n","# What if you want to transpose the images?\n","gray_images_tr = np.einsum('wbch, c -> bhw', images, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EEWp-NDzsB4V"},"source":["# Plot the gray images\n","# âœï¸ your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ga_3wk7BPmp6"},"source":["# Plot the gray transposed images\n","# âœï¸ your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oF6FdNroYMqn"},"source":["#### **EXERCISE 2**\n",">\n","> Given $k$ points organized in a tensor $X \\in \\mathbb{R}^{k \\times 2}$ apply a reflection along the $y$ axis as a linear transformation.\n"]},{"cell_type":"code","metadata":{"id":"q_d9KnJ5Y3e0"},"source":["# Define some points in R^2\n","x = np.arange(100, dtype=np.float32)\n","y = x ** 2\n","\n","# Define some points in R^2\n","data = np.stack((x, y), axis=0).T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h0_JB4C1Y2vm"},"source":["px.scatter(x = data[:, 0], y = data[:, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1Xt5XEkDYvB"},"source":["# âœï¸ your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWCEwmIfaebY","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","# Define a matrix that encodes a linear transformation\n","S = np.array([[-1, 0],\n","              [ 0, 1]], dtype=np.float32)\n","\n","# Apply the linear transformation: the order is important\n","new_data = np.einsum('nk, dk -> nd', data, S)\n","\n","# Double check yourself:\n","# The linear transformation correctly maps the basis vectors!\n","S @ np.array([[0],\n","              [1]], dtype=np.float32)\n","S @ np.array([[1],\n","              [0]], dtype=np.float32)\n","\n","# Check if at least the shape is correct\n","new_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5H7VEJFXbaMb"},"source":["# Plot the new points\n","px.scatter(x = new_data[:, 0], y = new_data[:, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1tHmuIBM9ka"},"source":["#### **EXERCISE 3**\n",">\n",">  You are given $b$ images with size $w \\times h$. Each pixel in each image has `(r, g, b)` channels. These images are organized in a tensor $X \\in \\mathbb{R}^{w \\times b \\times c \\times h}$, i.e. the same tensor as in the exercise 1.\n",">\n","> You want to swap the `red` color with the `blue` color, and decrease the intensity of the `green` by half.\n",">\n","> Perform the transformation on all the images simultaneously."]},{"cell_type":"code","metadata":{"id":"lppGxeMSO0c8"},"source":["images.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"POWStC54w2ZZ"},"source":["# âœï¸ your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VE2xc7i_ORzg","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","# Define the linear transformation to swap the blue and red colors\n","# and half the green\n","S = np.array([[ 0, 0, 1],\n","              [ 0, .5, 0],\n","              [ 1, 0, 0]], dtype=np.float32)\n","\n","# Apply the linear transformation to the color channel!\n","rb_images = np.einsum('wbch, dc -> bwhd', images, S)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zdfYOMBOxjv"},"source":["# Plot the images\n","# âœï¸ your code here"],"execution_count":null,"outputs":[]}]}