{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/erodola/ML-s2-2024/blob/main/labs/03_SGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"4C5Ct9yoZKYa"},"source":["# Machine Learning\n","\n","# Tutorial 3: Stochastic Gradient Descent\n","\n","In this tutorial, we will cover:\n","\n","- Features and feature scaling\n","- Multinomial regression\n","- Logistic regression with scikit-learn\n","- SGD and momentum\n","\n","Authors:\n","\n","- Prof. Emanuele RodolÃ \n","\n","Course:\n","\n","- Lectures and notebooks at https://github.com/erodola/ML-s2-2024/"]},{"cell_type":"markdown","source":["# Imports and utilities"],"metadata":{"id":"iXd3HJRDfLEO"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pRePt-K1_yw9"},"outputs":[],"source":["# @title import dependencies\n","\n","from typing import Mapping, Union, Optional, Tuple\n","import argparse\n","\n","import numpy as np\n","\n","from tqdm.notebook import tqdm\n","\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","import plotly.graph_objects as go\n","\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","\n","import sklearn\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import PolynomialFeatures"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tGN_bJOcfd3","cellView":"form"},"outputs":[],"source":["# @title reproducibility stuff\n","\n","import random\n","np.random.seed(42)\n","random.seed(0)"]},{"cell_type":"markdown","metadata":{"id":"UvrnMPqEi8tH"},"source":["# Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"s00hN6W4k2HR"},"source":["## Binary model\n","\n","In theory class, we saw the linear regression model as a **binary classifier**. If we have a bunch of data points (e.g. images) $\\mathbf{x}_i$, the classifier outputs a number in $[0,1]$ telling if each $\\mathbf{x}_i$ belongs to class $0$ or $1$:\n","\n","$$ p_i = \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) $$\n","\n","To find the optimal $\\mathbf{w}$, we need to solve an optimization problem where $\\mathbf{w}$ is the unknown, and $(\\mathbf{x}_i, y_i)$ are the data; note that $y_i$ are the true labels, while $p_i$ are our model's predictions. We define the loss:\n","\n","$$ \\ell(\\mathbf{w}) = -\\sum_i ( y_i\\log p_i + (1-y_i)\\log (1-p_i)) $$\n","\n","Let's solve this with Scikit-learn!"]},{"cell_type":"markdown","source":["> **EXERCISE**: Check scikit-learn's docs on binary logistic regression. Two additional terms appear: $s_i$ and $r(w)$. What are they?"],"metadata":{"id":"IlR49crTnl2r"}},{"cell_type":"markdown","source":["## Data loading\n","\n","To familiarize with the problem, we are going to solve it using a black-box approach and then interpret the results. After this, we will write our own solver!\n","\n","First, let's load some data:"],"metadata":{"id":"tLUn_0Kum3wq"}},{"cell_type":"code","source":["from sklearn.datasets import load_digits\n","digits = load_digits()\n","\n","digits.data.shape  # there are 1797 images, each with 8x8=64 pixels"],"metadata":{"id":"1gvxfw1Jobls"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our training data is not just the images; we also need their ground-truth labels:"],"metadata":{"id":"X2pDsPytpT-c"}},{"cell_type":"code","source":["digits.target.shape"],"metadata":{"id":"Ho5NWbSDpYWm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Always visualize the data:"],"metadata":{"id":"eEcdI1KDpIlC"}},{"cell_type":"code","source":["random_idx = np.random.randint(0, digits.data.shape[0], 5)\n","plt.figure(figsize=(6,2))\n","for i, idx in enumerate(random_idx):\n","  plt.subplot(1, 5, i + 1)\n","  plt.imshow(np.reshape(digits.data[idx], (8,8)), cmap=plt.cm.gray)\n","  plt.title(f\"label: {digits.target[idx]}\")\n","  plt.axis(\"off\")"],"metadata":{"id":"cU4FEHv-pLFK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE**: Only visualize the **7** digits. Plot 16 of them in a 4x4 grid."],"metadata":{"id":"lgEzN7yafyG1"}},{"cell_type":"markdown","source":["## Multinomial logistic regression\n","\n","Hang on, we know how to solve a _binary_ classification task, but now we have 10 classes, one per digit. This makes it a _multiclass_ problem, and we need a new learning model to deal with it. Enter **multinomial classification**.\n","\n","Here's the good old binary logistic regression model:\n","\n","$$ p = \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) $$\n","\n","In the multinomial case, we now have:\n","\n","$$ \\mathbf{p} = \\text{softmax} (\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b}) \\,.$$\n","\n","where $\\mathbf{p}$ represents the _probabilities_ over 10 classes.\n","\n","For example, if $\\mathbf{p}=(0, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0.1)$, it means that $\\mathbf{x}_i$ belongs to the second class with probability 0.8, to the third class with probability 0.1, and so on. Also observe that the **weight matrix** $\\mathbf{W}$ has as many rows as the number of classes, and that the **bias** is now a vector containing one value per class.\n","\n","What is that _softmax_ function? Formally, it is defined as follows:\n","\n","$$\\text{softmax}(\\mathbf{x}) = \\{\\frac{\\exp(x_0)}{\\sum_{j}^{ }\\exp(x_j))}, \\frac{\\exp(x_1)}{\\sum_{j}^{ }\\exp(x_j))}, ... , \\frac{\\exp(x_9)}{\\sum_{j}^{ }\\exp(x_j))}\\}$$\n","\n","By construction, $\\text{softmax}(\\mathbf{x})$ sums up to one and each element is in $[0,1]$; therefore we can treat the softmax vector as a discrete probability distribution over the set of classes.\n"],"metadata":{"id":"UfaPsAc8NuMP"}},{"cell_type":"markdown","source":["\n","The following plot gives a useful intuition of how softmax behaves; we'll do this by considering $\\exp(\\alpha x)$ with different $\\alpha$:"],"metadata":{"id":"XGhteUS0x5Ww"}},{"cell_type":"code","source":["x = np.random.rand(40)"],"metadata":{"id":"B7hSd1zTdcQN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Softmax: crank up the alpha!  { run: \"auto\" }\n","\n","import plotly.graph_objects as go\n","\n","alpha = 1  #@param {type:\"slider\", min:1, max:50, step:1}\n","\n","sx = np.exp(alpha*x)\n","sx /= sx.sum()\n","\n","fig = go.Figure()\n","# fig.add_trace(go.Bar(y=x, name='x', marker_color='blue'))\n","fig.add_trace(go.Bar(y=sx, name='sx', marker_color='red'))\n","fig.update_layout(barmode='group', title='Softmax of a random vector', width=800, height=300)\n","fig.show()"],"metadata":{"cellView":"form","id":"dz0X9hFLdg6a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sums up to one\n","print(f\"{np.sum(sx):.1f}\")\n","\n","# all values are in [0,1]\n","np.all((sx >= 0) & (sx <= 1))"],"metadata":{"id":"btw4Jlhtd6y9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You now got an idea if why it's called soft**max**? One useful way to think about the softmax is as a smooth approximation of the indicator function, centered around the maximum."],"metadata":{"id":"n0WEur9Cdq-I"}},{"cell_type":"markdown","source":["## Logits\n","\n","Another bit of terminology:\n","\n","$$ \\text{softmax}(\\underbrace{\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b}}_{\\mathbf{z}_i}) $$\n","\n","The $\\mathbf{z}_i$ are also called the **logits** in ML lingo. Familiarize with this term, as it occurs often!\n","\n","You can think of the logits as raw class scores that get converted into probabilities by the softmax.\n"],"metadata":{"id":"HZQ-bkl3yQiL"}},{"cell_type":"markdown","source":["## Pixels as features\n","\n","So far we have been treating each image $\\mathbf{x}_i$ as _flattened_: it's a $k$-dimensional vector, where $k$ is the number of pixels. In ML, we call $\\mathbf{x}_i$ the **feature vector** of image $i$. In this vector, **each dimension is a feature**."],"metadata":{"id":"FLA7ON6F5iC-"}},{"cell_type":"code","source":["xi = digits.data[17]\n","print(f\"{len(xi)} features\")\n","\n","plt.figure(figsize=(8, 2))\n","_ = plt.bar(range(len(xi)), xi)"],"metadata":{"id":"CDmRyo0g6RPp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clearly, in this case each feature is a pixel. From this plot we also learned that the maximum value (corresponding to a white pixel) is 16.\n","\n","Other features are possible. For instance, a possible (but not very informative) feature vector for an image might be its color histogram:"],"metadata":{"id":"J9eYx1NX_V6y"}},{"cell_type":"code","source":["hist, _ = np.histogram(xi, bins='auto')\n","\n","plt.figure(figsize=(8, 2))\n","_ = plt.bar(range(len(hist)), hist)"],"metadata":{"id":"i7CF6h7r7E97"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's stay _raw_ for now, and use pixel features as input to our learning algorithm."],"metadata":{"id":"DIdPnYgE7FgO"}},{"cell_type":"markdown","source":["## Training with scikit-learn\n","\n","Now that we have pinned down the terminology, we are ready to set up our learning model for logistic regression and train it:"],"metadata":{"id":"Xo1FFeep8Bbc"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","reg = LogisticRegression(max_iter=10)\n","_ = reg.fit(digits.data, digits.target)"],"metadata":{"id":"mxNcPE4Krg7K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hang on, what are those warnings? `lbfgs` failed converge, meaning that the solver ([LBFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) in this case, but we will implement SGD) didn't reach a minimum. Indeed, we specified `max_iter=10`, so it didn't have time to reach a minimum. Increase that value to 100 or 1000 and see what happens!"],"metadata":{"id":"_mp6wSvuupY8"}},{"cell_type":"markdown","source":["## Feature scaling\n","\n","Apparently, even with `max_iter=1000` we keep getting a warning. A possible problem is that **our data is not normalized**.\n","\n","Normalizing the data means (i) centering it around zero, and (ii) rescaling it to have unit standard deviation:\n","\n","$$ x \\mapsto \\frac{x - \\mu}{\\sigma} $$\n","\n","Here, $x$ is a **feature** and $\\mu$ is the mean of that feature over the entire training set. Similarly, $\\sigma$ is the standard deviation.\n","\n","For example, if the feature is a pixel at position $(12,8)$, we must take the mean over all the pixels at position $(12,8)$ in the entire dataset. In other words, centering and scaling must be done _independently on each feature_.\n","\n","Feature scaling is also referred to as **standardization**."],"metadata":{"id":"SUETsZ463yZa"}},{"cell_type":"markdown","source":["> **EXERCISE**: Create a `standard_scale` function implementing feature scaling, and apply it to the input data."],"metadata":{"id":"NOMpQ_nkFguX"}},{"cell_type":"code","source":["# âï¸ your code here\n","\n","#X = standard_scale(digits.data)"],"metadata":{"id":"SeoW2_hKFwbp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ð Solution\n","\n","def standard_scale(X):\n","  means = np.mean(X, axis=0)  # mean across the feature dimensions\n","  stds = np.std(X, axis=0)\n","  stds[stds < 1e-6] = 1.  # avoid division by very small values\n","  X_scaled = (X - means) / stds\n","  return X_scaled\n","\n","X = standard_scale(digits.data)\n","# X = sklearn.preprocessing.scale(digits.data, axis=0)  # scikit-learn version"],"metadata":{"cellView":"form","id":"q2EUWFTKBPNe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE:** Is it possible that a feature has zero standard deviation? When does it happen with our current data?"],"metadata":{"id":"nX_4QnXuISBQ"}},{"cell_type":"markdown","source":["Standardization is important for numerical stability: if a feature has a variance that is orders of magnitude larger than others, it might dominate the loss and make the training unable to learn from other features."],"metadata":{"id":"LTbPhm_pBRlz"}},{"cell_type":"code","source":["reg = LogisticRegression(max_iter=100)\n","_ = reg.fit(X, digits.target)"],"metadata":{"id":"DPH8L7AID1GS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The warnings are gone, and the optimizer converged!"],"metadata":{"id":"z_REvzP7EE1A"}},{"cell_type":"markdown","source":["## Evaluation\n","\n","We can now evaluate the quality of our model. By the way, did you notice that we have as many weights as we have features (i.e. image pixels in this case)? It's obvious if we look at the model expressions:\n","\n","$$ \\sigma(\\mathbf{w}^\\top\\mathbf{x}_i + b) \\quad , \\quad\n","\\text{softmax} (\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b})$$\n","\n","This suggests that we can reshape the set of weights (just the $\\mathbf{w}$ or $\\mathbf{W}$) as an image, and have a look at them!\n"],"metadata":{"id":"oWfuB5e0FFeE"}},{"cell_type":"code","source":["plt.figure(figsize=(10,2))\n","for i in range(10):\n","  plt.subplot(1, 10, i + 1)\n","  plt.imshow(reg.coef_[i].reshape(8, 8), cmap=plt.cm.viridis)\n","  plt.title(f\"{i}\")\n","  plt.axis(\"off\")"],"metadata":{"id":"2zhIgyLyhzsS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Not so easy to read, but look at the weights responsible to classify the **0** digit, which are perhaps more clear. That image is a \"0\" classifier. It will not always be possible to grasp such a clear intuition just by looking at the learned weights, but studying them is a valid research direction by itself!"],"metadata":{"id":"UZ7K9WSRkc44"}},{"cell_type":"markdown","source":["To get a _quantitative_ evaluation, we can compute the hit rate, i.e., the percentage of correctly classified images:"],"metadata":{"id":"w15I80o2Pw5G"}},{"cell_type":"code","source":["preds = reg.predict(X)\n","\n","# calculate the amount of correctly classified images\n","accuracy = np.sum(preds == digits.target) / len(digits.target)\n","\n","print(f\"{100*accuracy:.2f}%\")"],"metadata":{"id":"bT8vRfJXFLyy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you did everything correctly, you should see an accuracy larger than 99%. Is it a perfect classifier? To answer this question, we must test how well the model generalizes to unseen data. But we don't have any new data to use!\n","\n","Let's restart from scratch, but this time we'll split the available images into 75% **training** data and use the remaining 25% as **validation** data."],"metadata":{"id":"pYpyGmvGFcwi"}},{"cell_type":"code","source":["X = digits.data.copy()\n","labels = digits.target.copy()\n","\n","def shuffle(X_, y_):\n","  shuffled_idx = np.random.permutation(X_.shape[0])\n","  return X_[shuffled_idx], y_[shuffled_idx]\n","\n","# shuffle the data to avoid any bias\n","X, labels = shuffle(X, labels)\n","\n","# prepare training and validation sets\n","n_train = int(0.75 * X.shape[0])\n","X_train = X[:n_train]\n","X_valid = X[n_train:]\n","\n","# center and scale\n","X_train = standard_scale(X_train)\n","X_valid = standard_scale(X_valid)"],"metadata":{"id":"L_PiDdFHHB_D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["â ï¸ **Leakage warning:** A common mistake is to rescale data *before* splitting into training and test sets. _This will bias the model_ because statistics of the _test set_ are carried over to the training set."],"metadata":{"id":"z4_zfRWDyHGC"}},{"cell_type":"markdown","source":["Let's retrain our model, and test it on the validation set:"],"metadata":{"id":"Znr4u4tAKt5b"}},{"cell_type":"code","source":["reg = LogisticRegression(max_iter=100)\n","_ = reg.fit(X_train, labels[:n_train])\n","\n","preds = reg.predict(X_valid)\n","accuracy = np.sum(preds == labels[n_train:]) / (X.shape[0] - n_train)\n","\n","print(f\"{100*accuracy:.2f}%\")"],"metadata":{"id":"BxrR4DHUsIRg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lower than before, but still good! Let's look at the **misclassified images**, i.e. the images from the validation set that did _not_ get a correct prediction:"],"metadata":{"id":"88D3qdjMLJ0-"}},{"cell_type":"code","source":["wrong_idx = np.where(preds != labels[n_train:])[0]\n","\n","plt.figure(figsize=(6,2))\n","for i, idx in enumerate(wrong_idx[:5]):\n","  plt.subplot(1, 5, i + 1)\n","  plt.imshow(np.reshape(X_valid[idx], (8,8)), cmap=plt.cm.gray)\n","  plt.title(f\"predict: {preds[idx]}\")\n","  plt.axis(\"off\")"],"metadata":{"id":"15K9RoyLLbmp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hmm, these digits are not so easy to read (especially after normalization!). Let's move to a more readable dataset that provides a standard baseline in ML and DL: the **MNIST dataset**."],"metadata":{"id":"uv1dv-GSM7RB"}},{"cell_type":"markdown","source":["## Classifying MNIST digits\n","\n","MNIST is a dataset consisting of 70,000 handwritten digits and it's typically used to prototype ideas and new methods. We will download it directly from the web, as Scikit-learn doesn't offer it among its datasets. The code below already takes care of the training/validation splitting:"],"metadata":{"id":"6d5H6bkPNbVP"}},{"cell_type":"code","source":["!wget https://s3.amazonaws.com/img-datasets/mnist.npz\n","\n","def load_data_impl():\n","    # file retrieved by:\n","    #   wget https://s3.amazonaws.com/img-datasets/mnist.npz -O code/dlgo/nn/mnist.npz\n","    # code based on:\n","    #   site-packages/keras/datasets/mnist.py\n","    path = 'mnist.npz'\n","    f = np.load(path)\n","    x_train, y_train = f['x_train'].reshape(-1, 784), f['y_train']\n","    x_test, y_test = f['x_test'].reshape(-1, 784), f['y_test']\n","    f.close()\n","    return (x_train.astype(np.float32), y_train), (x_test.astype(np.float32), y_test)\n","\n","(x_train, y_train), (x_valid, y_valid) = load_data_impl()\n","\n","# bring pixel values to [0, 1]\n","x_train /= 255\n","x_valid /= 255"],"metadata":{"id":"qNBY8WTpNR8D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Each MNIST image is 28x28 (=784 pixels)."],"metadata":{"id":"eCXFiR2SpDh3"}},{"cell_type":"markdown","source":["> **EXERCISE:** Visualize a few MNIST images from the training set, _before_ normalization and _after_ normalization. Make sure to normalize all the data before proceeding to the next cells."],"metadata":{"id":"VXffSNtgpIDn"}},{"cell_type":"code","source":["# âï¸ your code here\n"],"metadata":{"id":"gbClTaTcqtxs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ð Solution\n","\n","plt.figure(figsize=(6,2))\n","for i in range(5):\n","  plt.subplot(1, 5, i + 1)\n","  plt.imshow(x_train[i].reshape(28, 28), cmap=plt.cm.gray)\n","  plt.axis(\"off\")\n","\n","x_train = standard_scale(x_train)\n","\n","plt.figure(figsize=(6,2))\n","for i in range(5):\n","  plt.subplot(1, 5, i + 1)\n","  plt.imshow(x_train[i].reshape(28, 28), cmap=plt.cm.gray)\n","  plt.axis(\"off\")\n","\n","x_valid = standard_scale(x_valid)"],"metadata":{"cellView":"form","id":"Ttdem6dlo8nl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE:** You know what to do: train a classifier and evaluate it. Can you get **above 95%** accuracy?"],"metadata":{"id":"v0Erh_0BrNTK"}},{"cell_type":"code","source":["# âï¸ your code here\n"],"metadata":{"id":"vca_3Um5sEyM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It's always a good idea to have a look at the misclassified samples:"],"metadata":{"id":"i7HypAojuCrT"}},{"cell_type":"code","source":["# âï¸ your code here\n"],"metadata":{"id":"z7kCn1mIuOmu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stochastic Gradient Descent (SGD)"],"metadata":{"id":"n6Hj0Ukf5xWY"}},{"cell_type":"markdown","source":["In our experiments so far, we trusted scikit-learn's implementation of the logistic regression model, together with its training (the `fit()` function). We are now going to implement our own training procedure via gradient descent:\n","\n","$$\\mathbf{W}^{(t+1)} = \\mathbf{W}^{(t)} - \\alpha \\nabla \\ell(\\mathbf{W}^{(t)})$$\n","with\n","$$\\nabla \\ell(\\mathbf{W}^{(t)}) = \\frac{1}{m} \\sum_i^m \\nabla \\ell_{\\{\\mathbf{x}_i, y_i\\}}(\\mathbf{W}^{(t)}) $$\n","\n","For $m \\ll n$ we get **stochastic** gradient descent, as we have seen in theory class.\n","\n","_Note:_ Remember that we also have the update equations for the bias $\\mathbf{b}$; we are not writing them here for brevity."],"metadata":{"id":"lYdPOELz7Sdw"}},{"cell_type":"markdown","source":["Let's first prepare some data:"],"metadata":{"id":"IMEkQ6bzCVkq"}},{"cell_type":"code","source":["# we'll use the small digit data again\n","from sklearn.datasets import load_digits\n","digits = load_digits()\n","\n","X = digits.data.copy()\n","labels = digits.target.copy()\n","n_classes = 10\n","\n","# shuffle the data to avoid any bias\n","X, labels = shuffle(X, labels)\n","\n","# prepare training and validation sets\n","n_train = int(0.75 * X.shape[0])\n","X_train = X[:n_train]\n","y_train = labels[:n_train]\n","X_valid = X[n_train:]\n","y_valid = labels[n_train:]\n","\n","# center and scale\n","X_train = standard_scale(X_train)\n","X_valid = standard_scale(X_valid)"],"metadata":{"id":"Ve6DtI1GB5A8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The loss\n","\n","Before computing the gradient, we still need to define a loss! We have cross-entropy for the binary setting, but what about the multinomial case?\n","\n","Here's our learning model:\n","\n","$$ \\mathbf{p} = \\text{softmax} (\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b}) $$\n","\n","The first thing we do is modify it slightly, to output the _log-probabilities_ instead of the probabilities, namely:\n","\n","$$ \\log(\\mathbf{p}) = \\log(\\text{softmax} (\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b}) ) $$"],"metadata":{"id":"VeQXd_vE8fb3"}},{"cell_type":"markdown","source":["> **EXERCISE:** _(Pen-and-paper)_ Rewrite the formula for $\\log(\\mathbf{p})$ by explicitly applying the $\\log$ to $\\text{softmax}$."],"metadata":{"id":"QR--4Emm-FwQ"}},{"cell_type":"markdown","source":["You can use the result from the simplified formula to solve the next exercise."],"metadata":{"id":"gapNM1ii1unM"}},{"cell_type":"markdown","source":["> **EXERCISE:** Write a function `log_softmax()` that takes as input a vector `t` and returns its log(softmax)."],"metadata":{"id":"eoCgsGUZBSlg"}},{"cell_type":"code","source":["# âï¸ your code here\n"],"metadata":{"id":"CdyrM38gJgWw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ð Solution\n","\n","def log_softmax(t):\n","  return t - np.log(np.sum(np.exp(t), axis=1))[:, None]"],"metadata":{"cellView":"form","id":"l7roF0p1JarY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's compute some predictions on a minibatch of 12 images, using random weights and biases:"],"metadata":{"id":"yZF5n0UzJl0e"}},{"cell_type":"code","source":["def model(xb):\n","  return log_softmax(xb @ weights + bias)  # WARNING: this is using the _global_ variables 'weights' and 'bias'!\n","                                           #          not a good coding style, but ok for this notebook.\n","\n","m = 50\n","batch = X_train[:m]\n","\n","weights = np.random.rand(X_train.shape[1], n_classes)\n","bias = np.random.rand(1, n_classes)\n","\n","preds = model(batch)\n","preds[17]  # the predicted negative log-probabilities for image 17"],"metadata":{"id":"ElQXNvyNBRbN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the loss itself we'll use the **negative log-likelihood** (NLL). For an image $\\mathbf{x}_i$ with true class label $c$, it is defined as:\n","\n","$$ -\\log (p_{c_i}) $$\n","\n","where $p_{c_i}$ is the model's predicted probability that $\\mathbf{x}_i$ belongs to class $c$. The loss makes sense: higher probabilities for the correct class yield lower losses.\n","\n","We can rewrite this as:\n","\n","$$ -\\log(\\text{softmax}(\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b})_c) $$\n","\n","Again, $\\text{softmax}(\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b})_c$ is the predicted probability that the $i$-th sample belongs to class $c$.\n","\n","For a batch of $m$ images, we simply average their NLL loss to make it independent from the batch size:\n","\n","$$ \\ell(\\mathbf{W}, \\mathbf{b}) = - \\frac{1}{m}  \\sum_{i=1}^m \\sum_{c=1}^n \\mathbf{y}_{i,c} \\log(\\text{softmax}(\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b})_c)\\,, $$\n","\n","where $\\mathbf{y}_i$ is a one-hot encoding of the correct class for sample $i$."],"metadata":{"id":"H0_5NMYV-B5D"}},{"cell_type":"markdown","source":["> **EXERCISE:** _(Pen and paper)_ Show how the cross-entropy loss of binary logistic regression relates to the formula above. Are they are equivalent?"],"metadata":{"id":"bIBnaO_44L9I"}},{"cell_type":"markdown","source":["Writing the loss as a python function is a one-liner, and we'll need it to test our learning algorithms."],"metadata":{"id":"Rxa-JfkC4ybn"}},{"cell_type":"markdown","source":["> **EXERCISE:** Write a function `nll()` that computes the NLL loss, taking as input a matrix of log-probabilities and a vector of ground-truth labels."],"metadata":{"id":"vcUYgLjcKGYg"}},{"cell_type":"code","source":["# âï¸ your code here\n"],"metadata":{"id":"qlRt1jDMKoto"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ð Solution\n","\n","def nll(log_prob, target):\n","  batch_size = log_prob.shape[0]\n","  loss = -np.mean(log_prob[np.arange(batch_size), target])\n","  return loss\n","\n","nll(preds, y_train[:m])"],"metadata":{"cellView":"form","id":"eUEqPcloBH9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Take a note of this loss value -- we are going to bring it down!"],"metadata":{"id":"nTMa2VDSNyWg"}},{"cell_type":"markdown","source":["## The gradient\n","\n","It's the dreaded time to compute the gradient ð©. Here's our complete loss from before, maximizing for each sample the probability of the correct class:\n","\n","$$ \\ell(\\mathbf{W}, \\mathbf{b}) = - \\frac{1}{m}  \\sum_{i=1}^m \\sum_{c=1}^n \\mathbf{y}_{i,c} \\log(\\text{softmax}(\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b})_c)\\,, $$\n","\n","We are expected to compute its gradient with respect to $\\mathbf{W}$ and $\\mathbf{b}$.\n","\n","Lo and behold, this is actually quite easy! If we have $m$ samples per minibatch, $f$ features, and $c$ classes, we get the compact expressions:\n","\n","$$ \\nabla_\\mathbf{W} \\ell = \\mathbf{X}^\\top( \\text{softmax}(\\mathbf{Z}) - \\mathbf{Y}) $$\n","\n","$$ \\nabla_\\mathbf{b} \\ell = \\mathbf{1}^\\top( \\text{softmax}(\\mathbf{Z}) - \\mathbf{Y}) $$\n","\n","where $\\mathbf{X}$ is a $m \\times f$ matrix of features, $\\mathbf{Z}$ is a $m \\times c$ matrix of logits, and $\\mathbf{Y}$ is a matrix containing as rows the one-hot representations of the correct classes, a row per sample.\n"],"metadata":{"id":"e3OH_dLN83se"}},{"cell_type":"markdown","source":["> **EXERCISE:** What are the dimensions of $\\mathbf{W}$ and $\\mathbf{b}$? What about $\\nabla_\\mathbf{W} \\ell$ and $\\nabla_\\mathbf{b} \\ell$?"],"metadata":{"id":"vuJiB58WbSE8"}},{"cell_type":"markdown","source":["You know what to do: write down the code for computing the gradient over a batch, and then use it in a (stochastic) gradient descent loop. Let's start with the gradient:"],"metadata":{"id":"tjbnaK3wbewh"}},{"cell_type":"markdown","source":["> **EXERCISE:** Write a `grad` function that takes as input a minibatch `batch`, together with its ground-truth labels `target`, and returns a tuple `W_grad, b_grad` with the gradients."],"metadata":{"id":"CFOE3q_MY8Nw"}},{"cell_type":"code","source":["# âï¸ your code here\n"],"metadata":{"id":"B1ko3DL1dK5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ð Solution\n","\n","def grad(batch, target):\n","\n","  Z = batch @ weights + bias  # logits\n","  sZ = np.exp(Z)\n","  sZ /= np.sum(sZ, axis=1)[:, None]\n","  Y = np.zeros_like(sZ)\n","  Y[np.arange(batch.shape[0]), target] = 1\n","\n","  W_grad = batch.transpose() @ (sZ - Y) / batch.shape[0]\n","  b_grad = np.sum(sZ - Y, axis=0)[None, :]\n","\n","  return W_grad, b_grad"],"metadata":{"id":"Jp7JKUGLeXBU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The algorithm\n","\n","We now have all the ingredients to write down our own version of SGD! Let's start with standard gradient descent, meaning that we only have one batch of data corresponding to the entire training set.\n","\n","Let's initialize our model with random parameters:"],"metadata":{"id":"soC_nJPEdVt_"}},{"cell_type":"code","source":["weights = np.random.rand(X_train.shape[1], n_classes)\n","bias = np.random.rand(1, n_classes)"],"metadata":{"id":"HRMzxolZiEY3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE:** Write the gradient descent algorithm, training with the entire dataset `X_train` and ground-truth labels `y_train`. Pick your own learning rate and maximum number of iterations, and plot the loss vs. the number of iterations."],"metadata":{"id":"jnbWnW7pi-zh"}},{"cell_type":"code","source":["# âï¸ your code here\n"],"metadata":{"id":"lCe1fjFrjLDx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ð Solution\n","\n","lr = 0.01\n","max_iter = 1000\n","\n","losses = []\n","\n","for iter in range(max_iter):\n","\n","  W_grad, b_grad = grad(X_train, y_train)\n","\n","  weights -= lr * W_grad\n","  bias -= lr * b_grad\n","\n","  loss = nll(model(X_train), y_train)\n","  losses.append(loss)\n","\n","plt.figure(figsize=(6, 3))\n","plt.plot(losses)\n","plt.xlabel('iterations')\n","plt.ylabel('training loss')\n","plt.show()"],"metadata":{"cellView":"form","id":"m1VgOlfLiI7o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE:** Try to play with the learning rate a bit. You might notice that for some choices like `lr = 1.0` you get an overshooting effect. Why is that? Shouldn't a unitary learning rate guarantee that the descent step has the same exact magnitude of the gradient?"],"metadata":{"id":"xSDG4EsZk62B"}},{"cell_type":"markdown","source":["Make sure you understood the details of this part -- we are now going for our final algorithm! Stochastic gradient descent differs from the standard algorithm in one key aspect: **we must use _mini-batches_ of data**, rather than all the training data at once.\n","\n","So the first thing we need is a way to break down our training data into mini-batches of prescribed size $m$."],"metadata":{"id":"T7RfD9BLj98Y"}},{"cell_type":"code","source":["m = 50  # what happens if m does not divide n_train exactly?\n","n_batches = int(np.ceil(n_train / m))\n","n_batches"],"metadata":{"id":"74EJp53qdMy_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before we implement SGD: if we now compute the loss over all the mini-batches composing our training data, what do you expect to see?"],"metadata":{"id":"sKqKGSbrfFxz"}},{"cell_type":"markdown","source":["> **EXERCISE:** Show a plot of the loss computed over all the mini-batches. Put the batch number along the x-axis."],"metadata":{"id":"4bPZoqmggR6q"}},{"cell_type":"markdown","source":["If the loss is this noisy even before we start the optimization (recall that we are evaluating the model at a fixed point of the loss landscape! The one given by the current weights and biases), it stands to reason that it won't go down as smoothly as standard gradient descent. Let's test this!"],"metadata":{"id":"5XXM3aM7grFS"}},{"cell_type":"markdown","source":["> **EXERCISE:** Write the entire SGD algorithm, and plot the training loss vs. the number of epochs. In the same figure, also plot standard GD and draw your (hopefully enthusiastic) conclusions."],"metadata":{"id":"Zl5FQWiBhvKW"}},{"cell_type":"code","source":["# âï¸ your code here\n","\n","# re-initialize with random parameters\n","weights_init = np.random.rand(X_train.shape[1], n_classes)\n","bias_init = np.random.rand(1, n_classes)\n","\n","lr = 0.01\n","max_epochs = 1000\n","\n","# ..."],"metadata":{"id":"-AKJOgLzmLmf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ð Solution\n","\n","lr = 0.01\n","max_epochs = 1000\n","\n","weights_init = np.random.rand(X_train.shape[1], n_classes)\n","bias_init = np.random.rand(1, n_classes)\n","\n","plt.figure(figsize=(6, 3))\n","\n","for m in [50, n_train]:  # SGD, GD\n","\n","  weights = weights_init.copy()  # we don't want shallow copies\n","  bias = bias_init.copy()\n","\n","  n_batches = int(np.ceil(n_train / m))\n","\n","  losses = np.zeros(max_epochs)\n","  for epoch in range(max_epochs):\n","\n","    # reshuffle the training set at each epoch\n","    X_train, y_train = shuffle(X_train, y_train)\n","\n","    for i in range(n_batches):\n","\n","      start = i * m\n","      end = start + m\n","      batch = X_train[start:end]\n","      target = y_train[start:end]\n","\n","      W_grad, b_grad = grad(batch, target)\n","\n","      weights -= lr * W_grad\n","      bias -= lr * b_grad\n","\n","    # store the training loss for plotting\n","    losses[epoch] = nll(model(batch), target)\n","\n","  if m < n_train:\n","    plt.plot(losses, color='red', label='SGD')\n","  else:\n","    plt.plot(losses, color='black', label='GD')\n","\n","plt.xlabel('epochs')\n","plt.ylabel('training loss')\n","plt.legend()\n","plt.show()"],"metadata":{"cellView":"form","id":"ZIX6yFiLe0Xj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Not only SGD is faster, but also more accurate ðª\n","\n","Experiment with different choices of batch size, learning rate, and number of epochs to see how the loss goes down differently. There is no recipe for choosing the right hyper-parameters, and their impact on the so-called **training dynamics** is still an open research direction."],"metadata":{"id":"g6j1v5imqvdL"}},{"cell_type":"markdown","metadata":{"id":"U2eKSdGZCqXc"},"source":["### On the optimal batch size\n","\n","As we have seen in theory class, we don't really want to reach the global minima of the loss. At the global minimum, an overparametrized model like a deep neural network would overfit big time.\n","\n","Quoting the work of Keskar et al. [*On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima*](https://ar5iv.org/abs/1609.04836):\n","\n",">The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation.\n","\n","Nevertheless, the optimal size of a mini-batch is still debated and many interesting works have been published in the last years, for instance:\n","\n","- [*Don't Decay the Learning Rate, Increase the Batch Size*](https://ar5iv.org/abs/1711.00489)\n","- [*Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour*](https://arxiv.org/abs/1706.02677)"]},{"cell_type":"markdown","source":["## Evaluation\n","\n","We are not done yet, because we still need to evaluate the performance of our trained model on the validation set."],"metadata":{"id":"0Fk09CLmr7u5"}},{"cell_type":"markdown","source":["> **EXERCISE:** Test your SGD-trained model on the validation set of the small digit data. In particular, do the following plots:\n","> - Figure 1: plot the loss on training and validation data (two different curves) across the epochs.\n","> - Figure 2: plot the validation _accuracy_ across the epochs."],"metadata":{"id":"vVxI8IWf6UuD"}},{"cell_type":"code","source":["# âï¸ your code here\n"],"metadata":{"id":"0eNrLP5S8ZX4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Can you reach the same accuracy as the one we obtained with Scikit-learn?"],"metadata":{"id":"d8OfOaR38vta"}},{"cell_type":"markdown","source":["## EXERCISE: Classifying MNIST digits\n","\n","You now have all the skills needed to setup, configure, and solve a classification problem using SGD as a training algorithm. Importantly, **you coded a functioning learning algorithm from start to finish, all by yourself**, using existing libraries only for computing matrix products and plotting. Congratulations! ð¤\n","\n","Let's finish this notebook by applying all we learned to classify MNIST digits. **Can you do better than Scikit-learn?**\n","\n","Perhaps by implementing **momentum**?"],"metadata":{"id":"IodnlK_M7n4J"}},{"cell_type":"code","source":["# âï¸ your Scikit-learn beating code here\n"],"metadata":{"id":"cg7xuaBq--GA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Coda\n","\n","Can you think of any classification problem that you would like to solve in your own everyday life?\n","\n","Maybe something that recognizes faces of your friends from your photos ð§? Or monsters in Pokemon ð¾? Or written text from the sound of a laptop keyboard ð»ðµ? All these are solvable with what you learned today, so go out there and have fun!"],"metadata":{"id":"V2YRWUNW-nbK"}}],"metadata":{"colab":{"collapsed_sections":["iXd3HJRDfLEO","UvrnMPqEi8tH","s00hN6W4k2HR","qb6JXFrQftIE","_xpMqBAJfzyc","9b-qUjGSzK1l","_JcRld0z43cu","U2eKSdGZCqXc","h71mh066FQ0K","5LaIS0vU9QBj","H0bauc84HDZI","N5yT0fptmAbI","CVgjkHqEsfpz","xpfC6MtSzj1a","8pbUkeGInqo6","QtGwWrVozyst","bQPNQ7O6tgxB","bLgvF66DrOqH","2SfcOlmR3K4l","DgTAQ-1v6DSb","O_UW943f3K46","P41mCzbVioo-"],"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
