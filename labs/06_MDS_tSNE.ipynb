{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/erodola/DLAI-s2-2023/blob/main/labs/04/4_Logistic_Regression_and_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"4C5Ct9yoZKYa"},"source":["# Machine Learning\n","\n","# Tutorial 6: MDS and t-SNE\n","\n","In this tutorial, we will cover:\n","\n","- Multi-dimensional scaling\n","- t-SNE\n","\n","Authors:\n","\n","- Prof. Emanuele Rodolà\n","\n","Course:\n","\n","- Lectures and notebooks at https://github.com/erodola/ML-s2-2024/"]},{"cell_type":"markdown","metadata":{"id":"iXd3HJRDfLEO"},"source":["# Imports and utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pRePt-K1_yw9"},"outputs":[],"source":["# @title import dependencies\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.display import display, clear_output\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2tGN_bJOcfd3"},"outputs":[],"source":["# @title reproducibility stuff\n","\n","import random\n","np.random.seed(42)\n","random.seed(0)"]},{"cell_type":"markdown","metadata":{"id":"E-665qN07U0Q"},"source":["# Multi-Dimensional Scaling"]},{"cell_type":"markdown","metadata":{"id":"MyZEKMgF7U0Q"},"source":["Today we start simple! Our first goal is to reconstruct a square grid of 2D points, when given all the inter-point distances as input. Let's begin by building the 2D grid:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"swblMWyA7U0Q"},"outputs":[],"source":["side = 10\n","n = side * side\n","\n","pts = np.linspace(0, 1, side)\n","ptsx = pts[:, None] * np.ones((1, side))\n","ptsy = np.ones((side, 1)) * pts[None, :]\n","pts = np.column_stack([ptsx.reshape(-1), ptsy.reshape(-1)])\n","\n","plt.figure(figsize=(4,2))\n","plt.scatter(pts[:, 0], pts[:, 1])\n","plt.axis('equal')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"LQGrqNmF7U0Q"},"source":["Now compute all the pairwise distances:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TASRIRL67U0R"},"outputs":[],"source":["D = np.linalg.norm(pts[:, None] - pts[None, :], axis=-1)\n","\n","plt.figure(figsize=(4,2))\n","plt.imshow(D)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-81DDt_w7U0R"},"source":["At this point we are going to _forget about the coordinates of the 2D points_; imagine we lost this information, and we have to recover it from the distance matrix alone! This is one way of looking at multi-dimensional scaling (MDS): a recipe to recover (absolute) point coordinates from (relative) distances."]},{"cell_type":"markdown","metadata":{"id":"nXWCw0Tc7U0R"},"source":["The MDS algorithm itself is not difficult. We need to minimize the metric distortion (_stress_), and we are going to do this with gradient descent. To refresh your memory, here's the optimization problem we are going to minimize:\n","\n","$$ \\min_\\mathbf{Z} \\| \\mathbf{D} - D(\\mathbf{Z}) \\|_F^2 $$\n","\n","where $\\mathbf{D}$ is the $n \\times n$ input matrix of pairwise distances, $\\mathbf{Z} \\in \\mathbb{R}^{n\\times k}$ are the $k$-dimensional points we are seeking to recover, and $D(\\mathbf{Z})$ is the pairwise distance matrix computed from the recovered points. The gradient of this quadratic stress is given by:\n","\n","$$ 2 \\mathbf{VZ} - 2 B(\\mathbf{Z})\\mathbf{Z} $$\n","\n","with\n","\n","\\begin{align}\n","\\mathbf{V} &= n \\mathbf{I} - \\mathbf{11}^\\top \\\\\n","B(\\mathbf{Z}) &= -\\mathbf{D} \\oslash D(\\mathbf{Z}) + \\text{diag}( ( \\mathbf{D} \\oslash D(\\mathbf{Z}) ) \\mathbf{1} )\n","\\end{align}\n","\n","Let's bring the stress down!"]},{"cell_type":"markdown","metadata":{"id":"RoSqoO-_7U0R"},"source":["> **EXERCISE:** Create a `mds` function taking as input the following parameters:\n",">\n","> - `D`: a $n\\times n$ distance matrix\n","> - `k`: the desired target dimension of the embedded points\n","> - `lr`: the learning rate for gradient descent\n","> - `max_iter`: the maximum number of iterations for gradient descent\n","> - `tol`: a tolerance value to use as stop criterion for gradient descent\n",">\n","> The function must return a $n\\times k$ matrix `Z` of points whose pairwise Euclidean distances are as close as possible to $\\mathbf{D}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5-T7Gfx7U0R"},"outputs":[],"source":["# ✏️ your solution here\n","# def mds(...)\n","\n","# test your solution with this call:\n","# Z = mds(D, 2, lr=2e-3, max_iter=1000, tol=1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xkVTxXn97U0S"},"outputs":[],"source":["# @title 👀 Solution\n","\n","def mds(D,\n","        k: int,\n","        lr: float,\n","        max_iter: int,\n","        tol: float):\n","\n","    n_ = D.shape[0]\n","\n","    Z = np.random.rand(n_, k)\n","\n","    V = n_ * np.eye(n_) - np.ones((n_, n_))\n","\n","    if k == 2:\n","        plt.figure(figsize=(4,2))\n","        plt.scatter(Z[:, 0], Z[:, 1])\n","        plt.axis('equal')\n","        display(plt.gcf())\n","\n","    stress = np.zeros(max_iter, dtype=np.float32)\n","\n","    for i in range(max_iter):\n","\n","        if k == 2:\n","            clear_output(wait=True)\n","\n","        Dz = np.linalg.norm(Z[:, None] - Z[None, :], axis=-1)\n","\n","        B = -D / (1e-8 + Dz)\n","        B = B + np.diag(-np.sum(B, axis=1))\n","\n","        stress[i] = np.linalg.norm(D - Dz)**2\n","\n","        if i > 0 and np.abs(stress[i] - stress[i-1]) < tol:\n","            stress = stress[:i]\n","            break\n","\n","        grad = 2 * (V - B) @ Z\n","        Z -= lr * grad\n","\n","        if k == 2:\n","            plt.clf()\n","            plt.scatter(Z[:, 0], Z[:, 1])\n","            plt.axis('equal')\n","            display(plt.gcf())\n","\n","    plt.figure(figsize=(4,2))\n","    plt.plot(stress)\n","    plt.show()\n","\n","    return Z"]},{"cell_type":"markdown","metadata":{"id":"u4n2Cap07U0S"},"source":["Can you recover the original 2D grid? Let's see:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugqbNdeS7U0S"},"outputs":[],"source":["_ = mds(D, 2, lr=2e-3, max_iter=1000, tol=1e-4)"]},{"cell_type":"markdown","metadata":{"id":"vKbDO9K57U0S"},"source":["> **EXERCISE:** Why aren't we obtaining exactly the same grid we started with? Is something broken in our algorithm, or is this expected?"]},{"cell_type":"markdown","metadata":{"id":"DVDoj3jH7U0S"},"source":["## Using autograd\n","\n","Now, I know it was a bit painful to compute gradients by hand during the theory class, and it's quite likely that you went through some additional pain when implementing gradient descent correctly in the last exercise.\n","\n","...But we could have just used **automatic differentiation** to solve our problem instead of doing everything by hand! Let's do this, and enjoy the simplicity of the new code."]},{"cell_type":"markdown","metadata":{"id":"0F6fmlRt7U0S"},"source":["> **EXERCISE:** Write a new function `mds_ad` that implements the same algorithm as `mds`, but using automatic differentiation instead of hand-written gradients. You can use the `torch` package for the autograd."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaJItd-E7U0S"},"outputs":[],"source":["# ✏️ your solution here"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"SEspOe967U0T"},"outputs":[],"source":["# @title 👀 Solution\n","\n","def mds_ad(D,\n","           k: int,\n","           lr: float,\n","           max_iter: int,\n","           tol: float):\n","\n","    n_ = D.shape[0]\n","\n","    D_torch = torch.from_numpy(D)\n","    Z = torch.rand((n_, k), requires_grad=True)  # we need the gradient wrt Z\n","\n","    if k == 2:\n","        with torch.no_grad():\n","            plt.figure(figsize=(4,2))\n","            plt.scatter(Z[:, 0], Z[:, 1])\n","            plt.axis('equal')\n","            display(plt.gcf())\n","\n","    stress = torch.zeros(max_iter)\n","\n","    for i in range(max_iter):\n","\n","        if k == 2:\n","            clear_output(wait=True)\n","\n","        Dz = torch.linalg.norm(Z[:, None] - Z[None, :], dim=-1)\n","        loss = torch.linalg.norm(D_torch - Dz)**2\n","\n","        stress[i] = loss.item()\n","\n","        if i > 0 and torch.abs(stress[i] - stress[i-1]) < tol:\n","            stress = stress[:i]\n","            break\n","\n","        loss.backward()\n","\n","        with torch.no_grad():\n","            Z -= lr * Z.grad\n","            Z.grad = None  # remember this to avoid gradient accumulation!\n","\n","        if k == 2:\n","            with torch.no_grad():\n","                plt.clf()\n","                plt.scatter(Z[:, 0], Z[:, 1])\n","                plt.axis('equal')\n","                display(plt.gcf())\n","\n","    plt.figure(figsize=(4,2))\n","    plt.plot(stress)\n","    plt.show()\n","\n","    return Z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dse6yn3C7U0T"},"outputs":[],"source":["_ = mds_ad(D, 2, lr=2e-3, max_iter=1000, tol=1e-4)"]},{"cell_type":"markdown","metadata":{"id":"lNfM9veo7U0T"},"source":["## Dimensionality reduction\n","\n","As we have seen, MDS is an embedding algorithm that can be used to recover point positions from their pairwise distances.\n","\n","But it can do more than that! Since we can choose the number of dimensions in the embedding space, we can exploit this to do _dimensionality reduction_.\n","\n","For example, let's embed our 2D grid into $\\mathbb{R}$:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7RoM2mdQ7U0T"},"outputs":[],"source":["Z1d = mds(D, 1, lr=2e-3, max_iter=1000, tol=1e-10)\n","\n","plt.figure(figsize=(4,2))\n","plt.plot(Z1d)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iSNUhOfi7U0T"},"source":["Hmm, this 1D embedding is not too easy to interpret. For example, what does it mean that point 80 has value 1.3?\n","\n","Let's visualize this embedding in a different way."]},{"cell_type":"markdown","metadata":{"id":"t1EuxmpK7U0T"},"source":["> **EXERCISE:** Visualize the 1D embedding of the 2D grid as _colors_ for each point in the grid. For example, if point number 80 has value 1.3, then paint that point with a color proportional to 1.3 from the `jet` colormap.\n",">\n","> Once you have done this, how can you interpret the 1D embedding?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRVWH0cj7U0T"},"outputs":[],"source":["# ✏️ your solution here"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mO0eHicy7U0U"},"outputs":[],"source":["# @title 👀 Solution\n","\n","plt.figure(figsize=(4,2))\n","sc = plt.scatter(pts[:, 0], pts[:, 1], c=Z1d, cmap='jet')\n","plt.colorbar(sc)\n","plt.axis('equal')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fp4uTjY97U0U"},"source":["> **EXERCISE:** Take the small digits dataset from Scikit-learn and embed it in 2D using MDS. What results do you get? Can you spot any _clusters_ of different digits?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7sAy24S7U0U"},"outputs":[],"source":["from sklearn.datasets import load_digits\n","\n","digits = load_digits().data\n","\n","# ✏️ your solution here"]},{"cell_type":"markdown","metadata":{"id":"7ZOejHft7U0U"},"source":["> **EXERCISE:** Using the same images from the previous exercise, use PCA to embed them in 2D. Compare with the result you got using MDS. From a qualitative perspective, what are the main differences?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YTKfS9HK7U0U"},"outputs":[],"source":["from sklearn.datasets import load_digits\n","\n","digits = load_digits().data\n","\n","# ✏️ your solution here"]},{"cell_type":"markdown","metadata":{"id":"TIB30dN47U0U"},"source":["# EXERCISE: t-SNE\n","\n","_**NOTE: The 3 best solutions will get +1 point to the final grade!**_\n","\n","We are now 2/3 through the ML course -- it's time to prove yourself a worthy ML practitioner! 🤓\n","\n","Your task is to fill in this Section. In particular:\n","\n","- Use markdown and code cells, as if you are writing this part of the notebook for your own students!\n","- You can use Scikit-learn's implementation of t-SNE, your own, or whatever you prefer.\n","\n","The Section should include _at least_ the following experiments:\n","\n","- Embed the small digit data into 2D using t-SNE.\n","- Embed the small digit data into 3D using t-SNE.\n","- Comment on the differences you observe between the two plots.\n","- Comment on the differences you observe with PCA and MDS embeddings in the same target space.\n","- Embed a _larger_ dataset into 2D using t-SNE, e.g. MNIST, or character emojis, or pokemon images... you choose it!\n","- Add whatever you think it's worth adding. If you think a t-SNE parameter needs further discussion, or some other aspect is worth discussing, feel free to do it!\n","\n","Send me your notebooks via email. Participation is not mandatory."]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}